{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AFNANAMIN/AI_Freelancing/blob/master/task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpQZPnKafTay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg0SMPc9ferl",
        "colab_type": "code",
        "outputId": "c39ff3c0-28ab-47b9-ee97-844e790c330c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8znfBi2Mfg-c",
        "colab_type": "code",
        "outputId": "b20c37d4-6a22-4c77-9337-6a133b493a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/task2/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/task2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUzLl2_ngHhF",
        "colab_type": "code",
        "outputId": "edaa7f52-bd5b-4fe1-cce6-e38a60db5899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w1.txt\tw2.txt\tX_test.csv  X_train.csv  Y_test.csv  Y_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbGSIm9ugf40",
        "colab_type": "code",
        "outputId": "3ab57c8c-0e21-4f89-80a8-44174a2ee93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "train=pd.read_csv('X_train.csv',header=None)\n",
        "test=pd.read_csv('X_test.csv',header=None)\n",
        "label_train=pd.read_csv('Y_train.csv',names=['target'])\n",
        "label_test=pd.read_csv('Y_test.csv',names=['target'])\n",
        "merged_train = train.join(label_train)\n",
        "merged_train"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>859</th>\n",
              "      <th>860</th>\n",
              "      <th>861</th>\n",
              "      <th>862</th>\n",
              "      <th>863</th>\n",
              "      <th>864</th>\n",
              "      <th>865</th>\n",
              "      <th>866</th>\n",
              "      <th>867</th>\n",
              "      <th>868</th>\n",
              "      <th>869</th>\n",
              "      <th>870</th>\n",
              "      <th>871</th>\n",
              "      <th>872</th>\n",
              "      <th>873</th>\n",
              "      <th>874</th>\n",
              "      <th>875</th>\n",
              "      <th>876</th>\n",
              "      <th>877</th>\n",
              "      <th>878</th>\n",
              "      <th>879</th>\n",
              "      <th>880</th>\n",
              "      <th>881</th>\n",
              "      <th>882</th>\n",
              "      <th>883</th>\n",
              "      <th>884</th>\n",
              "      <th>885</th>\n",
              "      <th>886</th>\n",
              "      <th>887</th>\n",
              "      <th>888</th>\n",
              "      <th>889</th>\n",
              "      <th>890</th>\n",
              "      <th>891</th>\n",
              "      <th>892</th>\n",
              "      <th>893</th>\n",
              "      <th>894</th>\n",
              "      <th>895</th>\n",
              "      <th>896</th>\n",
              "      <th>897</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>77</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>92</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>121</td>\n",
              "      <td>0</td>\n",
              "      <td>79</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>647</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>647</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>223</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>127</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>37</td>\n",
              "      <td>149</td>\n",
              "      <td>0</td>\n",
              "      <td>226</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1143</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>1143</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>225</td>\n",
              "      <td>6</td>\n",
              "      <td>22</td>\n",
              "      <td>16</td>\n",
              "      <td>127</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>59</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>228</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>65.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1230</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>1230</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>336</td>\n",
              "      <td>10</td>\n",
              "      <td>25</td>\n",
              "      <td>40</td>\n",
              "      <td>243</td>\n",
              "      <td>29</td>\n",
              "      <td>45</td>\n",
              "      <td>93</td>\n",
              "      <td>314</td>\n",
              "      <td>6</td>\n",
              "      <td>341</td>\n",
              "      <td>14</td>\n",
              "      <td>9</td>\n",
              "      <td>25</td>\n",
              "      <td>40</td>\n",
              "      <td>2</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>41.250000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>2159</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>2159</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>13</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>457</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>53</td>\n",
              "      <td>277</td>\n",
              "      <td>29</td>\n",
              "      <td>45</td>\n",
              "      <td>105</td>\n",
              "      <td>331</td>\n",
              "      <td>6</td>\n",
              "      <td>463</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>30</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>39.200000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>2586</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>2586</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>40</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>204</td>\n",
              "      <td>4</td>\n",
              "      <td>46</td>\n",
              "      <td>17</td>\n",
              "      <td>51</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>75</td>\n",
              "      <td>192</td>\n",
              "      <td>0</td>\n",
              "      <td>208</td>\n",
              "      <td>44</td>\n",
              "      <td>19</td>\n",
              "      <td>47</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>44</td>\n",
              "      <td>66.181818</td>\n",
              "      <td>0.244949</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1385</td>\n",
              "      <td>18</td>\n",
              "      <td>54.485322</td>\n",
              "      <td>5</td>\n",
              "      <td>1385</td>\n",
              "      <td>1156.777778</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1196</th>\n",
              "      <td>40</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>209</td>\n",
              "      <td>4</td>\n",
              "      <td>52</td>\n",
              "      <td>18</td>\n",
              "      <td>64</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>212</td>\n",
              "      <td>0</td>\n",
              "      <td>213</td>\n",
              "      <td>44</td>\n",
              "      <td>19</td>\n",
              "      <td>53</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>6</td>\n",
              "      <td>46</td>\n",
              "      <td>73.500000</td>\n",
              "      <td>0.252315</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>1473</td>\n",
              "      <td>18</td>\n",
              "      <td>54.485322</td>\n",
              "      <td>5</td>\n",
              "      <td>1473</td>\n",
              "      <td>1156.777778</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1197</th>\n",
              "      <td>42</td>\n",
              "      <td>6</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>214</td>\n",
              "      <td>4</td>\n",
              "      <td>54</td>\n",
              "      <td>18</td>\n",
              "      <td>64</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>88</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>218</td>\n",
              "      <td>47</td>\n",
              "      <td>19</td>\n",
              "      <td>55</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>48</td>\n",
              "      <td>71.307692</td>\n",
              "      <td>0.232906</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>1531</td>\n",
              "      <td>18</td>\n",
              "      <td>54.485322</td>\n",
              "      <td>5</td>\n",
              "      <td>1531</td>\n",
              "      <td>1156.777778</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1198</th>\n",
              "      <td>43</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>277</td>\n",
              "      <td>4</td>\n",
              "      <td>54</td>\n",
              "      <td>19</td>\n",
              "      <td>145</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>88</td>\n",
              "      <td>334</td>\n",
              "      <td>0</td>\n",
              "      <td>282</td>\n",
              "      <td>76</td>\n",
              "      <td>19</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>71</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>7</td>\n",
              "      <td>48</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>0.287698</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>1962</td>\n",
              "      <td>18</td>\n",
              "      <td>54.485322</td>\n",
              "      <td>5</td>\n",
              "      <td>1962</td>\n",
              "      <td>1156.777778</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1199</th>\n",
              "      <td>45</td>\n",
              "      <td>9</td>\n",
              "      <td>15</td>\n",
              "      <td>5</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>279</td>\n",
              "      <td>4</td>\n",
              "      <td>54</td>\n",
              "      <td>21</td>\n",
              "      <td>148</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>88</td>\n",
              "      <td>339</td>\n",
              "      <td>0</td>\n",
              "      <td>285</td>\n",
              "      <td>77</td>\n",
              "      <td>20</td>\n",
              "      <td>55</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>48</td>\n",
              "      <td>65.466667</td>\n",
              "      <td>0.335185</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.066667</td>\n",
              "      <td>1999</td>\n",
              "      <td>18</td>\n",
              "      <td>54.485322</td>\n",
              "      <td>5</td>\n",
              "      <td>1999</td>\n",
              "      <td>1156.777778</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1200 rows × 899 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0   1   2   3         4  ...        894  895   896          897  target\n",
              "0     11   3   0   4 -1.000000  ...  36.990000    2   647  1553.000000       3\n",
              "1     14   4   1   6 -1.000000  ...  36.990000    2  1143  1553.000000       0\n",
              "2     14   4   2   6 -1.000000  ...  36.990000    2  1230  1553.000000       3\n",
              "3     24   9   4  10 -1.000000  ...  36.990000    2  2159  1553.000000       2\n",
              "4     28  10   5  13 -1.000000  ...  36.990000    2  2586  1553.000000       3\n",
              "...   ..  ..  ..  ..       ...  ...        ...  ...   ...          ...     ...\n",
              "1195  40   6  11   5  0.333333  ...  54.485322    5  1385  1156.777778       1\n",
              "1196  40   6  12   5  0.333333  ...  54.485322    5  1473  1156.777778       0\n",
              "1197  42   6  13   5  0.333333  ...  54.485322    5  1531  1156.777778       3\n",
              "1198  43   9  14   5  0.333333  ...  54.485322    5  1962  1156.777778       3\n",
              "1199  45   9  15   5  1.000000  ...  54.485322    5  1999  1156.777778       0\n",
              "\n",
              "[1200 rows x 899 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKI45ihfRJwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data():\n",
        "    #get train data\n",
        "    train_data_path ='X_train.csv'\n",
        "    train = pd.read_csv(train_data_path,header=None)\n",
        "    \n",
        "    #get test data\n",
        "    test_data_path ='X_test.csv'\n",
        "    test = pd.read_csv(test_data_path,header=None)\n",
        "\n",
        "    target=pd.read_csv('Y_train.csv',names=['target'])\n",
        "    \n",
        "    return train , test,target\n",
        "\n",
        "def get_combined_data():\n",
        "  #reading train data\n",
        "  train , test,target = get_data()\n",
        "\n",
        "  target = train.SalePrice\n",
        "  train.drop(['SalePrice'],axis = 1 , inplace = True)\n",
        "\n",
        "  combined = train.append(test)\n",
        "  combined.reset_index(inplace=True)\n",
        "  combined.drop(['index', 'Id'], inplace=True, axis=1)\n",
        "  return combined, target\n",
        "\n",
        "#Load train and test data into pandas DataFrames\n",
        "train_data, test_data = get_data()\n",
        "\n",
        "#Combine train and test data to process them together\n",
        "combined, target = get_combined_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN59qo5qRJzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine=train.append(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5aH68bBRJ2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "d5f76303-556d-4ac9-9226-64fae42233bf"
      },
      "source": [
        "combine"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>858</th>\n",
              "      <th>859</th>\n",
              "      <th>860</th>\n",
              "      <th>861</th>\n",
              "      <th>862</th>\n",
              "      <th>863</th>\n",
              "      <th>864</th>\n",
              "      <th>865</th>\n",
              "      <th>866</th>\n",
              "      <th>867</th>\n",
              "      <th>868</th>\n",
              "      <th>869</th>\n",
              "      <th>870</th>\n",
              "      <th>871</th>\n",
              "      <th>872</th>\n",
              "      <th>873</th>\n",
              "      <th>874</th>\n",
              "      <th>875</th>\n",
              "      <th>876</th>\n",
              "      <th>877</th>\n",
              "      <th>878</th>\n",
              "      <th>879</th>\n",
              "      <th>880</th>\n",
              "      <th>881</th>\n",
              "      <th>882</th>\n",
              "      <th>883</th>\n",
              "      <th>884</th>\n",
              "      <th>885</th>\n",
              "      <th>886</th>\n",
              "      <th>887</th>\n",
              "      <th>888</th>\n",
              "      <th>889</th>\n",
              "      <th>890</th>\n",
              "      <th>891</th>\n",
              "      <th>892</th>\n",
              "      <th>893</th>\n",
              "      <th>894</th>\n",
              "      <th>895</th>\n",
              "      <th>896</th>\n",
              "      <th>897</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>77</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>92</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>121</td>\n",
              "      <td>0</td>\n",
              "      <td>79</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>647</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>647</td>\n",
              "      <td>1553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>223</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>127</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>37</td>\n",
              "      <td>149</td>\n",
              "      <td>0</td>\n",
              "      <td>226</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>39.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1143</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>1143</td>\n",
              "      <td>1553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>225</td>\n",
              "      <td>6</td>\n",
              "      <td>22</td>\n",
              "      <td>16</td>\n",
              "      <td>127</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>59</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>228</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>65.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1230</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>1230</td>\n",
              "      <td>1553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>336</td>\n",
              "      <td>10</td>\n",
              "      <td>25</td>\n",
              "      <td>40</td>\n",
              "      <td>243</td>\n",
              "      <td>29</td>\n",
              "      <td>45</td>\n",
              "      <td>93</td>\n",
              "      <td>314</td>\n",
              "      <td>6</td>\n",
              "      <td>341</td>\n",
              "      <td>14</td>\n",
              "      <td>9</td>\n",
              "      <td>25</td>\n",
              "      <td>40</td>\n",
              "      <td>2</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>41.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2159</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>2159</td>\n",
              "      <td>1553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>13</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>457</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>53</td>\n",
              "      <td>277</td>\n",
              "      <td>29</td>\n",
              "      <td>45</td>\n",
              "      <td>105</td>\n",
              "      <td>331</td>\n",
              "      <td>6</td>\n",
              "      <td>463</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>30</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>39.20</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.6</td>\n",
              "      <td>2586</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>2586</td>\n",
              "      <td>1553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>103</td>\n",
              "      <td>2</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>103</td>\n",
              "      <td>125.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>34.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>147</td>\n",
              "      <td>2</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>147</td>\n",
              "      <td>125.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>201</td>\n",
              "      <td>7</td>\n",
              "      <td>64.138095</td>\n",
              "      <td>5</td>\n",
              "      <td>201</td>\n",
              "      <td>1779.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>21</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>131</td>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "      <td>6</td>\n",
              "      <td>69</td>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>156</td>\n",
              "      <td>0</td>\n",
              "      <td>134</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>22</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>121.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>833</td>\n",
              "      <td>7</td>\n",
              "      <td>64.138095</td>\n",
              "      <td>5</td>\n",
              "      <td>833</td>\n",
              "      <td>1779.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>21</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>151</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>8</td>\n",
              "      <td>73</td>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>26</td>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "      <td>155</td>\n",
              "      <td>31</td>\n",
              "      <td>20</td>\n",
              "      <td>26</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>95.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>923</td>\n",
              "      <td>7</td>\n",
              "      <td>64.138095</td>\n",
              "      <td>5</td>\n",
              "      <td>923</td>\n",
              "      <td>1779.142857</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1599 rows × 898 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3    4    5    ...   892  893        894  895   896          897\n",
              "0     11    3    0    4 -1.0 -1.0  ...   647    5  36.990000    2   647  1553.000000\n",
              "1     14    4    1    6 -1.0  1.0  ...  1143    5  36.990000    2  1143  1553.000000\n",
              "2     14    4    2    6 -1.0  1.0  ...  1230    5  36.990000    2  1230  1553.000000\n",
              "3     24    9    4   10 -1.0  0.0  ...  2159    5  36.990000    2  2159  1553.000000\n",
              "4     28   10    5   13 -1.0  0.5  ...  2586    5  36.990000    2  2586  1553.000000\n",
              "..   ...  ...  ...  ...  ...  ...  ...   ...  ...        ...  ...   ...          ...\n",
              "394    7    1    0    0 -1.0 -1.0  ...   103    2  17.000000    2   103   125.000000\n",
              "395    8    1    1    0 -1.0  1.0  ...   147    2  17.000000    2   147   125.000000\n",
              "396    7    1    0    2 -1.0 -1.0  ...   201    7  64.138095    5   201  1779.142857\n",
              "397   21    8    1    4 -1.0 -1.0  ...   833    7  64.138095    5   833  1779.142857\n",
              "398   21    8    2    5  1.0 -1.0  ...   923    7  64.138095    5   923  1779.142857\n",
              "\n",
              "[1599 rows x 898 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO_BNidaVKis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_combined():\n",
        "    global combine\n",
        "    train = combine[:1200]\n",
        "    test = combine[1200:]\n",
        "\n",
        "    return train , test "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGP4v_NZVg9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = split_combined()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NavfRvwhVg7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvUBESCXVg4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ca7b1dVKez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTL7RdCYWD1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayH00fNAWD5M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "e9b61219-1b5e-4f5f-bd84-6320010199e8"
      },
      "source": [
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 128)               115072    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 279,937\n",
            "Trainable params: 279,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YTeMeDAWDyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PThPvfm6Wfnf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1cb45bee-1485-403b-a11d-9deef3708fc8"
      },
      "source": [
        "NN_model.fit(train, label_train.target, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 960 samples, validate on 240 samples\n",
            "Epoch 1/500\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "960/960 [==============================] - 1s 940us/step - loss: 13.4026 - mean_absolute_error: 13.4026 - val_loss: 1.8107 - val_mean_absolute_error: 1.8107\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.81072, saving model to Weights-001--1.81072.hdf5\n",
            "Epoch 2/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 1.9540 - mean_absolute_error: 1.9540 - val_loss: 1.7053 - val_mean_absolute_error: 1.7053\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.81072 to 1.70528, saving model to Weights-002--1.70528.hdf5\n",
            "Epoch 3/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 1.8181 - mean_absolute_error: 1.8181 - val_loss: 1.6344 - val_mean_absolute_error: 1.6344\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.70528 to 1.63445, saving model to Weights-003--1.63445.hdf5\n",
            "Epoch 4/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 1.5639 - mean_absolute_error: 1.5639 - val_loss: 1.8058 - val_mean_absolute_error: 1.8058\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.63445\n",
            "Epoch 5/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 1.3735 - mean_absolute_error: 1.3735 - val_loss: 2.0214 - val_mean_absolute_error: 2.0214\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.63445\n",
            "Epoch 6/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 1.2454 - mean_absolute_error: 1.2454 - val_loss: 1.4280 - val_mean_absolute_error: 1.4280\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.63445 to 1.42797, saving model to Weights-006--1.42797.hdf5\n",
            "Epoch 7/500\n",
            "960/960 [==============================] - 0s 177us/step - loss: 1.2979 - mean_absolute_error: 1.2979 - val_loss: 1.4853 - val_mean_absolute_error: 1.4853\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.42797\n",
            "Epoch 8/500\n",
            "960/960 [==============================] - 0s 177us/step - loss: 1.1722 - mean_absolute_error: 1.1722 - val_loss: 1.6261 - val_mean_absolute_error: 1.6261\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.42797\n",
            "Epoch 9/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 1.1336 - mean_absolute_error: 1.1336 - val_loss: 1.3332 - val_mean_absolute_error: 1.3332\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.42797 to 1.33325, saving model to Weights-009--1.33325.hdf5\n",
            "Epoch 10/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 1.0262 - mean_absolute_error: 1.0262 - val_loss: 1.4231 - val_mean_absolute_error: 1.4231\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.33325\n",
            "Epoch 11/500\n",
            "960/960 [==============================] - 0s 201us/step - loss: 1.0347 - mean_absolute_error: 1.0347 - val_loss: 1.4917 - val_mean_absolute_error: 1.4917\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.33325\n",
            "Epoch 12/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.9620 - mean_absolute_error: 0.9620 - val_loss: 1.2405 - val_mean_absolute_error: 1.2405\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.33325 to 1.24048, saving model to Weights-012--1.24048.hdf5\n",
            "Epoch 13/500\n",
            "960/960 [==============================] - 0s 192us/step - loss: 0.8851 - mean_absolute_error: 0.8851 - val_loss: 1.3883 - val_mean_absolute_error: 1.3883\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.24048\n",
            "Epoch 14/500\n",
            "960/960 [==============================] - 0s 174us/step - loss: 0.9002 - mean_absolute_error: 0.9002 - val_loss: 1.3737 - val_mean_absolute_error: 1.3737\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.24048\n",
            "Epoch 15/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.9334 - mean_absolute_error: 0.9334 - val_loss: 1.6808 - val_mean_absolute_error: 1.6808\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.24048\n",
            "Epoch 16/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.9972 - mean_absolute_error: 0.9972 - val_loss: 1.2420 - val_mean_absolute_error: 1.2420\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.24048\n",
            "Epoch 17/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.9343 - mean_absolute_error: 0.9343 - val_loss: 2.0883 - val_mean_absolute_error: 2.0883\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.24048\n",
            "Epoch 18/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.9629 - mean_absolute_error: 0.9629 - val_loss: 1.2470 - val_mean_absolute_error: 1.2470\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.24048\n",
            "Epoch 19/500\n",
            "960/960 [==============================] - 0s 176us/step - loss: 0.8657 - mean_absolute_error: 0.8657 - val_loss: 1.2347 - val_mean_absolute_error: 1.2347\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.24048 to 1.23467, saving model to Weights-019--1.23467.hdf5\n",
            "Epoch 20/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.8034 - mean_absolute_error: 0.8034 - val_loss: 1.2265 - val_mean_absolute_error: 1.2265\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.23467 to 1.22652, saving model to Weights-020--1.22652.hdf5\n",
            "Epoch 21/500\n",
            "960/960 [==============================] - 0s 193us/step - loss: 0.7963 - mean_absolute_error: 0.7963 - val_loss: 1.2675 - val_mean_absolute_error: 1.2675\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.22652\n",
            "Epoch 22/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.8007 - mean_absolute_error: 0.8007 - val_loss: 1.4626 - val_mean_absolute_error: 1.4626\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.22652\n",
            "Epoch 23/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.8258 - mean_absolute_error: 0.8258 - val_loss: 1.2598 - val_mean_absolute_error: 1.2598\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.22652\n",
            "Epoch 24/500\n",
            "960/960 [==============================] - 0s 154us/step - loss: 0.7578 - mean_absolute_error: 0.7578 - val_loss: 1.2633 - val_mean_absolute_error: 1.2633\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.22652\n",
            "Epoch 25/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.7469 - mean_absolute_error: 0.7469 - val_loss: 1.3092 - val_mean_absolute_error: 1.3092\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.22652\n",
            "Epoch 26/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.7646 - mean_absolute_error: 0.7646 - val_loss: 1.5175 - val_mean_absolute_error: 1.5175\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.22652\n",
            "Epoch 27/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.7482 - mean_absolute_error: 0.7482 - val_loss: 1.2706 - val_mean_absolute_error: 1.2706\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.22652\n",
            "Epoch 28/500\n",
            "960/960 [==============================] - 0s 181us/step - loss: 0.7241 - mean_absolute_error: 0.7241 - val_loss: 1.2039 - val_mean_absolute_error: 1.2039\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.22652 to 1.20386, saving model to Weights-028--1.20386.hdf5\n",
            "Epoch 29/500\n",
            "960/960 [==============================] - 0s 195us/step - loss: 0.7452 - mean_absolute_error: 0.7452 - val_loss: 1.2697 - val_mean_absolute_error: 1.2697\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.20386\n",
            "Epoch 30/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.7689 - mean_absolute_error: 0.7689 - val_loss: 1.3986 - val_mean_absolute_error: 1.3986\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.20386\n",
            "Epoch 31/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.7671 - mean_absolute_error: 0.7671 - val_loss: 1.1494 - val_mean_absolute_error: 1.1494\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.20386 to 1.14942, saving model to Weights-031--1.14942.hdf5\n",
            "Epoch 32/500\n",
            "960/960 [==============================] - 0s 183us/step - loss: 0.7181 - mean_absolute_error: 0.7181 - val_loss: 1.2367 - val_mean_absolute_error: 1.2367\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.14942\n",
            "Epoch 33/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.7079 - mean_absolute_error: 0.7079 - val_loss: 1.4142 - val_mean_absolute_error: 1.4142\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.14942\n",
            "Epoch 34/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.6835 - mean_absolute_error: 0.6835 - val_loss: 1.4962 - val_mean_absolute_error: 1.4962\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.14942\n",
            "Epoch 35/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.7212 - mean_absolute_error: 0.7212 - val_loss: 1.2020 - val_mean_absolute_error: 1.2020\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.14942\n",
            "Epoch 36/500\n",
            "960/960 [==============================] - 0s 148us/step - loss: 0.7046 - mean_absolute_error: 0.7046 - val_loss: 1.2348 - val_mean_absolute_error: 1.2348\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.14942\n",
            "Epoch 37/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.6647 - mean_absolute_error: 0.6647 - val_loss: 1.3276 - val_mean_absolute_error: 1.3276\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.14942\n",
            "Epoch 38/500\n",
            "960/960 [==============================] - 0s 176us/step - loss: 0.6648 - mean_absolute_error: 0.6648 - val_loss: 1.2500 - val_mean_absolute_error: 1.2500\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.14942\n",
            "Epoch 39/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.6981 - mean_absolute_error: 0.6981 - val_loss: 1.2735 - val_mean_absolute_error: 1.2735\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.14942\n",
            "Epoch 40/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.6492 - mean_absolute_error: 0.6492 - val_loss: 1.6103 - val_mean_absolute_error: 1.6103\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.14942\n",
            "Epoch 41/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.6656 - mean_absolute_error: 0.6656 - val_loss: 1.3042 - val_mean_absolute_error: 1.3042\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.14942\n",
            "Epoch 42/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.6615 - mean_absolute_error: 0.6615 - val_loss: 1.2784 - val_mean_absolute_error: 1.2784\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.14942\n",
            "Epoch 43/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.6862 - mean_absolute_error: 0.6862 - val_loss: 1.3805 - val_mean_absolute_error: 1.3805\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.14942\n",
            "Epoch 44/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.6798 - mean_absolute_error: 0.6798 - val_loss: 1.6327 - val_mean_absolute_error: 1.6327\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.14942\n",
            "Epoch 45/500\n",
            "960/960 [==============================] - 0s 196us/step - loss: 0.6794 - mean_absolute_error: 0.6794 - val_loss: 1.4345 - val_mean_absolute_error: 1.4345\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.14942\n",
            "Epoch 46/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.6484 - mean_absolute_error: 0.6484 - val_loss: 1.3069 - val_mean_absolute_error: 1.3069\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.14942\n",
            "Epoch 47/500\n",
            "960/960 [==============================] - 0s 155us/step - loss: 0.6116 - mean_absolute_error: 0.6116 - val_loss: 1.2450 - val_mean_absolute_error: 1.2450\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.14942\n",
            "Epoch 48/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.6479 - mean_absolute_error: 0.6479 - val_loss: 1.6711 - val_mean_absolute_error: 1.6711\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.14942\n",
            "Epoch 49/500\n",
            "960/960 [==============================] - 0s 152us/step - loss: 0.6865 - mean_absolute_error: 0.6865 - val_loss: 1.1902 - val_mean_absolute_error: 1.1902\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.14942\n",
            "Epoch 50/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.6634 - mean_absolute_error: 0.6634 - val_loss: 1.4561 - val_mean_absolute_error: 1.4561\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.14942\n",
            "Epoch 51/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.6336 - mean_absolute_error: 0.6336 - val_loss: 1.3152 - val_mean_absolute_error: 1.3152\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.14942\n",
            "Epoch 52/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.6415 - mean_absolute_error: 0.6415 - val_loss: 1.3810 - val_mean_absolute_error: 1.3810\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.14942\n",
            "Epoch 53/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.6087 - mean_absolute_error: 0.6087 - val_loss: 1.6148 - val_mean_absolute_error: 1.6148\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.14942\n",
            "Epoch 54/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.6169 - mean_absolute_error: 0.6169 - val_loss: 1.3469 - val_mean_absolute_error: 1.3469\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.14942\n",
            "Epoch 55/500\n",
            "960/960 [==============================] - 0s 154us/step - loss: 0.5931 - mean_absolute_error: 0.5931 - val_loss: 1.2582 - val_mean_absolute_error: 1.2582\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.14942\n",
            "Epoch 56/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.5756 - mean_absolute_error: 0.5756 - val_loss: 1.4478 - val_mean_absolute_error: 1.4478\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.14942\n",
            "Epoch 57/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.5892 - mean_absolute_error: 0.5892 - val_loss: 1.4354 - val_mean_absolute_error: 1.4354\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.14942\n",
            "Epoch 58/500\n",
            "960/960 [==============================] - 0s 176us/step - loss: 0.5749 - mean_absolute_error: 0.5749 - val_loss: 1.4146 - val_mean_absolute_error: 1.4146\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.14942\n",
            "Epoch 59/500\n",
            "960/960 [==============================] - 0s 189us/step - loss: 0.6287 - mean_absolute_error: 0.6287 - val_loss: 1.4916 - val_mean_absolute_error: 1.4916\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.14942\n",
            "Epoch 60/500\n",
            "960/960 [==============================] - 0s 211us/step - loss: 0.6199 - mean_absolute_error: 0.6199 - val_loss: 1.3646 - val_mean_absolute_error: 1.3646\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.14942\n",
            "Epoch 61/500\n",
            "960/960 [==============================] - 0s 192us/step - loss: 0.5720 - mean_absolute_error: 0.5720 - val_loss: 1.6591 - val_mean_absolute_error: 1.6591\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.14942\n",
            "Epoch 62/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.5910 - mean_absolute_error: 0.5910 - val_loss: 1.3287 - val_mean_absolute_error: 1.3287\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.14942\n",
            "Epoch 63/500\n",
            "960/960 [==============================] - 0s 179us/step - loss: 0.6088 - mean_absolute_error: 0.6088 - val_loss: 1.5391 - val_mean_absolute_error: 1.5391\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.14942\n",
            "Epoch 64/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.5756 - mean_absolute_error: 0.5756 - val_loss: 1.3685 - val_mean_absolute_error: 1.3685\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.14942\n",
            "Epoch 65/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.5592 - mean_absolute_error: 0.5592 - val_loss: 1.3807 - val_mean_absolute_error: 1.3807\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.14942\n",
            "Epoch 66/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.5982 - mean_absolute_error: 0.5982 - val_loss: 1.6107 - val_mean_absolute_error: 1.6107\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.14942\n",
            "Epoch 67/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.6115 - mean_absolute_error: 0.6115 - val_loss: 1.4059 - val_mean_absolute_error: 1.4059\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.14942\n",
            "Epoch 68/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.5702 - mean_absolute_error: 0.5702 - val_loss: 1.4700 - val_mean_absolute_error: 1.4700\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.14942\n",
            "Epoch 69/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.5839 - mean_absolute_error: 0.5839 - val_loss: 1.3882 - val_mean_absolute_error: 1.3882\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.14942\n",
            "Epoch 70/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.5471 - mean_absolute_error: 0.5471 - val_loss: 1.2282 - val_mean_absolute_error: 1.2282\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.14942\n",
            "Epoch 71/500\n",
            "960/960 [==============================] - 0s 196us/step - loss: 0.5616 - mean_absolute_error: 0.5616 - val_loss: 1.4055 - val_mean_absolute_error: 1.4055\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.14942\n",
            "Epoch 72/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.6284 - mean_absolute_error: 0.6284 - val_loss: 1.3507 - val_mean_absolute_error: 1.3507\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.14942\n",
            "Epoch 73/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.6044 - mean_absolute_error: 0.6044 - val_loss: 1.4286 - val_mean_absolute_error: 1.4286\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.14942\n",
            "Epoch 74/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.5602 - mean_absolute_error: 0.5602 - val_loss: 1.3373 - val_mean_absolute_error: 1.3373\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.14942\n",
            "Epoch 75/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.5683 - mean_absolute_error: 0.5683 - val_loss: 1.3626 - val_mean_absolute_error: 1.3626\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.14942\n",
            "Epoch 76/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.5521 - mean_absolute_error: 0.5521 - val_loss: 1.3448 - val_mean_absolute_error: 1.3448\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.14942\n",
            "Epoch 77/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.5537 - mean_absolute_error: 0.5537 - val_loss: 1.3667 - val_mean_absolute_error: 1.3667\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.14942\n",
            "Epoch 78/500\n",
            "960/960 [==============================] - 0s 154us/step - loss: 0.5289 - mean_absolute_error: 0.5289 - val_loss: 1.2494 - val_mean_absolute_error: 1.2494\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.14942\n",
            "Epoch 79/500\n",
            "960/960 [==============================] - 0s 150us/step - loss: 0.5745 - mean_absolute_error: 0.5745 - val_loss: 1.3590 - val_mean_absolute_error: 1.3590\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.14942\n",
            "Epoch 80/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.5598 - mean_absolute_error: 0.5598 - val_loss: 1.4208 - val_mean_absolute_error: 1.4208\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.14942\n",
            "Epoch 81/500\n",
            "960/960 [==============================] - 0s 155us/step - loss: 0.5497 - mean_absolute_error: 0.5497 - val_loss: 1.3322 - val_mean_absolute_error: 1.3322\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.14942\n",
            "Epoch 82/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.5694 - mean_absolute_error: 0.5694 - val_loss: 1.4000 - val_mean_absolute_error: 1.4000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.14942\n",
            "Epoch 83/500\n",
            "960/960 [==============================] - 0s 154us/step - loss: 0.5511 - mean_absolute_error: 0.5511 - val_loss: 1.3827 - val_mean_absolute_error: 1.3827\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.14942\n",
            "Epoch 84/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.5292 - mean_absolute_error: 0.5292 - val_loss: 1.3804 - val_mean_absolute_error: 1.3804\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.14942\n",
            "Epoch 85/500\n",
            "960/960 [==============================] - 0s 151us/step - loss: 0.5668 - mean_absolute_error: 0.5668 - val_loss: 1.3074 - val_mean_absolute_error: 1.3074\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.14942\n",
            "Epoch 86/500\n",
            "960/960 [==============================] - 0s 192us/step - loss: 0.5450 - mean_absolute_error: 0.5450 - val_loss: 1.4407 - val_mean_absolute_error: 1.4407\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.14942\n",
            "Epoch 87/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.5483 - mean_absolute_error: 0.5483 - val_loss: 1.3396 - val_mean_absolute_error: 1.3396\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.14942\n",
            "Epoch 88/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.5391 - mean_absolute_error: 0.5391 - val_loss: 1.4849 - val_mean_absolute_error: 1.4849\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.14942\n",
            "Epoch 89/500\n",
            "960/960 [==============================] - 0s 187us/step - loss: 0.5584 - mean_absolute_error: 0.5584 - val_loss: 1.4267 - val_mean_absolute_error: 1.4267\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.14942\n",
            "Epoch 90/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.5252 - mean_absolute_error: 0.5252 - val_loss: 1.4364 - val_mean_absolute_error: 1.4364\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.14942\n",
            "Epoch 91/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.5036 - mean_absolute_error: 0.5036 - val_loss: 1.3261 - val_mean_absolute_error: 1.3261\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.14942\n",
            "Epoch 92/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.5279 - mean_absolute_error: 0.5279 - val_loss: 1.3778 - val_mean_absolute_error: 1.3778\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.14942\n",
            "Epoch 93/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.5274 - mean_absolute_error: 0.5274 - val_loss: 1.4015 - val_mean_absolute_error: 1.4015\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.14942\n",
            "Epoch 94/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.5415 - mean_absolute_error: 0.5415 - val_loss: 1.4023 - val_mean_absolute_error: 1.4023\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.14942\n",
            "Epoch 95/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.5528 - mean_absolute_error: 0.5528 - val_loss: 1.5086 - val_mean_absolute_error: 1.5086\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.14942\n",
            "Epoch 96/500\n",
            "960/960 [==============================] - 0s 191us/step - loss: 0.5276 - mean_absolute_error: 0.5276 - val_loss: 1.3734 - val_mean_absolute_error: 1.3734\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.14942\n",
            "Epoch 97/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.5279 - mean_absolute_error: 0.5279 - val_loss: 1.5041 - val_mean_absolute_error: 1.5041\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.14942\n",
            "Epoch 98/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.5196 - mean_absolute_error: 0.5196 - val_loss: 1.3927 - val_mean_absolute_error: 1.3927\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.14942\n",
            "Epoch 99/500\n",
            "960/960 [==============================] - 0s 182us/step - loss: 0.5309 - mean_absolute_error: 0.5309 - val_loss: 1.5004 - val_mean_absolute_error: 1.5004\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.14942\n",
            "Epoch 100/500\n",
            "960/960 [==============================] - 0s 177us/step - loss: 0.5000 - mean_absolute_error: 0.5000 - val_loss: 1.3676 - val_mean_absolute_error: 1.3676\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.14942\n",
            "Epoch 101/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.4987 - mean_absolute_error: 0.4987 - val_loss: 1.4347 - val_mean_absolute_error: 1.4347\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 1.14942\n",
            "Epoch 102/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.5112 - mean_absolute_error: 0.5112 - val_loss: 1.3682 - val_mean_absolute_error: 1.3682\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 1.14942\n",
            "Epoch 103/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.5231 - mean_absolute_error: 0.5231 - val_loss: 1.5468 - val_mean_absolute_error: 1.5468\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 1.14942\n",
            "Epoch 104/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.5040 - mean_absolute_error: 0.5040 - val_loss: 1.3992 - val_mean_absolute_error: 1.3992\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 1.14942\n",
            "Epoch 105/500\n",
            "960/960 [==============================] - 0s 194us/step - loss: 0.4841 - mean_absolute_error: 0.4841 - val_loss: 1.4887 - val_mean_absolute_error: 1.4887\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 1.14942\n",
            "Epoch 106/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.4980 - mean_absolute_error: 0.4980 - val_loss: 1.4782 - val_mean_absolute_error: 1.4782\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 1.14942\n",
            "Epoch 107/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.5015 - mean_absolute_error: 0.5015 - val_loss: 1.5006 - val_mean_absolute_error: 1.5006\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 1.14942\n",
            "Epoch 108/500\n",
            "960/960 [==============================] - 0s 174us/step - loss: 0.5223 - mean_absolute_error: 0.5223 - val_loss: 1.4856 - val_mean_absolute_error: 1.4856\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 1.14942\n",
            "Epoch 109/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.4977 - mean_absolute_error: 0.4977 - val_loss: 1.3346 - val_mean_absolute_error: 1.3346\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 1.14942\n",
            "Epoch 110/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.5120 - mean_absolute_error: 0.5120 - val_loss: 1.5381 - val_mean_absolute_error: 1.5381\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 1.14942\n",
            "Epoch 111/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.5013 - mean_absolute_error: 0.5013 - val_loss: 1.3640 - val_mean_absolute_error: 1.3640\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 1.14942\n",
            "Epoch 112/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.4819 - mean_absolute_error: 0.4819 - val_loss: 1.5048 - val_mean_absolute_error: 1.5048\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 1.14942\n",
            "Epoch 113/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.4556 - mean_absolute_error: 0.4556 - val_loss: 1.3837 - val_mean_absolute_error: 1.3837\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 1.14942\n",
            "Epoch 114/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.5002 - mean_absolute_error: 0.5002 - val_loss: 1.3396 - val_mean_absolute_error: 1.3396\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 1.14942\n",
            "Epoch 115/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.4943 - mean_absolute_error: 0.4943 - val_loss: 1.5296 - val_mean_absolute_error: 1.5296\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 1.14942\n",
            "Epoch 116/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.4704 - mean_absolute_error: 0.4704 - val_loss: 1.4662 - val_mean_absolute_error: 1.4662\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 1.14942\n",
            "Epoch 117/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.4688 - mean_absolute_error: 0.4688 - val_loss: 1.3423 - val_mean_absolute_error: 1.3423\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 1.14942\n",
            "Epoch 118/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.4792 - mean_absolute_error: 0.4792 - val_loss: 1.3548 - val_mean_absolute_error: 1.3548\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 1.14942\n",
            "Epoch 119/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.4933 - mean_absolute_error: 0.4933 - val_loss: 1.5085 - val_mean_absolute_error: 1.5085\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 1.14942\n",
            "Epoch 120/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.4750 - mean_absolute_error: 0.4750 - val_loss: 1.3768 - val_mean_absolute_error: 1.3768\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 1.14942\n",
            "Epoch 121/500\n",
            "960/960 [==============================] - 0s 174us/step - loss: 0.4666 - mean_absolute_error: 0.4666 - val_loss: 1.4586 - val_mean_absolute_error: 1.4586\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 1.14942\n",
            "Epoch 122/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.4669 - mean_absolute_error: 0.4669 - val_loss: 1.5296 - val_mean_absolute_error: 1.5296\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 1.14942\n",
            "Epoch 123/500\n",
            "960/960 [==============================] - 0s 180us/step - loss: 0.4914 - mean_absolute_error: 0.4914 - val_loss: 1.3733 - val_mean_absolute_error: 1.3733\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 1.14942\n",
            "Epoch 124/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.4953 - mean_absolute_error: 0.4953 - val_loss: 1.4468 - val_mean_absolute_error: 1.4468\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 1.14942\n",
            "Epoch 125/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.4739 - mean_absolute_error: 0.4739 - val_loss: 1.4404 - val_mean_absolute_error: 1.4404\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 1.14942\n",
            "Epoch 126/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.4918 - mean_absolute_error: 0.4918 - val_loss: 1.3875 - val_mean_absolute_error: 1.3875\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 1.14942\n",
            "Epoch 127/500\n",
            "960/960 [==============================] - 0s 183us/step - loss: 0.5190 - mean_absolute_error: 0.5190 - val_loss: 1.2525 - val_mean_absolute_error: 1.2525\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 1.14942\n",
            "Epoch 128/500\n",
            "960/960 [==============================] - 0s 180us/step - loss: 0.4899 - mean_absolute_error: 0.4899 - val_loss: 1.4002 - val_mean_absolute_error: 1.4002\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 1.14942\n",
            "Epoch 129/500\n",
            "960/960 [==============================] - 0s 189us/step - loss: 0.4929 - mean_absolute_error: 0.4929 - val_loss: 1.4389 - val_mean_absolute_error: 1.4389\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 1.14942\n",
            "Epoch 130/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.4724 - mean_absolute_error: 0.4724 - val_loss: 1.4270 - val_mean_absolute_error: 1.4270\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 1.14942\n",
            "Epoch 131/500\n",
            "960/960 [==============================] - 0s 174us/step - loss: 0.4524 - mean_absolute_error: 0.4524 - val_loss: 1.4349 - val_mean_absolute_error: 1.4349\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 1.14942\n",
            "Epoch 132/500\n",
            "960/960 [==============================] - 0s 177us/step - loss: 0.4390 - mean_absolute_error: 0.4390 - val_loss: 1.4432 - val_mean_absolute_error: 1.4432\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 1.14942\n",
            "Epoch 133/500\n",
            "960/960 [==============================] - 0s 187us/step - loss: 0.4487 - mean_absolute_error: 0.4487 - val_loss: 1.4466 - val_mean_absolute_error: 1.4466\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 1.14942\n",
            "Epoch 134/500\n",
            "960/960 [==============================] - 0s 174us/step - loss: 0.4428 - mean_absolute_error: 0.4428 - val_loss: 1.3998 - val_mean_absolute_error: 1.3998\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 1.14942\n",
            "Epoch 135/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.4543 - mean_absolute_error: 0.4543 - val_loss: 1.3869 - val_mean_absolute_error: 1.3869\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 1.14942\n",
            "Epoch 136/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.4542 - mean_absolute_error: 0.4542 - val_loss: 1.3924 - val_mean_absolute_error: 1.3924\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 1.14942\n",
            "Epoch 137/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.4860 - mean_absolute_error: 0.4860 - val_loss: 1.3494 - val_mean_absolute_error: 1.3494\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 1.14942\n",
            "Epoch 138/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.4984 - mean_absolute_error: 0.4984 - val_loss: 1.3981 - val_mean_absolute_error: 1.3981\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 1.14942\n",
            "Epoch 139/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.4774 - mean_absolute_error: 0.4774 - val_loss: 1.3914 - val_mean_absolute_error: 1.3914\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 1.14942\n",
            "Epoch 140/500\n",
            "960/960 [==============================] - 0s 183us/step - loss: 0.4701 - mean_absolute_error: 0.4701 - val_loss: 1.4420 - val_mean_absolute_error: 1.4420\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 1.14942\n",
            "Epoch 141/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.4633 - mean_absolute_error: 0.4633 - val_loss: 1.3922 - val_mean_absolute_error: 1.3922\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 1.14942\n",
            "Epoch 142/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.4421 - mean_absolute_error: 0.4421 - val_loss: 1.4664 - val_mean_absolute_error: 1.4664\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 1.14942\n",
            "Epoch 143/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.5007 - mean_absolute_error: 0.5007 - val_loss: 1.5481 - val_mean_absolute_error: 1.5481\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 1.14942\n",
            "Epoch 144/500\n",
            "960/960 [==============================] - 0s 178us/step - loss: 0.4922 - mean_absolute_error: 0.4922 - val_loss: 1.4197 - val_mean_absolute_error: 1.4197\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 1.14942\n",
            "Epoch 145/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.4800 - mean_absolute_error: 0.4800 - val_loss: 1.4665 - val_mean_absolute_error: 1.4665\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 1.14942\n",
            "Epoch 146/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.4720 - mean_absolute_error: 0.4720 - val_loss: 1.3781 - val_mean_absolute_error: 1.3781\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 1.14942\n",
            "Epoch 147/500\n",
            "960/960 [==============================] - 0s 178us/step - loss: 0.4652 - mean_absolute_error: 0.4652 - val_loss: 1.3449 - val_mean_absolute_error: 1.3449\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 1.14942\n",
            "Epoch 148/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.4457 - mean_absolute_error: 0.4457 - val_loss: 1.3995 - val_mean_absolute_error: 1.3995\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 1.14942\n",
            "Epoch 149/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.4905 - mean_absolute_error: 0.4905 - val_loss: 1.4003 - val_mean_absolute_error: 1.4003\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 1.14942\n",
            "Epoch 150/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.4921 - mean_absolute_error: 0.4921 - val_loss: 1.3590 - val_mean_absolute_error: 1.3590\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 1.14942\n",
            "Epoch 151/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.4412 - mean_absolute_error: 0.4412 - val_loss: 1.3474 - val_mean_absolute_error: 1.3474\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 1.14942\n",
            "Epoch 152/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.4295 - mean_absolute_error: 0.4295 - val_loss: 1.3635 - val_mean_absolute_error: 1.3635\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 1.14942\n",
            "Epoch 153/500\n",
            "960/960 [==============================] - 0s 179us/step - loss: 0.4628 - mean_absolute_error: 0.4628 - val_loss: 1.4094 - val_mean_absolute_error: 1.4094\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 1.14942\n",
            "Epoch 154/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.4572 - mean_absolute_error: 0.4572 - val_loss: 1.4594 - val_mean_absolute_error: 1.4594\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 1.14942\n",
            "Epoch 155/500\n",
            "960/960 [==============================] - 0s 178us/step - loss: 0.4468 - mean_absolute_error: 0.4468 - val_loss: 1.3194 - val_mean_absolute_error: 1.3194\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 1.14942\n",
            "Epoch 156/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.4756 - mean_absolute_error: 0.4756 - val_loss: 1.4607 - val_mean_absolute_error: 1.4607\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 1.14942\n",
            "Epoch 157/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.4890 - mean_absolute_error: 0.4890 - val_loss: 1.3486 - val_mean_absolute_error: 1.3486\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 1.14942\n",
            "Epoch 158/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.4566 - mean_absolute_error: 0.4566 - val_loss: 1.3643 - val_mean_absolute_error: 1.3643\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 1.14942\n",
            "Epoch 159/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.5111 - mean_absolute_error: 0.5111 - val_loss: 1.4214 - val_mean_absolute_error: 1.4214\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 1.14942\n",
            "Epoch 160/500\n",
            "960/960 [==============================] - 0s 178us/step - loss: 0.4588 - mean_absolute_error: 0.4588 - val_loss: 1.4844 - val_mean_absolute_error: 1.4844\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 1.14942\n",
            "Epoch 161/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.4525 - mean_absolute_error: 0.4525 - val_loss: 1.4909 - val_mean_absolute_error: 1.4909\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 1.14942\n",
            "Epoch 162/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.4454 - mean_absolute_error: 0.4454 - val_loss: 1.5541 - val_mean_absolute_error: 1.5541\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 1.14942\n",
            "Epoch 163/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.4356 - mean_absolute_error: 0.4356 - val_loss: 1.4397 - val_mean_absolute_error: 1.4397\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 1.14942\n",
            "Epoch 164/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.4407 - mean_absolute_error: 0.4407 - val_loss: 1.3960 - val_mean_absolute_error: 1.3960\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 1.14942\n",
            "Epoch 165/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.4770 - mean_absolute_error: 0.4770 - val_loss: 1.4829 - val_mean_absolute_error: 1.4829\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 1.14942\n",
            "Epoch 166/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.4300 - mean_absolute_error: 0.4300 - val_loss: 1.3696 - val_mean_absolute_error: 1.3696\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 1.14942\n",
            "Epoch 167/500\n",
            "960/960 [==============================] - 0s 184us/step - loss: 0.4350 - mean_absolute_error: 0.4350 - val_loss: 1.3532 - val_mean_absolute_error: 1.3532\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 1.14942\n",
            "Epoch 168/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.4304 - mean_absolute_error: 0.4304 - val_loss: 1.5370 - val_mean_absolute_error: 1.5370\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 1.14942\n",
            "Epoch 169/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.4451 - mean_absolute_error: 0.4451 - val_loss: 1.5833 - val_mean_absolute_error: 1.5833\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 1.14942\n",
            "Epoch 170/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.4410 - mean_absolute_error: 0.4410 - val_loss: 1.3774 - val_mean_absolute_error: 1.3774\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 1.14942\n",
            "Epoch 171/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.4295 - mean_absolute_error: 0.4295 - val_loss: 1.4936 - val_mean_absolute_error: 1.4936\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 1.14942\n",
            "Epoch 172/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.4654 - mean_absolute_error: 0.4654 - val_loss: 1.3845 - val_mean_absolute_error: 1.3845\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 1.14942\n",
            "Epoch 173/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.4482 - mean_absolute_error: 0.4482 - val_loss: 1.3089 - val_mean_absolute_error: 1.3089\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 1.14942\n",
            "Epoch 174/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.4110 - mean_absolute_error: 0.4110 - val_loss: 1.3722 - val_mean_absolute_error: 1.3722\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 1.14942\n",
            "Epoch 175/500\n",
            "960/960 [==============================] - 0s 181us/step - loss: 0.4224 - mean_absolute_error: 0.4224 - val_loss: 1.3181 - val_mean_absolute_error: 1.3181\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 1.14942\n",
            "Epoch 176/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.4227 - mean_absolute_error: 0.4227 - val_loss: 1.4485 - val_mean_absolute_error: 1.4485\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 1.14942\n",
            "Epoch 177/500\n",
            "960/960 [==============================] - 0s 241us/step - loss: 0.4169 - mean_absolute_error: 0.4169 - val_loss: 1.4586 - val_mean_absolute_error: 1.4586\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 1.14942\n",
            "Epoch 178/500\n",
            "960/960 [==============================] - 0s 187us/step - loss: 0.4198 - mean_absolute_error: 0.4198 - val_loss: 1.4928 - val_mean_absolute_error: 1.4928\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 1.14942\n",
            "Epoch 179/500\n",
            "960/960 [==============================] - 0s 194us/step - loss: 0.4431 - mean_absolute_error: 0.4431 - val_loss: 1.4183 - val_mean_absolute_error: 1.4183\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 1.14942\n",
            "Epoch 180/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.4194 - mean_absolute_error: 0.4194 - val_loss: 1.4026 - val_mean_absolute_error: 1.4026\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 1.14942\n",
            "Epoch 181/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.4320 - mean_absolute_error: 0.4320 - val_loss: 1.4187 - val_mean_absolute_error: 1.4187\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 1.14942\n",
            "Epoch 182/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.4372 - mean_absolute_error: 0.4372 - val_loss: 1.3760 - val_mean_absolute_error: 1.3760\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 1.14942\n",
            "Epoch 183/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.4181 - mean_absolute_error: 0.4181 - val_loss: 1.3333 - val_mean_absolute_error: 1.3333\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 1.14942\n",
            "Epoch 184/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.4078 - mean_absolute_error: 0.4078 - val_loss: 1.4696 - val_mean_absolute_error: 1.4696\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 1.14942\n",
            "Epoch 185/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.4005 - mean_absolute_error: 0.4005 - val_loss: 1.3895 - val_mean_absolute_error: 1.3895\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 1.14942\n",
            "Epoch 186/500\n",
            "960/960 [==============================] - 0s 155us/step - loss: 0.3894 - mean_absolute_error: 0.3894 - val_loss: 1.5312 - val_mean_absolute_error: 1.5312\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 1.14942\n",
            "Epoch 187/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.4230 - mean_absolute_error: 0.4230 - val_loss: 1.4075 - val_mean_absolute_error: 1.4075\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 1.14942\n",
            "Epoch 188/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.3923 - mean_absolute_error: 0.3923 - val_loss: 1.4993 - val_mean_absolute_error: 1.4993\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 1.14942\n",
            "Epoch 189/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.4215 - mean_absolute_error: 0.4215 - val_loss: 1.3390 - val_mean_absolute_error: 1.3390\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 1.14942\n",
            "Epoch 190/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.4351 - mean_absolute_error: 0.4351 - val_loss: 1.4868 - val_mean_absolute_error: 1.4868\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 1.14942\n",
            "Epoch 191/500\n",
            "960/960 [==============================] - 0s 182us/step - loss: 0.4051 - mean_absolute_error: 0.4051 - val_loss: 1.5506 - val_mean_absolute_error: 1.5506\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 1.14942\n",
            "Epoch 192/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.3930 - mean_absolute_error: 0.3930 - val_loss: 1.4277 - val_mean_absolute_error: 1.4277\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 1.14942\n",
            "Epoch 193/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.3922 - mean_absolute_error: 0.3922 - val_loss: 1.3350 - val_mean_absolute_error: 1.3350\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 1.14942\n",
            "Epoch 194/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.3975 - mean_absolute_error: 0.3975 - val_loss: 1.4695 - val_mean_absolute_error: 1.4695\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 1.14942\n",
            "Epoch 195/500\n",
            "960/960 [==============================] - 0s 154us/step - loss: 0.4050 - mean_absolute_error: 0.4050 - val_loss: 1.3646 - val_mean_absolute_error: 1.3646\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 1.14942\n",
            "Epoch 196/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.4121 - mean_absolute_error: 0.4121 - val_loss: 1.4500 - val_mean_absolute_error: 1.4500\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 1.14942\n",
            "Epoch 197/500\n",
            "960/960 [==============================] - 0s 154us/step - loss: 0.3934 - mean_absolute_error: 0.3934 - val_loss: 1.3822 - val_mean_absolute_error: 1.3822\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 1.14942\n",
            "Epoch 198/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.3918 - mean_absolute_error: 0.3918 - val_loss: 1.3257 - val_mean_absolute_error: 1.3257\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 1.14942\n",
            "Epoch 199/500\n",
            "960/960 [==============================] - 0s 190us/step - loss: 0.4015 - mean_absolute_error: 0.4015 - val_loss: 1.3441 - val_mean_absolute_error: 1.3441\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 1.14942\n",
            "Epoch 200/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.4001 - mean_absolute_error: 0.4001 - val_loss: 1.3528 - val_mean_absolute_error: 1.3528\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 1.14942\n",
            "Epoch 201/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.3964 - mean_absolute_error: 0.3964 - val_loss: 1.4600 - val_mean_absolute_error: 1.4600\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 1.14942\n",
            "Epoch 202/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.4289 - mean_absolute_error: 0.4289 - val_loss: 1.3795 - val_mean_absolute_error: 1.3795\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 1.14942\n",
            "Epoch 203/500\n",
            "960/960 [==============================] - 0s 179us/step - loss: 0.4378 - mean_absolute_error: 0.4378 - val_loss: 1.3297 - val_mean_absolute_error: 1.3297\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 1.14942\n",
            "Epoch 204/500\n",
            "960/960 [==============================] - 0s 183us/step - loss: 0.4127 - mean_absolute_error: 0.4127 - val_loss: 1.2901 - val_mean_absolute_error: 1.2901\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 1.14942\n",
            "Epoch 205/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.3928 - mean_absolute_error: 0.3928 - val_loss: 1.3016 - val_mean_absolute_error: 1.3016\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 1.14942\n",
            "Epoch 206/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.3757 - mean_absolute_error: 0.3757 - val_loss: 1.3599 - val_mean_absolute_error: 1.3599\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 1.14942\n",
            "Epoch 207/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.3750 - mean_absolute_error: 0.3750 - val_loss: 1.3119 - val_mean_absolute_error: 1.3119\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 1.14942\n",
            "Epoch 208/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.3846 - mean_absolute_error: 0.3846 - val_loss: 1.3559 - val_mean_absolute_error: 1.3559\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 1.14942\n",
            "Epoch 209/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.3903 - mean_absolute_error: 0.3903 - val_loss: 1.3367 - val_mean_absolute_error: 1.3367\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 1.14942\n",
            "Epoch 210/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.3841 - mean_absolute_error: 0.3841 - val_loss: 1.3892 - val_mean_absolute_error: 1.3892\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 1.14942\n",
            "Epoch 211/500\n",
            "960/960 [==============================] - 0s 177us/step - loss: 0.3983 - mean_absolute_error: 0.3983 - val_loss: 1.3728 - val_mean_absolute_error: 1.3728\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 1.14942\n",
            "Epoch 212/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.3865 - mean_absolute_error: 0.3865 - val_loss: 1.2795 - val_mean_absolute_error: 1.2795\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 1.14942\n",
            "Epoch 213/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.3688 - mean_absolute_error: 0.3688 - val_loss: 1.4326 - val_mean_absolute_error: 1.4326\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 1.14942\n",
            "Epoch 214/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.3986 - mean_absolute_error: 0.3986 - val_loss: 1.3489 - val_mean_absolute_error: 1.3489\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 1.14942\n",
            "Epoch 215/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.3975 - mean_absolute_error: 0.3975 - val_loss: 1.3070 - val_mean_absolute_error: 1.3070\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 1.14942\n",
            "Epoch 216/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.3989 - mean_absolute_error: 0.3989 - val_loss: 1.3510 - val_mean_absolute_error: 1.3510\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 1.14942\n",
            "Epoch 217/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.3957 - mean_absolute_error: 0.3957 - val_loss: 1.4029 - val_mean_absolute_error: 1.4029\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 1.14942\n",
            "Epoch 218/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.3670 - mean_absolute_error: 0.3670 - val_loss: 1.3859 - val_mean_absolute_error: 1.3859\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 1.14942\n",
            "Epoch 219/500\n",
            "960/960 [==============================] - 0s 179us/step - loss: 0.3743 - mean_absolute_error: 0.3743 - val_loss: 1.3626 - val_mean_absolute_error: 1.3626\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 1.14942\n",
            "Epoch 220/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.3780 - mean_absolute_error: 0.3780 - val_loss: 1.3066 - val_mean_absolute_error: 1.3066\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 1.14942\n",
            "Epoch 221/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.3570 - mean_absolute_error: 0.3570 - val_loss: 1.2921 - val_mean_absolute_error: 1.2921\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 1.14942\n",
            "Epoch 222/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.3640 - mean_absolute_error: 0.3640 - val_loss: 1.2888 - val_mean_absolute_error: 1.2888\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 1.14942\n",
            "Epoch 223/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.4077 - mean_absolute_error: 0.4077 - val_loss: 1.3875 - val_mean_absolute_error: 1.3875\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 1.14942\n",
            "Epoch 224/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.3758 - mean_absolute_error: 0.3758 - val_loss: 1.4135 - val_mean_absolute_error: 1.4135\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 1.14942\n",
            "Epoch 225/500\n",
            "960/960 [==============================] - 0s 179us/step - loss: 0.3839 - mean_absolute_error: 0.3839 - val_loss: 1.3994 - val_mean_absolute_error: 1.3994\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 1.14942\n",
            "Epoch 226/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.3995 - mean_absolute_error: 0.3995 - val_loss: 1.3077 - val_mean_absolute_error: 1.3077\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 1.14942\n",
            "Epoch 227/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.4009 - mean_absolute_error: 0.4009 - val_loss: 1.3233 - val_mean_absolute_error: 1.3233\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 1.14942\n",
            "Epoch 228/500\n",
            "960/960 [==============================] - 0s 191us/step - loss: 0.4010 - mean_absolute_error: 0.4010 - val_loss: 1.2086 - val_mean_absolute_error: 1.2086\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 1.14942\n",
            "Epoch 229/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.3697 - mean_absolute_error: 0.3697 - val_loss: 1.3578 - val_mean_absolute_error: 1.3578\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 1.14942\n",
            "Epoch 230/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.3865 - mean_absolute_error: 0.3865 - val_loss: 1.2004 - val_mean_absolute_error: 1.2004\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 1.14942\n",
            "Epoch 231/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.4110 - mean_absolute_error: 0.4110 - val_loss: 1.4282 - val_mean_absolute_error: 1.4282\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 1.14942\n",
            "Epoch 232/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.4071 - mean_absolute_error: 0.4071 - val_loss: 1.3639 - val_mean_absolute_error: 1.3639\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 1.14942\n",
            "Epoch 233/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.4315 - mean_absolute_error: 0.4315 - val_loss: 1.2365 - val_mean_absolute_error: 1.2365\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 1.14942\n",
            "Epoch 234/500\n",
            "960/960 [==============================] - 0s 151us/step - loss: 0.4173 - mean_absolute_error: 0.4173 - val_loss: 1.3058 - val_mean_absolute_error: 1.3058\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 1.14942\n",
            "Epoch 235/500\n",
            "960/960 [==============================] - 0s 155us/step - loss: 0.4506 - mean_absolute_error: 0.4506 - val_loss: 1.2240 - val_mean_absolute_error: 1.2240\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 1.14942\n",
            "Epoch 236/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.4419 - mean_absolute_error: 0.4419 - val_loss: 1.2971 - val_mean_absolute_error: 1.2971\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 1.14942\n",
            "Epoch 237/500\n",
            "960/960 [==============================] - 0s 177us/step - loss: 0.3857 - mean_absolute_error: 0.3857 - val_loss: 1.4502 - val_mean_absolute_error: 1.4502\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 1.14942\n",
            "Epoch 238/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.3885 - mean_absolute_error: 0.3885 - val_loss: 1.2901 - val_mean_absolute_error: 1.2901\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 1.14942\n",
            "Epoch 239/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.3922 - mean_absolute_error: 0.3922 - val_loss: 1.2399 - val_mean_absolute_error: 1.2399\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 1.14942\n",
            "Epoch 240/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.3891 - mean_absolute_error: 0.3891 - val_loss: 1.4205 - val_mean_absolute_error: 1.4205\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 1.14942\n",
            "Epoch 241/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.3958 - mean_absolute_error: 0.3958 - val_loss: 1.3372 - val_mean_absolute_error: 1.3372\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 1.14942\n",
            "Epoch 242/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.3754 - mean_absolute_error: 0.3754 - val_loss: 1.4048 - val_mean_absolute_error: 1.4048\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 1.14942\n",
            "Epoch 243/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.3641 - mean_absolute_error: 0.3641 - val_loss: 1.3085 - val_mean_absolute_error: 1.3085\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 1.14942\n",
            "Epoch 244/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.3924 - mean_absolute_error: 0.3924 - val_loss: 1.3082 - val_mean_absolute_error: 1.3082\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 1.14942\n",
            "Epoch 245/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.3621 - mean_absolute_error: 0.3621 - val_loss: 1.2555 - val_mean_absolute_error: 1.2555\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 1.14942\n",
            "Epoch 246/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.3767 - mean_absolute_error: 0.3767 - val_loss: 1.3386 - val_mean_absolute_error: 1.3386\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 1.14942\n",
            "Epoch 247/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.3639 - mean_absolute_error: 0.3639 - val_loss: 1.2861 - val_mean_absolute_error: 1.2861\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 1.14942\n",
            "Epoch 248/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.3744 - mean_absolute_error: 0.3744 - val_loss: 1.3758 - val_mean_absolute_error: 1.3758\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 1.14942\n",
            "Epoch 249/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.3631 - mean_absolute_error: 0.3631 - val_loss: 1.2912 - val_mean_absolute_error: 1.2912\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 1.14942\n",
            "Epoch 250/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 1.2414 - val_mean_absolute_error: 1.2414\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 1.14942\n",
            "Epoch 251/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.3696 - mean_absolute_error: 0.3696 - val_loss: 1.2793 - val_mean_absolute_error: 1.2793\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 1.14942\n",
            "Epoch 252/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.3662 - mean_absolute_error: 0.3662 - val_loss: 1.3437 - val_mean_absolute_error: 1.3437\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 1.14942\n",
            "Epoch 253/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.3699 - mean_absolute_error: 0.3699 - val_loss: 1.2182 - val_mean_absolute_error: 1.2182\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 1.14942\n",
            "Epoch 254/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.3609 - mean_absolute_error: 0.3609 - val_loss: 1.3290 - val_mean_absolute_error: 1.3290\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 1.14942\n",
            "Epoch 255/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.3929 - mean_absolute_error: 0.3929 - val_loss: 1.1880 - val_mean_absolute_error: 1.1880\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 1.14942\n",
            "Epoch 256/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.3680 - mean_absolute_error: 0.3680 - val_loss: 1.2735 - val_mean_absolute_error: 1.2735\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 1.14942\n",
            "Epoch 257/500\n",
            "960/960 [==============================] - 0s 151us/step - loss: 0.3628 - mean_absolute_error: 0.3628 - val_loss: 1.2093 - val_mean_absolute_error: 1.2093\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 1.14942\n",
            "Epoch 258/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.3728 - mean_absolute_error: 0.3728 - val_loss: 1.2850 - val_mean_absolute_error: 1.2850\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 1.14942\n",
            "Epoch 259/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.3695 - mean_absolute_error: 0.3695 - val_loss: 1.3135 - val_mean_absolute_error: 1.3135\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 1.14942\n",
            "Epoch 260/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.3362 - mean_absolute_error: 0.3362 - val_loss: 1.3379 - val_mean_absolute_error: 1.3379\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 1.14942\n",
            "Epoch 261/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.3563 - mean_absolute_error: 0.3563 - val_loss: 1.4365 - val_mean_absolute_error: 1.4365\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 1.14942\n",
            "Epoch 262/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.3843 - mean_absolute_error: 0.3843 - val_loss: 1.2730 - val_mean_absolute_error: 1.2730\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 1.14942\n",
            "Epoch 263/500\n",
            "960/960 [==============================] - 0s 185us/step - loss: 0.3584 - mean_absolute_error: 0.3584 - val_loss: 1.2039 - val_mean_absolute_error: 1.2039\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 1.14942\n",
            "Epoch 264/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.3393 - mean_absolute_error: 0.3393 - val_loss: 1.3210 - val_mean_absolute_error: 1.3210\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 1.14942\n",
            "Epoch 265/500\n",
            "960/960 [==============================] - 0s 178us/step - loss: 0.3760 - mean_absolute_error: 0.3760 - val_loss: 1.3393 - val_mean_absolute_error: 1.3393\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 1.14942\n",
            "Epoch 266/500\n",
            "960/960 [==============================] - 0s 178us/step - loss: 0.3461 - mean_absolute_error: 0.3461 - val_loss: 1.2416 - val_mean_absolute_error: 1.2416\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 1.14942\n",
            "Epoch 267/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.3320 - mean_absolute_error: 0.3320 - val_loss: 1.1890 - val_mean_absolute_error: 1.1890\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 1.14942\n",
            "Epoch 268/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.3315 - mean_absolute_error: 0.3315 - val_loss: 1.2838 - val_mean_absolute_error: 1.2838\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 1.14942\n",
            "Epoch 269/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.3225 - mean_absolute_error: 0.3225 - val_loss: 1.2357 - val_mean_absolute_error: 1.2357\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 1.14942\n",
            "Epoch 270/500\n",
            "960/960 [==============================] - 0s 177us/step - loss: 0.3613 - mean_absolute_error: 0.3613 - val_loss: 1.3227 - val_mean_absolute_error: 1.3227\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 1.14942\n",
            "Epoch 271/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.3570 - mean_absolute_error: 0.3570 - val_loss: 1.1985 - val_mean_absolute_error: 1.1985\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 1.14942\n",
            "Epoch 272/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.3456 - mean_absolute_error: 0.3456 - val_loss: 1.2428 - val_mean_absolute_error: 1.2428\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 1.14942\n",
            "Epoch 273/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.3541 - mean_absolute_error: 0.3541 - val_loss: 1.3096 - val_mean_absolute_error: 1.3096\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 1.14942\n",
            "Epoch 274/500\n",
            "960/960 [==============================] - 0s 174us/step - loss: 0.3372 - mean_absolute_error: 0.3372 - val_loss: 1.3235 - val_mean_absolute_error: 1.3235\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 1.14942\n",
            "Epoch 275/500\n",
            "960/960 [==============================] - 0s 149us/step - loss: 0.3312 - mean_absolute_error: 0.3312 - val_loss: 1.2896 - val_mean_absolute_error: 1.2896\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 1.14942\n",
            "Epoch 276/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.3541 - mean_absolute_error: 0.3541 - val_loss: 1.3382 - val_mean_absolute_error: 1.3382\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 1.14942\n",
            "Epoch 277/500\n",
            "960/960 [==============================] - 0s 178us/step - loss: 0.3163 - mean_absolute_error: 0.3163 - val_loss: 1.3839 - val_mean_absolute_error: 1.3839\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 1.14942\n",
            "Epoch 278/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.3522 - mean_absolute_error: 0.3522 - val_loss: 1.2673 - val_mean_absolute_error: 1.2673\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 1.14942\n",
            "Epoch 279/500\n",
            "960/960 [==============================] - 0s 182us/step - loss: 0.3938 - mean_absolute_error: 0.3938 - val_loss: 1.2166 - val_mean_absolute_error: 1.2166\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 1.14942\n",
            "Epoch 280/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.4059 - mean_absolute_error: 0.4059 - val_loss: 1.2025 - val_mean_absolute_error: 1.2025\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 1.14942\n",
            "Epoch 281/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.4069 - mean_absolute_error: 0.4069 - val_loss: 1.2030 - val_mean_absolute_error: 1.2030\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 1.14942\n",
            "Epoch 282/500\n",
            "960/960 [==============================] - 0s 174us/step - loss: 0.3876 - mean_absolute_error: 0.3876 - val_loss: 1.1468 - val_mean_absolute_error: 1.1468\n",
            "\n",
            "Epoch 00282: val_loss improved from 1.14942 to 1.14684, saving model to Weights-282--1.14684.hdf5\n",
            "Epoch 283/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.3679 - mean_absolute_error: 0.3679 - val_loss: 1.3208 - val_mean_absolute_error: 1.3208\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 1.14684\n",
            "Epoch 284/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.3503 - mean_absolute_error: 0.3503 - val_loss: 1.2162 - val_mean_absolute_error: 1.2162\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 1.14684\n",
            "Epoch 285/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.3283 - mean_absolute_error: 0.3283 - val_loss: 1.1631 - val_mean_absolute_error: 1.1631\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 1.14684\n",
            "Epoch 286/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.3180 - mean_absolute_error: 0.3180 - val_loss: 1.1990 - val_mean_absolute_error: 1.1990\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 1.14684\n",
            "Epoch 287/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.3185 - mean_absolute_error: 0.3185 - val_loss: 1.1982 - val_mean_absolute_error: 1.1982\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 1.14684\n",
            "Epoch 288/500\n",
            "960/960 [==============================] - 0s 174us/step - loss: 0.3245 - mean_absolute_error: 0.3245 - val_loss: 1.2624 - val_mean_absolute_error: 1.2624\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 1.14684\n",
            "Epoch 289/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.3132 - mean_absolute_error: 0.3132 - val_loss: 1.2005 - val_mean_absolute_error: 1.2005\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 1.14684\n",
            "Epoch 290/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.3294 - mean_absolute_error: 0.3294 - val_loss: 1.2435 - val_mean_absolute_error: 1.2435\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 1.14684\n",
            "Epoch 291/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.3124 - mean_absolute_error: 0.3124 - val_loss: 1.1560 - val_mean_absolute_error: 1.1560\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 1.14684\n",
            "Epoch 292/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.3105 - mean_absolute_error: 0.3105 - val_loss: 1.2765 - val_mean_absolute_error: 1.2765\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 1.14684\n",
            "Epoch 293/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.3396 - mean_absolute_error: 0.3396 - val_loss: 1.2970 - val_mean_absolute_error: 1.2970\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 1.14684\n",
            "Epoch 294/500\n",
            "960/960 [==============================] - 0s 185us/step - loss: 0.3680 - mean_absolute_error: 0.3680 - val_loss: 1.2577 - val_mean_absolute_error: 1.2577\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 1.14684\n",
            "Epoch 295/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.3155 - mean_absolute_error: 0.3155 - val_loss: 1.3078 - val_mean_absolute_error: 1.3078\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 1.14684\n",
            "Epoch 296/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.3130 - mean_absolute_error: 0.3130 - val_loss: 1.1985 - val_mean_absolute_error: 1.1985\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 1.14684\n",
            "Epoch 297/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.3979 - mean_absolute_error: 0.3979 - val_loss: 1.2336 - val_mean_absolute_error: 1.2336\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 1.14684\n",
            "Epoch 298/500\n",
            "960/960 [==============================] - 0s 151us/step - loss: 0.3883 - mean_absolute_error: 0.3883 - val_loss: 1.2135 - val_mean_absolute_error: 1.2135\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 1.14684\n",
            "Epoch 299/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.3464 - mean_absolute_error: 0.3464 - val_loss: 1.2479 - val_mean_absolute_error: 1.2479\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 1.14684\n",
            "Epoch 300/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.3215 - mean_absolute_error: 0.3215 - val_loss: 1.2636 - val_mean_absolute_error: 1.2636\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 1.14684\n",
            "Epoch 301/500\n",
            "960/960 [==============================] - 0s 177us/step - loss: 0.3161 - mean_absolute_error: 0.3161 - val_loss: 1.1620 - val_mean_absolute_error: 1.1620\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 1.14684\n",
            "Epoch 302/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.3186 - mean_absolute_error: 0.3186 - val_loss: 1.2944 - val_mean_absolute_error: 1.2944\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 1.14684\n",
            "Epoch 303/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.3850 - mean_absolute_error: 0.3850 - val_loss: 1.3163 - val_mean_absolute_error: 1.3163\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 1.14684\n",
            "Epoch 304/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.3860 - mean_absolute_error: 0.3860 - val_loss: 1.2838 - val_mean_absolute_error: 1.2838\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 1.14684\n",
            "Epoch 305/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.3433 - mean_absolute_error: 0.3433 - val_loss: 1.2083 - val_mean_absolute_error: 1.2083\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 1.14684\n",
            "Epoch 306/500\n",
            "960/960 [==============================] - 0s 147us/step - loss: 0.3115 - mean_absolute_error: 0.3115 - val_loss: 1.2808 - val_mean_absolute_error: 1.2808\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 1.14684\n",
            "Epoch 307/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.3096 - mean_absolute_error: 0.3096 - val_loss: 1.2046 - val_mean_absolute_error: 1.2046\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 1.14684\n",
            "Epoch 308/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.3184 - mean_absolute_error: 0.3184 - val_loss: 1.2285 - val_mean_absolute_error: 1.2285\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 1.14684\n",
            "Epoch 309/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.3159 - mean_absolute_error: 0.3159 - val_loss: 1.3272 - val_mean_absolute_error: 1.3272\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 1.14684\n",
            "Epoch 310/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.2863 - mean_absolute_error: 0.2863 - val_loss: 1.2614 - val_mean_absolute_error: 1.2614\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 1.14684\n",
            "Epoch 311/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.3009 - mean_absolute_error: 0.3009 - val_loss: 1.2794 - val_mean_absolute_error: 1.2794\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 1.14684\n",
            "Epoch 312/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.2807 - mean_absolute_error: 0.2807 - val_loss: 1.3306 - val_mean_absolute_error: 1.3306\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 1.14684\n",
            "Epoch 313/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.3064 - mean_absolute_error: 0.3064 - val_loss: 1.2075 - val_mean_absolute_error: 1.2075\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 1.14684\n",
            "Epoch 314/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.3016 - mean_absolute_error: 0.3016 - val_loss: 1.3556 - val_mean_absolute_error: 1.3556\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 1.14684\n",
            "Epoch 315/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.3091 - mean_absolute_error: 0.3091 - val_loss: 1.2754 - val_mean_absolute_error: 1.2754\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 1.14684\n",
            "Epoch 316/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.2900 - mean_absolute_error: 0.2900 - val_loss: 1.2938 - val_mean_absolute_error: 1.2938\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 1.14684\n",
            "Epoch 317/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.2999 - mean_absolute_error: 0.2999 - val_loss: 1.2807 - val_mean_absolute_error: 1.2807\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 1.14684\n",
            "Epoch 318/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.3028 - mean_absolute_error: 0.3028 - val_loss: 1.2261 - val_mean_absolute_error: 1.2261\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 1.14684\n",
            "Epoch 319/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.3204 - mean_absolute_error: 0.3204 - val_loss: 1.2441 - val_mean_absolute_error: 1.2441\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 1.14684\n",
            "Epoch 320/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.3241 - mean_absolute_error: 0.3241 - val_loss: 1.2357 - val_mean_absolute_error: 1.2357\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 1.14684\n",
            "Epoch 321/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.3011 - mean_absolute_error: 0.3011 - val_loss: 1.1929 - val_mean_absolute_error: 1.1929\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 1.14684\n",
            "Epoch 322/500\n",
            "960/960 [==============================] - 0s 178us/step - loss: 0.3468 - mean_absolute_error: 0.3468 - val_loss: 1.1913 - val_mean_absolute_error: 1.1913\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 1.14684\n",
            "Epoch 323/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.3455 - mean_absolute_error: 0.3455 - val_loss: 1.1976 - val_mean_absolute_error: 1.1976\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 1.14684\n",
            "Epoch 324/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.3305 - mean_absolute_error: 0.3305 - val_loss: 1.2385 - val_mean_absolute_error: 1.2385\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 1.14684\n",
            "Epoch 325/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.3099 - mean_absolute_error: 0.3099 - val_loss: 1.1697 - val_mean_absolute_error: 1.1697\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 1.14684\n",
            "Epoch 326/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.2878 - mean_absolute_error: 0.2878 - val_loss: 1.2214 - val_mean_absolute_error: 1.2214\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 1.14684\n",
            "Epoch 327/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.2829 - mean_absolute_error: 0.2829 - val_loss: 1.2026 - val_mean_absolute_error: 1.2026\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 1.14684\n",
            "Epoch 328/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.3113 - mean_absolute_error: 0.3113 - val_loss: 1.2895 - val_mean_absolute_error: 1.2895\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 1.14684\n",
            "Epoch 329/500\n",
            "960/960 [==============================] - 0s 147us/step - loss: 0.2831 - mean_absolute_error: 0.2831 - val_loss: 1.2234 - val_mean_absolute_error: 1.2234\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 1.14684\n",
            "Epoch 330/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.3098 - mean_absolute_error: 0.3098 - val_loss: 1.2134 - val_mean_absolute_error: 1.2134\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 1.14684\n",
            "Epoch 331/500\n",
            "960/960 [==============================] - 0s 149us/step - loss: 0.3187 - mean_absolute_error: 0.3187 - val_loss: 1.1302 - val_mean_absolute_error: 1.1302\n",
            "\n",
            "Epoch 00331: val_loss improved from 1.14684 to 1.13016, saving model to Weights-331--1.13016.hdf5\n",
            "Epoch 332/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.3272 - mean_absolute_error: 0.3272 - val_loss: 1.1580 - val_mean_absolute_error: 1.1580\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 1.13016\n",
            "Epoch 333/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.3029 - mean_absolute_error: 0.3029 - val_loss: 1.2817 - val_mean_absolute_error: 1.2817\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 1.13016\n",
            "Epoch 334/500\n",
            "960/960 [==============================] - 0s 176us/step - loss: 0.2944 - mean_absolute_error: 0.2944 - val_loss: 1.2864 - val_mean_absolute_error: 1.2864\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 1.13016\n",
            "Epoch 335/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.3063 - mean_absolute_error: 0.3063 - val_loss: 1.2579 - val_mean_absolute_error: 1.2579\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 1.13016\n",
            "Epoch 336/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.3074 - mean_absolute_error: 0.3074 - val_loss: 1.2874 - val_mean_absolute_error: 1.2874\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 1.13016\n",
            "Epoch 337/500\n",
            "960/960 [==============================] - 0s 151us/step - loss: 0.2986 - mean_absolute_error: 0.2986 - val_loss: 1.2640 - val_mean_absolute_error: 1.2640\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 1.13016\n",
            "Epoch 338/500\n",
            "960/960 [==============================] - 0s 150us/step - loss: 0.3260 - mean_absolute_error: 0.3260 - val_loss: 1.2535 - val_mean_absolute_error: 1.2535\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 1.13016\n",
            "Epoch 339/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.3349 - mean_absolute_error: 0.3349 - val_loss: 1.3134 - val_mean_absolute_error: 1.3134\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 1.13016\n",
            "Epoch 340/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.3419 - mean_absolute_error: 0.3419 - val_loss: 1.1556 - val_mean_absolute_error: 1.1556\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 1.13016\n",
            "Epoch 341/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.3108 - mean_absolute_error: 0.3108 - val_loss: 1.3131 - val_mean_absolute_error: 1.3131\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 1.13016\n",
            "Epoch 342/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.3223 - mean_absolute_error: 0.3223 - val_loss: 1.3015 - val_mean_absolute_error: 1.3015\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 1.13016\n",
            "Epoch 343/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.3152 - mean_absolute_error: 0.3152 - val_loss: 1.2259 - val_mean_absolute_error: 1.2259\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 1.13016\n",
            "Epoch 344/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.3183 - mean_absolute_error: 0.3183 - val_loss: 1.2039 - val_mean_absolute_error: 1.2039\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 1.13016\n",
            "Epoch 345/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.3059 - mean_absolute_error: 0.3059 - val_loss: 1.2398 - val_mean_absolute_error: 1.2398\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 1.13016\n",
            "Epoch 346/500\n",
            "960/960 [==============================] - 0s 151us/step - loss: 0.3202 - mean_absolute_error: 0.3202 - val_loss: 1.1188 - val_mean_absolute_error: 1.1188\n",
            "\n",
            "Epoch 00346: val_loss improved from 1.13016 to 1.11884, saving model to Weights-346--1.11884.hdf5\n",
            "Epoch 347/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.3055 - mean_absolute_error: 0.3055 - val_loss: 1.2661 - val_mean_absolute_error: 1.2661\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 1.11884\n",
            "Epoch 348/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.2983 - mean_absolute_error: 0.2983 - val_loss: 1.1866 - val_mean_absolute_error: 1.1866\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 1.11884\n",
            "Epoch 349/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.3140 - mean_absolute_error: 0.3140 - val_loss: 1.2341 - val_mean_absolute_error: 1.2341\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 1.11884\n",
            "Epoch 350/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.3022 - mean_absolute_error: 0.3022 - val_loss: 1.2778 - val_mean_absolute_error: 1.2778\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 1.11884\n",
            "Epoch 351/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.2977 - mean_absolute_error: 0.2977 - val_loss: 1.2872 - val_mean_absolute_error: 1.2872\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 1.11884\n",
            "Epoch 352/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.3057 - mean_absolute_error: 0.3057 - val_loss: 1.2248 - val_mean_absolute_error: 1.2248\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 1.11884\n",
            "Epoch 353/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.2982 - mean_absolute_error: 0.2982 - val_loss: 1.2457 - val_mean_absolute_error: 1.2457\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 1.11884\n",
            "Epoch 354/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.3035 - mean_absolute_error: 0.3035 - val_loss: 1.1835 - val_mean_absolute_error: 1.1835\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 1.11884\n",
            "Epoch 355/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.3140 - mean_absolute_error: 0.3140 - val_loss: 1.2635 - val_mean_absolute_error: 1.2635\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 1.11884\n",
            "Epoch 356/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.2918 - mean_absolute_error: 0.2918 - val_loss: 1.2016 - val_mean_absolute_error: 1.2016\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 1.11884\n",
            "Epoch 357/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.2799 - mean_absolute_error: 0.2799 - val_loss: 1.1449 - val_mean_absolute_error: 1.1449\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 1.11884\n",
            "Epoch 358/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.2922 - mean_absolute_error: 0.2922 - val_loss: 1.2811 - val_mean_absolute_error: 1.2811\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 1.11884\n",
            "Epoch 359/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.2691 - mean_absolute_error: 0.2691 - val_loss: 1.2199 - val_mean_absolute_error: 1.2199\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 1.11884\n",
            "Epoch 360/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.2791 - mean_absolute_error: 0.2791 - val_loss: 1.2500 - val_mean_absolute_error: 1.2500\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 1.11884\n",
            "Epoch 361/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.2716 - mean_absolute_error: 0.2716 - val_loss: 1.2374 - val_mean_absolute_error: 1.2374\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 1.11884\n",
            "Epoch 362/500\n",
            "960/960 [==============================] - 0s 155us/step - loss: 0.2728 - mean_absolute_error: 0.2728 - val_loss: 1.2806 - val_mean_absolute_error: 1.2806\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 1.11884\n",
            "Epoch 363/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.2778 - mean_absolute_error: 0.2778 - val_loss: 1.2530 - val_mean_absolute_error: 1.2530\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 1.11884\n",
            "Epoch 364/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.2986 - mean_absolute_error: 0.2986 - val_loss: 1.2382 - val_mean_absolute_error: 1.2382\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 1.11884\n",
            "Epoch 365/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.3532 - mean_absolute_error: 0.3532 - val_loss: 1.2683 - val_mean_absolute_error: 1.2683\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 1.11884\n",
            "Epoch 366/500\n",
            "960/960 [==============================] - 0s 155us/step - loss: 0.3115 - mean_absolute_error: 0.3115 - val_loss: 1.2312 - val_mean_absolute_error: 1.2312\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 1.11884\n",
            "Epoch 367/500\n",
            "960/960 [==============================] - 0s 150us/step - loss: 0.3628 - mean_absolute_error: 0.3628 - val_loss: 1.2286 - val_mean_absolute_error: 1.2286\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 1.11884\n",
            "Epoch 368/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.3524 - mean_absolute_error: 0.3524 - val_loss: 1.1056 - val_mean_absolute_error: 1.1056\n",
            "\n",
            "Epoch 00368: val_loss improved from 1.11884 to 1.10558, saving model to Weights-368--1.10558.hdf5\n",
            "Epoch 369/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.3134 - mean_absolute_error: 0.3134 - val_loss: 1.2537 - val_mean_absolute_error: 1.2537\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 1.10558\n",
            "Epoch 370/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.2904 - mean_absolute_error: 0.2904 - val_loss: 1.3417 - val_mean_absolute_error: 1.3417\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 1.10558\n",
            "Epoch 371/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.2963 - mean_absolute_error: 0.2963 - val_loss: 1.2633 - val_mean_absolute_error: 1.2633\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 1.10558\n",
            "Epoch 372/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.2971 - mean_absolute_error: 0.2971 - val_loss: 1.1668 - val_mean_absolute_error: 1.1668\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 1.10558\n",
            "Epoch 373/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.3250 - mean_absolute_error: 0.3250 - val_loss: 1.2265 - val_mean_absolute_error: 1.2265\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 1.10558\n",
            "Epoch 374/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.3096 - mean_absolute_error: 0.3096 - val_loss: 1.2440 - val_mean_absolute_error: 1.2440\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 1.10558\n",
            "Epoch 375/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.2893 - mean_absolute_error: 0.2893 - val_loss: 1.2094 - val_mean_absolute_error: 1.2094\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 1.10558\n",
            "Epoch 376/500\n",
            "960/960 [==============================] - 0s 187us/step - loss: 0.2977 - mean_absolute_error: 0.2977 - val_loss: 1.2152 - val_mean_absolute_error: 1.2152\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 1.10558\n",
            "Epoch 377/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.2786 - mean_absolute_error: 0.2786 - val_loss: 1.2192 - val_mean_absolute_error: 1.2192\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 1.10558\n",
            "Epoch 378/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.2548 - mean_absolute_error: 0.2548 - val_loss: 1.1955 - val_mean_absolute_error: 1.1955\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 1.10558\n",
            "Epoch 379/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.2634 - mean_absolute_error: 0.2634 - val_loss: 1.2235 - val_mean_absolute_error: 1.2235\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 1.10558\n",
            "Epoch 380/500\n",
            "960/960 [==============================] - 0s 176us/step - loss: 0.2770 - mean_absolute_error: 0.2770 - val_loss: 1.2035 - val_mean_absolute_error: 1.2035\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 1.10558\n",
            "Epoch 381/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.2959 - mean_absolute_error: 0.2959 - val_loss: 1.1694 - val_mean_absolute_error: 1.1694\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 1.10558\n",
            "Epoch 382/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.3102 - mean_absolute_error: 0.3102 - val_loss: 1.1231 - val_mean_absolute_error: 1.1231\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 1.10558\n",
            "Epoch 383/500\n",
            "960/960 [==============================] - 0s 180us/step - loss: 0.2969 - mean_absolute_error: 0.2969 - val_loss: 1.1094 - val_mean_absolute_error: 1.1094\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 1.10558\n",
            "Epoch 384/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.2638 - mean_absolute_error: 0.2638 - val_loss: 1.2286 - val_mean_absolute_error: 1.2286\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 1.10558\n",
            "Epoch 385/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.2758 - mean_absolute_error: 0.2758 - val_loss: 1.1843 - val_mean_absolute_error: 1.1843\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 1.10558\n",
            "Epoch 386/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.2806 - mean_absolute_error: 0.2806 - val_loss: 1.2624 - val_mean_absolute_error: 1.2624\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 1.10558\n",
            "Epoch 387/500\n",
            "960/960 [==============================] - 0s 178us/step - loss: 0.2627 - mean_absolute_error: 0.2627 - val_loss: 1.2251 - val_mean_absolute_error: 1.2251\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 1.10558\n",
            "Epoch 388/500\n",
            "960/960 [==============================] - 0s 179us/step - loss: 0.2685 - mean_absolute_error: 0.2685 - val_loss: 1.2212 - val_mean_absolute_error: 1.2212\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 1.10558\n",
            "Epoch 389/500\n",
            "960/960 [==============================] - 0s 178us/step - loss: 0.2591 - mean_absolute_error: 0.2591 - val_loss: 1.2106 - val_mean_absolute_error: 1.2106\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 1.10558\n",
            "Epoch 390/500\n",
            "960/960 [==============================] - 0s 176us/step - loss: 0.3064 - mean_absolute_error: 0.3064 - val_loss: 1.3705 - val_mean_absolute_error: 1.3705\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 1.10558\n",
            "Epoch 391/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.3608 - mean_absolute_error: 0.3608 - val_loss: 1.3743 - val_mean_absolute_error: 1.3743\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 1.10558\n",
            "Epoch 392/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.3076 - mean_absolute_error: 0.3076 - val_loss: 1.2399 - val_mean_absolute_error: 1.2399\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 1.10558\n",
            "Epoch 393/500\n",
            "960/960 [==============================] - 0s 177us/step - loss: 0.2763 - mean_absolute_error: 0.2763 - val_loss: 1.1542 - val_mean_absolute_error: 1.1542\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 1.10558\n",
            "Epoch 394/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.2856 - mean_absolute_error: 0.2856 - val_loss: 1.2728 - val_mean_absolute_error: 1.2728\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 1.10558\n",
            "Epoch 395/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.2856 - mean_absolute_error: 0.2856 - val_loss: 1.2708 - val_mean_absolute_error: 1.2708\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 1.10558\n",
            "Epoch 396/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.2566 - mean_absolute_error: 0.2566 - val_loss: 1.2434 - val_mean_absolute_error: 1.2434\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 1.10558\n",
            "Epoch 397/500\n",
            "960/960 [==============================] - 0s 150us/step - loss: 0.2646 - mean_absolute_error: 0.2646 - val_loss: 1.2477 - val_mean_absolute_error: 1.2477\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 1.10558\n",
            "Epoch 398/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.2677 - mean_absolute_error: 0.2677 - val_loss: 1.2528 - val_mean_absolute_error: 1.2528\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 1.10558\n",
            "Epoch 399/500\n",
            "960/960 [==============================] - 0s 155us/step - loss: 0.2567 - mean_absolute_error: 0.2567 - val_loss: 1.2400 - val_mean_absolute_error: 1.2400\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 1.10558\n",
            "Epoch 400/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.2917 - mean_absolute_error: 0.2917 - val_loss: 1.3223 - val_mean_absolute_error: 1.3223\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 1.10558\n",
            "Epoch 401/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.2721 - mean_absolute_error: 0.2721 - val_loss: 1.3488 - val_mean_absolute_error: 1.3488\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 1.10558\n",
            "Epoch 402/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.2639 - mean_absolute_error: 0.2639 - val_loss: 1.3116 - val_mean_absolute_error: 1.3116\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 1.10558\n",
            "Epoch 403/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.2909 - mean_absolute_error: 0.2909 - val_loss: 1.3126 - val_mean_absolute_error: 1.3126\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 1.10558\n",
            "Epoch 404/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.3213 - mean_absolute_error: 0.3213 - val_loss: 1.2363 - val_mean_absolute_error: 1.2363\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 1.10558\n",
            "Epoch 405/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.2626 - mean_absolute_error: 0.2626 - val_loss: 1.3052 - val_mean_absolute_error: 1.3052\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 1.10558\n",
            "Epoch 406/500\n",
            "960/960 [==============================] - 0s 187us/step - loss: 0.2593 - mean_absolute_error: 0.2593 - val_loss: 1.3019 - val_mean_absolute_error: 1.3019\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 1.10558\n",
            "Epoch 407/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.2574 - mean_absolute_error: 0.2574 - val_loss: 1.2370 - val_mean_absolute_error: 1.2370\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 1.10558\n",
            "Epoch 408/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 1.2496 - val_mean_absolute_error: 1.2496\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 1.10558\n",
            "Epoch 409/500\n",
            "960/960 [==============================] - 0s 151us/step - loss: 0.2450 - mean_absolute_error: 0.2450 - val_loss: 1.2305 - val_mean_absolute_error: 1.2305\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 1.10558\n",
            "Epoch 410/500\n",
            "960/960 [==============================] - 0s 150us/step - loss: 0.2660 - mean_absolute_error: 0.2660 - val_loss: 1.2025 - val_mean_absolute_error: 1.2025\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 1.10558\n",
            "Epoch 411/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.2514 - mean_absolute_error: 0.2514 - val_loss: 1.1493 - val_mean_absolute_error: 1.1493\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 1.10558\n",
            "Epoch 412/500\n",
            "960/960 [==============================] - 0s 149us/step - loss: 0.2639 - mean_absolute_error: 0.2639 - val_loss: 1.2451 - val_mean_absolute_error: 1.2451\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 1.10558\n",
            "Epoch 413/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.2736 - mean_absolute_error: 0.2736 - val_loss: 1.2825 - val_mean_absolute_error: 1.2825\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 1.10558\n",
            "Epoch 414/500\n",
            "960/960 [==============================] - 0s 194us/step - loss: 0.2767 - mean_absolute_error: 0.2767 - val_loss: 1.2496 - val_mean_absolute_error: 1.2496\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 1.10558\n",
            "Epoch 415/500\n",
            "960/960 [==============================] - 0s 155us/step - loss: 0.2628 - mean_absolute_error: 0.2628 - val_loss: 1.2367 - val_mean_absolute_error: 1.2367\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 1.10558\n",
            "Epoch 416/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.2717 - mean_absolute_error: 0.2717 - val_loss: 1.3118 - val_mean_absolute_error: 1.3118\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 1.10558\n",
            "Epoch 417/500\n",
            "960/960 [==============================] - 0s 150us/step - loss: 0.2901 - mean_absolute_error: 0.2901 - val_loss: 1.2358 - val_mean_absolute_error: 1.2358\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 1.10558\n",
            "Epoch 418/500\n",
            "960/960 [==============================] - 0s 152us/step - loss: 0.2803 - mean_absolute_error: 0.2803 - val_loss: 1.1749 - val_mean_absolute_error: 1.1749\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 1.10558\n",
            "Epoch 419/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.2723 - mean_absolute_error: 0.2723 - val_loss: 1.2506 - val_mean_absolute_error: 1.2506\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 1.10558\n",
            "Epoch 420/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.2617 - mean_absolute_error: 0.2617 - val_loss: 1.2652 - val_mean_absolute_error: 1.2652\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 1.10558\n",
            "Epoch 421/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.2710 - mean_absolute_error: 0.2710 - val_loss: 1.2792 - val_mean_absolute_error: 1.2792\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 1.10558\n",
            "Epoch 422/500\n",
            "960/960 [==============================] - 0s 153us/step - loss: 0.2599 - mean_absolute_error: 0.2599 - val_loss: 1.3020 - val_mean_absolute_error: 1.3020\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 1.10558\n",
            "Epoch 423/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.2645 - mean_absolute_error: 0.2645 - val_loss: 1.2141 - val_mean_absolute_error: 1.2141\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 1.10558\n",
            "Epoch 424/500\n",
            "960/960 [==============================] - 0s 153us/step - loss: 0.2802 - mean_absolute_error: 0.2802 - val_loss: 1.0938 - val_mean_absolute_error: 1.0938\n",
            "\n",
            "Epoch 00424: val_loss improved from 1.10558 to 1.09380, saving model to Weights-424--1.09380.hdf5\n",
            "Epoch 425/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.3231 - mean_absolute_error: 0.3231 - val_loss: 1.2714 - val_mean_absolute_error: 1.2714\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 1.09380\n",
            "Epoch 426/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.2902 - mean_absolute_error: 0.2902 - val_loss: 1.3039 - val_mean_absolute_error: 1.3039\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 1.09380\n",
            "Epoch 427/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.2811 - mean_absolute_error: 0.2811 - val_loss: 1.2115 - val_mean_absolute_error: 1.2115\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 1.09380\n",
            "Epoch 428/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.2763 - mean_absolute_error: 0.2763 - val_loss: 1.1127 - val_mean_absolute_error: 1.1127\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 1.09380\n",
            "Epoch 429/500\n",
            "960/960 [==============================] - 0s 146us/step - loss: 0.2564 - mean_absolute_error: 0.2564 - val_loss: 1.1519 - val_mean_absolute_error: 1.1519\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 1.09380\n",
            "Epoch 430/500\n",
            "960/960 [==============================] - 0s 152us/step - loss: 0.2712 - mean_absolute_error: 0.2712 - val_loss: 1.2397 - val_mean_absolute_error: 1.2397\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 1.09380\n",
            "Epoch 431/500\n",
            "960/960 [==============================] - 0s 152us/step - loss: 0.2803 - mean_absolute_error: 0.2803 - val_loss: 1.2326 - val_mean_absolute_error: 1.2326\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 1.09380\n",
            "Epoch 432/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.3001 - mean_absolute_error: 0.3001 - val_loss: 1.2733 - val_mean_absolute_error: 1.2733\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 1.09380\n",
            "Epoch 433/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.2748 - mean_absolute_error: 0.2748 - val_loss: 1.2480 - val_mean_absolute_error: 1.2480\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 1.09380\n",
            "Epoch 434/500\n",
            "960/960 [==============================] - 0s 166us/step - loss: 0.2876 - mean_absolute_error: 0.2876 - val_loss: 1.2896 - val_mean_absolute_error: 1.2896\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 1.09380\n",
            "Epoch 435/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.3275 - mean_absolute_error: 0.3275 - val_loss: 1.3082 - val_mean_absolute_error: 1.3082\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 1.09380\n",
            "Epoch 436/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.3276 - mean_absolute_error: 0.3276 - val_loss: 1.2359 - val_mean_absolute_error: 1.2359\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 1.09380\n",
            "Epoch 437/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.3102 - mean_absolute_error: 0.3102 - val_loss: 1.1343 - val_mean_absolute_error: 1.1343\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 1.09380\n",
            "Epoch 438/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.2809 - mean_absolute_error: 0.2809 - val_loss: 1.2037 - val_mean_absolute_error: 1.2037\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 1.09380\n",
            "Epoch 439/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.2736 - mean_absolute_error: 0.2736 - val_loss: 1.3315 - val_mean_absolute_error: 1.3315\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 1.09380\n",
            "Epoch 440/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.2526 - mean_absolute_error: 0.2526 - val_loss: 1.2337 - val_mean_absolute_error: 1.2337\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 1.09380\n",
            "Epoch 441/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.2630 - mean_absolute_error: 0.2630 - val_loss: 1.2328 - val_mean_absolute_error: 1.2328\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 1.09380\n",
            "Epoch 442/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.2964 - mean_absolute_error: 0.2964 - val_loss: 1.1376 - val_mean_absolute_error: 1.1376\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 1.09380\n",
            "Epoch 443/500\n",
            "960/960 [==============================] - 0s 154us/step - loss: 0.3063 - mean_absolute_error: 0.3063 - val_loss: 1.1331 - val_mean_absolute_error: 1.1331\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 1.09380\n",
            "Epoch 444/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.2909 - mean_absolute_error: 0.2909 - val_loss: 1.2451 - val_mean_absolute_error: 1.2451\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 1.09380\n",
            "Epoch 445/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.2760 - mean_absolute_error: 0.2760 - val_loss: 1.3073 - val_mean_absolute_error: 1.3073\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 1.09380\n",
            "Epoch 446/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.2663 - mean_absolute_error: 0.2663 - val_loss: 1.3422 - val_mean_absolute_error: 1.3422\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 1.09380\n",
            "Epoch 447/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.2657 - mean_absolute_error: 0.2657 - val_loss: 1.3475 - val_mean_absolute_error: 1.3475\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 1.09380\n",
            "Epoch 448/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.2671 - mean_absolute_error: 0.2671 - val_loss: 1.2562 - val_mean_absolute_error: 1.2562\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 1.09380\n",
            "Epoch 449/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.2618 - mean_absolute_error: 0.2618 - val_loss: 1.2192 - val_mean_absolute_error: 1.2192\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 1.09380\n",
            "Epoch 450/500\n",
            "960/960 [==============================] - 0s 181us/step - loss: 0.2661 - mean_absolute_error: 0.2661 - val_loss: 1.3526 - val_mean_absolute_error: 1.3526\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 1.09380\n",
            "Epoch 451/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.2551 - mean_absolute_error: 0.2551 - val_loss: 1.2774 - val_mean_absolute_error: 1.2774\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 1.09380\n",
            "Epoch 452/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.2645 - mean_absolute_error: 0.2645 - val_loss: 1.2037 - val_mean_absolute_error: 1.2037\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 1.09380\n",
            "Epoch 453/500\n",
            "960/960 [==============================] - 0s 180us/step - loss: 0.2650 - mean_absolute_error: 0.2650 - val_loss: 1.1747 - val_mean_absolute_error: 1.1747\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 1.09380\n",
            "Epoch 454/500\n",
            "960/960 [==============================] - 0s 160us/step - loss: 0.2554 - mean_absolute_error: 0.2554 - val_loss: 1.2675 - val_mean_absolute_error: 1.2675\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 1.09380\n",
            "Epoch 455/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.2541 - mean_absolute_error: 0.2541 - val_loss: 1.2041 - val_mean_absolute_error: 1.2041\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 1.09380\n",
            "Epoch 456/500\n",
            "960/960 [==============================] - 0s 177us/step - loss: 0.2583 - mean_absolute_error: 0.2583 - val_loss: 1.2392 - val_mean_absolute_error: 1.2392\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 1.09380\n",
            "Epoch 457/500\n",
            "960/960 [==============================] - 0s 171us/step - loss: 0.2563 - mean_absolute_error: 0.2563 - val_loss: 1.1932 - val_mean_absolute_error: 1.1932\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 1.09380\n",
            "Epoch 458/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.2472 - mean_absolute_error: 0.2472 - val_loss: 1.1868 - val_mean_absolute_error: 1.1868\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 1.09380\n",
            "Epoch 459/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.2592 - mean_absolute_error: 0.2592 - val_loss: 1.2253 - val_mean_absolute_error: 1.2253\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 1.09380\n",
            "Epoch 460/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.2611 - mean_absolute_error: 0.2611 - val_loss: 1.2582 - val_mean_absolute_error: 1.2582\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 1.09380\n",
            "Epoch 461/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.2954 - mean_absolute_error: 0.2954 - val_loss: 1.1992 - val_mean_absolute_error: 1.1992\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 1.09380\n",
            "Epoch 462/500\n",
            "960/960 [==============================] - 0s 162us/step - loss: 0.3073 - mean_absolute_error: 0.3073 - val_loss: 1.3105 - val_mean_absolute_error: 1.3105\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 1.09380\n",
            "Epoch 463/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.3112 - mean_absolute_error: 0.3112 - val_loss: 1.2595 - val_mean_absolute_error: 1.2595\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 1.09380\n",
            "Epoch 464/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.2599 - mean_absolute_error: 0.2599 - val_loss: 1.1879 - val_mean_absolute_error: 1.1879\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 1.09380\n",
            "Epoch 465/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.2810 - mean_absolute_error: 0.2810 - val_loss: 1.2403 - val_mean_absolute_error: 1.2403\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 1.09380\n",
            "Epoch 466/500\n",
            "960/960 [==============================] - 0s 183us/step - loss: 0.2734 - mean_absolute_error: 0.2734 - val_loss: 1.1246 - val_mean_absolute_error: 1.1246\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 1.09380\n",
            "Epoch 467/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.2455 - mean_absolute_error: 0.2455 - val_loss: 1.2270 - val_mean_absolute_error: 1.2270\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 1.09380\n",
            "Epoch 468/500\n",
            "960/960 [==============================] - 0s 173us/step - loss: 0.2699 - mean_absolute_error: 0.2699 - val_loss: 1.2042 - val_mean_absolute_error: 1.2042\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 1.09380\n",
            "Epoch 469/500\n",
            "960/960 [==============================] - 0s 176us/step - loss: 0.2529 - mean_absolute_error: 0.2529 - val_loss: 1.0639 - val_mean_absolute_error: 1.0639\n",
            "\n",
            "Epoch 00469: val_loss improved from 1.09380 to 1.06395, saving model to Weights-469--1.06395.hdf5\n",
            "Epoch 470/500\n",
            "960/960 [==============================] - 0s 174us/step - loss: 0.2543 - mean_absolute_error: 0.2543 - val_loss: 1.0767 - val_mean_absolute_error: 1.0767\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 1.06395\n",
            "Epoch 471/500\n",
            "960/960 [==============================] - 0s 170us/step - loss: 0.2394 - mean_absolute_error: 0.2394 - val_loss: 1.1740 - val_mean_absolute_error: 1.1740\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 1.06395\n",
            "Epoch 472/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.2561 - mean_absolute_error: 0.2561 - val_loss: 1.1444 - val_mean_absolute_error: 1.1444\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 1.06395\n",
            "Epoch 473/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.2653 - mean_absolute_error: 0.2653 - val_loss: 1.1002 - val_mean_absolute_error: 1.1002\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 1.06395\n",
            "Epoch 474/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.2818 - mean_absolute_error: 0.2818 - val_loss: 1.1862 - val_mean_absolute_error: 1.1862\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 1.06395\n",
            "Epoch 475/500\n",
            "960/960 [==============================] - 0s 163us/step - loss: 0.2791 - mean_absolute_error: 0.2791 - val_loss: 1.2229 - val_mean_absolute_error: 1.2229\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 1.06395\n",
            "Epoch 476/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.2602 - mean_absolute_error: 0.2602 - val_loss: 1.1888 - val_mean_absolute_error: 1.1888\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 1.06395\n",
            "Epoch 477/500\n",
            "960/960 [==============================] - 0s 180us/step - loss: 0.2409 - mean_absolute_error: 0.2409 - val_loss: 1.2016 - val_mean_absolute_error: 1.2016\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 1.06395\n",
            "Epoch 478/500\n",
            "960/960 [==============================] - 0s 161us/step - loss: 0.2470 - mean_absolute_error: 0.2470 - val_loss: 1.2845 - val_mean_absolute_error: 1.2845\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 1.06395\n",
            "Epoch 479/500\n",
            "960/960 [==============================] - 0s 169us/step - loss: 0.2358 - mean_absolute_error: 0.2358 - val_loss: 1.2508 - val_mean_absolute_error: 1.2508\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 1.06395\n",
            "Epoch 480/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.2400 - mean_absolute_error: 0.2400 - val_loss: 1.2485 - val_mean_absolute_error: 1.2485\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 1.06395\n",
            "Epoch 481/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.2362 - mean_absolute_error: 0.2362 - val_loss: 1.0823 - val_mean_absolute_error: 1.0823\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 1.06395\n",
            "Epoch 482/500\n",
            "960/960 [==============================] - 0s 188us/step - loss: 0.2365 - mean_absolute_error: 0.2365 - val_loss: 1.0514 - val_mean_absolute_error: 1.0514\n",
            "\n",
            "Epoch 00482: val_loss improved from 1.06395 to 1.05137, saving model to Weights-482--1.05137.hdf5\n",
            "Epoch 483/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.2603 - mean_absolute_error: 0.2603 - val_loss: 1.0464 - val_mean_absolute_error: 1.0464\n",
            "\n",
            "Epoch 00483: val_loss improved from 1.05137 to 1.04641, saving model to Weights-483--1.04641.hdf5\n",
            "Epoch 484/500\n",
            "960/960 [==============================] - 0s 155us/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 1.1933 - val_mean_absolute_error: 1.1933\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 1.04641\n",
            "Epoch 485/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.2317 - mean_absolute_error: 0.2317 - val_loss: 1.2626 - val_mean_absolute_error: 1.2626\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 1.04641\n",
            "Epoch 486/500\n",
            "960/960 [==============================] - 0s 179us/step - loss: 0.2434 - mean_absolute_error: 0.2434 - val_loss: 1.1686 - val_mean_absolute_error: 1.1686\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 1.04641\n",
            "Epoch 487/500\n",
            "960/960 [==============================] - 0s 167us/step - loss: 0.2577 - mean_absolute_error: 0.2577 - val_loss: 1.1695 - val_mean_absolute_error: 1.1695\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 1.04641\n",
            "Epoch 488/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.2373 - mean_absolute_error: 0.2373 - val_loss: 1.1747 - val_mean_absolute_error: 1.1747\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 1.04641\n",
            "Epoch 489/500\n",
            "960/960 [==============================] - 0s 175us/step - loss: 0.2486 - mean_absolute_error: 0.2486 - val_loss: 1.1368 - val_mean_absolute_error: 1.1368\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 1.04641\n",
            "Epoch 490/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.2384 - mean_absolute_error: 0.2384 - val_loss: 1.1346 - val_mean_absolute_error: 1.1346\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 1.04641\n",
            "Epoch 491/500\n",
            "960/960 [==============================] - 0s 172us/step - loss: 0.2395 - mean_absolute_error: 0.2395 - val_loss: 1.1928 - val_mean_absolute_error: 1.1928\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 1.04641\n",
            "Epoch 492/500\n",
            "960/960 [==============================] - 0s 156us/step - loss: 0.2442 - mean_absolute_error: 0.2442 - val_loss: 1.1239 - val_mean_absolute_error: 1.1239\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 1.04641\n",
            "Epoch 493/500\n",
            "960/960 [==============================] - 0s 159us/step - loss: 0.2516 - mean_absolute_error: 0.2516 - val_loss: 1.1783 - val_mean_absolute_error: 1.1783\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 1.04641\n",
            "Epoch 494/500\n",
            "960/960 [==============================] - 0s 168us/step - loss: 0.2402 - mean_absolute_error: 0.2402 - val_loss: 1.1278 - val_mean_absolute_error: 1.1278\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 1.04641\n",
            "Epoch 495/500\n",
            "960/960 [==============================] - 0s 158us/step - loss: 0.2556 - mean_absolute_error: 0.2556 - val_loss: 1.2247 - val_mean_absolute_error: 1.2247\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 1.04641\n",
            "Epoch 496/500\n",
            "960/960 [==============================] - 0s 164us/step - loss: 0.2741 - mean_absolute_error: 0.2741 - val_loss: 1.2408 - val_mean_absolute_error: 1.2408\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 1.04641\n",
            "Epoch 497/500\n",
            "960/960 [==============================] - 0s 157us/step - loss: 0.2510 - mean_absolute_error: 0.2510 - val_loss: 1.3355 - val_mean_absolute_error: 1.3355\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 1.04641\n",
            "Epoch 498/500\n",
            "960/960 [==============================] - 0s 165us/step - loss: 0.2692 - mean_absolute_error: 0.2692 - val_loss: 1.3452 - val_mean_absolute_error: 1.3452\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 1.04641\n",
            "Epoch 499/500\n",
            "960/960 [==============================] - 0s 174us/step - loss: 0.2661 - mean_absolute_error: 0.2661 - val_loss: 1.1737 - val_mean_absolute_error: 1.1737\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 1.04641\n",
            "Epoch 500/500\n",
            "960/960 [==============================] - 0s 177us/step - loss: 0.2479 - mean_absolute_error: 0.2479 - val_loss: 1.1794 - val_mean_absolute_error: 1.1794\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 1.04641\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f174082abe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3H3UmbbWfuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load wights file of the best model :\n",
        "wights_file = 'Weights-483--1.04641.hdf5' # choose the best checkpoint \n",
        "NN_model.load_weights(wights_file) # load it\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6VyfdDBWfrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = NN_model.predict(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rX4KFrHWflP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff9dc644-7740-46ad-bd5e-0d35e70bf605"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.8970613e+00],\n",
              "       [ 2.8942325e+00],\n",
              "       [ 2.3165498e+00],\n",
              "       [ 2.7311971e+00],\n",
              "       [-1.2611717e-02],\n",
              "       [ 1.0576186e+00],\n",
              "       [-1.3341904e-02],\n",
              "       [ 1.0188944e+00],\n",
              "       [ 5.7083637e-01],\n",
              "       [ 1.9697105e+00],\n",
              "       [ 2.9481525e+00],\n",
              "       [ 2.8315921e+00],\n",
              "       [ 2.9831667e+00],\n",
              "       [ 2.2474818e+00],\n",
              "       [ 2.9931130e+00],\n",
              "       [ 2.9869595e+00],\n",
              "       [ 2.3869934e+00],\n",
              "       [ 1.5742465e+00],\n",
              "       [ 2.9606593e+00],\n",
              "       [ 2.7931957e+00],\n",
              "       [ 3.0199213e+00],\n",
              "       [ 2.9993830e+00],\n",
              "       [ 2.9010246e+00],\n",
              "       [-6.3292980e-03],\n",
              "       [-1.0146797e-03],\n",
              "       [ 3.0021558e+00],\n",
              "       [ 3.1142869e+00],\n",
              "       [ 2.9964969e+00],\n",
              "       [ 2.9808712e+00],\n",
              "       [-2.8894246e-03],\n",
              "       [ 2.9909756e+00],\n",
              "       [ 2.9885352e+00],\n",
              "       [ 2.9822478e+00],\n",
              "       [ 1.7033517e-02],\n",
              "       [ 2.1679773e+00],\n",
              "       [ 2.7606151e+00],\n",
              "       [ 2.9762533e+00],\n",
              "       [ 1.0139771e+00],\n",
              "       [ 1.3325541e+00],\n",
              "       [ 2.9743109e+00],\n",
              "       [ 3.0061502e+00],\n",
              "       [ 2.9936070e+00],\n",
              "       [ 2.9875200e+00],\n",
              "       [ 2.9776802e+00],\n",
              "       [ 2.9691923e+00],\n",
              "       [ 1.3204001e+00],\n",
              "       [ 2.9497657e+00],\n",
              "       [ 2.9758530e+00],\n",
              "       [ 2.9677894e+00],\n",
              "       [ 2.9812305e+00],\n",
              "       [ 2.9477119e+00],\n",
              "       [ 2.9076447e+00],\n",
              "       [ 1.0115385e+00],\n",
              "       [ 1.3921496e+00],\n",
              "       [-1.4007181e-02],\n",
              "       [-2.2489637e-02],\n",
              "       [-4.1022509e-02],\n",
              "       [ 1.6698209e+00],\n",
              "       [ 2.6211863e+00],\n",
              "       [ 2.9786289e+00],\n",
              "       [ 8.5824955e-01],\n",
              "       [ 2.8928976e+00],\n",
              "       [ 1.2626531e+00],\n",
              "       [ 8.6350322e-01],\n",
              "       [ 2.7571616e+00],\n",
              "       [ 2.7366357e+00],\n",
              "       [-2.6728809e-03],\n",
              "       [ 2.9876723e+00],\n",
              "       [ 2.9630499e+00],\n",
              "       [ 2.9531591e+00],\n",
              "       [ 9.2303026e-01],\n",
              "       [ 2.9667530e+00],\n",
              "       [ 2.3618350e+00],\n",
              "       [ 2.7315621e+00],\n",
              "       [ 2.7708254e+00],\n",
              "       [ 2.5484242e+00],\n",
              "       [ 2.3219535e+00],\n",
              "       [ 1.4301127e+00],\n",
              "       [ 8.0458164e-01],\n",
              "       [ 9.1757309e-01],\n",
              "       [ 1.2081834e+00],\n",
              "       [ 1.0111079e+00],\n",
              "       [ 1.0648432e+00],\n",
              "       [ 1.4770138e+00],\n",
              "       [ 2.6420362e+00],\n",
              "       [ 2.3852327e+00],\n",
              "       [ 2.9072549e+00],\n",
              "       [ 2.5197988e+00],\n",
              "       [ 6.0759246e-01],\n",
              "       [ 3.0022380e+00],\n",
              "       [ 1.9845799e+00],\n",
              "       [ 2.6357784e+00],\n",
              "       [ 2.7047634e+00],\n",
              "       [ 2.7997265e+00],\n",
              "       [ 2.9736791e+00],\n",
              "       [ 1.9162720e-01],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.4122633e+00],\n",
              "       [ 2.9750437e-02],\n",
              "       [-3.7218064e-02],\n",
              "       [ 2.0364717e-01],\n",
              "       [ 2.9899518e+00],\n",
              "       [-2.4063647e-02],\n",
              "       [ 2.1400290e+00],\n",
              "       [ 2.9832497e+00],\n",
              "       [ 2.9771819e+00],\n",
              "       [ 1.1654429e+00],\n",
              "       [ 2.9695714e+00],\n",
              "       [ 2.9144361e+00],\n",
              "       [ 3.0130210e+00],\n",
              "       [ 2.0212376e+00],\n",
              "       [ 1.8920666e+00],\n",
              "       [ 2.9767175e+00],\n",
              "       [ 1.2110182e+00],\n",
              "       [-1.1303127e-02],\n",
              "       [ 1.6084107e+00],\n",
              "       [ 2.4219820e+00],\n",
              "       [ 2.9717495e+00],\n",
              "       [-2.1627396e-02],\n",
              "       [ 2.9370742e+00],\n",
              "       [ 2.2108064e+00],\n",
              "       [ 1.8948441e+00],\n",
              "       [ 2.9842935e+00],\n",
              "       [ 1.4303280e+00],\n",
              "       [ 2.0848662e-02],\n",
              "       [ 2.1742694e+00],\n",
              "       [ 2.9520833e+00],\n",
              "       [ 3.0119503e+00],\n",
              "       [ 3.0020390e+00],\n",
              "       [ 2.9815855e+00],\n",
              "       [ 2.9571261e+00],\n",
              "       [ 2.4259107e+00],\n",
              "       [ 2.9573696e+00],\n",
              "       [ 2.9492221e+00],\n",
              "       [ 2.9545374e+00],\n",
              "       [ 2.4475355e+00],\n",
              "       [ 2.8259635e+00],\n",
              "       [ 2.9185894e+00],\n",
              "       [ 2.7781723e+00],\n",
              "       [ 2.9795337e+00],\n",
              "       [ 2.4108696e+00],\n",
              "       [ 2.9888377e+00],\n",
              "       [ 2.3970423e+00],\n",
              "       [ 2.8316257e+00],\n",
              "       [ 2.9738343e+00],\n",
              "       [ 2.9843464e+00],\n",
              "       [ 2.9719739e+00],\n",
              "       [ 2.9861856e+00],\n",
              "       [ 2.9351952e+00],\n",
              "       [ 2.9676545e+00],\n",
              "       [ 2.9829488e+00],\n",
              "       [ 3.0167871e+00],\n",
              "       [ 2.9845090e+00],\n",
              "       [ 3.0737917e+00],\n",
              "       [ 3.0597374e+00],\n",
              "       [ 3.0302773e+00],\n",
              "       [ 1.7500025e+00],\n",
              "       [ 3.0004511e+00],\n",
              "       [ 9.6152723e-03],\n",
              "       [ 2.6988866e+00],\n",
              "       [ 2.5555987e+00],\n",
              "       [ 2.1645129e-03],\n",
              "       [ 2.9215317e+00],\n",
              "       [ 3.1261355e-02],\n",
              "       [ 2.9710650e+00],\n",
              "       [ 2.9808359e+00],\n",
              "       [ 2.8276730e+00],\n",
              "       [ 3.0069437e+00],\n",
              "       [ 2.7750034e+00],\n",
              "       [ 2.8974853e+00],\n",
              "       [ 1.9261003e+00],\n",
              "       [ 1.1395346e+00],\n",
              "       [ 1.0138960e+00],\n",
              "       [ 2.9990711e+00],\n",
              "       [-3.6341846e-03],\n",
              "       [ 2.9255490e+00],\n",
              "       [ 2.0314157e-03],\n",
              "       [ 1.3578161e+00],\n",
              "       [ 1.5994537e+00],\n",
              "       [ 2.9607792e+00],\n",
              "       [ 2.9498260e+00],\n",
              "       [ 2.9425950e+00],\n",
              "       [ 2.9402721e+00],\n",
              "       [ 2.9498768e+00],\n",
              "       [ 2.9442973e+00],\n",
              "       [ 2.9489996e+00],\n",
              "       [ 2.9508286e+00],\n",
              "       [ 2.9558110e+00],\n",
              "       [ 2.9613962e+00],\n",
              "       [ 2.9574156e+00],\n",
              "       [ 2.9719701e+00],\n",
              "       [ 2.9868174e+00],\n",
              "       [ 2.9840155e+00],\n",
              "       [ 2.9894605e+00],\n",
              "       [ 2.9737067e+00],\n",
              "       [ 2.9945402e+00],\n",
              "       [ 2.9930108e+00],\n",
              "       [ 2.9849832e+00],\n",
              "       [ 2.9668086e+00],\n",
              "       [ 2.9861171e+00],\n",
              "       [ 2.9634776e+00],\n",
              "       [ 2.9910021e+00],\n",
              "       [ 2.9770501e+00],\n",
              "       [ 2.9635105e+00],\n",
              "       [ 2.9496222e+00],\n",
              "       [ 2.9647417e+00],\n",
              "       [ 2.5583932e+00],\n",
              "       [ 1.3082980e+00],\n",
              "       [ 1.4795002e+00],\n",
              "       [ 1.4636713e+00],\n",
              "       [ 1.1538197e+00],\n",
              "       [ 1.2096466e+00],\n",
              "       [ 1.3852545e+00],\n",
              "       [ 1.2063351e+00],\n",
              "       [ 1.0311607e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.1150988e+00],\n",
              "       [ 1.0910650e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.9710885e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 2.9469261e+00],\n",
              "       [ 1.3558799e+00],\n",
              "       [ 1.1340103e+00],\n",
              "       [ 6.4502537e-01],\n",
              "       [ 2.9306026e+00],\n",
              "       [ 3.0032368e+00],\n",
              "       [ 1.6807244e+00],\n",
              "       [ 1.9831717e+00],\n",
              "       [ 3.0080478e+00],\n",
              "       [ 2.9916947e+00],\n",
              "       [ 1.0161669e+00],\n",
              "       [ 2.9758167e+00],\n",
              "       [ 2.5543942e+00],\n",
              "       [ 2.0381246e+00],\n",
              "       [ 1.9417163e+00],\n",
              "       [ 1.4494418e+00],\n",
              "       [ 2.7298925e+00],\n",
              "       [ 1.6640246e-01],\n",
              "       [ 2.9481659e+00],\n",
              "       [ 2.7792687e+00],\n",
              "       [ 2.9918962e+00],\n",
              "       [ 2.9360442e+00],\n",
              "       [ 2.7195206e+00],\n",
              "       [-1.4699519e-02],\n",
              "       [-5.6181848e-03],\n",
              "       [-2.8045475e-03],\n",
              "       [ 1.8582412e+00],\n",
              "       [ 2.7890923e+00],\n",
              "       [ 3.0026007e+00],\n",
              "       [ 2.3692307e+00],\n",
              "       [ 2.9834743e+00],\n",
              "       [ 1.3489169e+00],\n",
              "       [ 1.0350711e+00],\n",
              "       [ 1.0567762e+00],\n",
              "       [ 2.9843893e+00],\n",
              "       [ 2.4726453e+00],\n",
              "       [ 2.9876120e+00],\n",
              "       [ 2.9881771e+00],\n",
              "       [ 2.9762740e+00],\n",
              "       [ 2.9933527e+00],\n",
              "       [ 2.9700146e+00],\n",
              "       [ 2.9624696e+00],\n",
              "       [ 2.9995630e+00],\n",
              "       [-2.0171702e-02],\n",
              "       [ 2.7459266e+00],\n",
              "       [ 3.0097988e+00],\n",
              "       [ 2.9505312e+00],\n",
              "       [ 3.0014873e+00],\n",
              "       [ 3.0058827e+00],\n",
              "       [ 1.7113944e+00],\n",
              "       [ 1.9065467e+00],\n",
              "       [ 2.9975512e+00],\n",
              "       [ 2.4300871e+00],\n",
              "       [ 2.9725573e+00],\n",
              "       [ 2.9814134e+00],\n",
              "       [ 2.9682040e+00],\n",
              "       [ 2.9801824e+00],\n",
              "       [ 2.9868202e+00],\n",
              "       [ 2.9923503e+00],\n",
              "       [ 1.0382124e+00],\n",
              "       [ 2.9492664e+00],\n",
              "       [ 2.9502168e+00],\n",
              "       [ 2.0706735e+00],\n",
              "       [ 2.2714891e+00],\n",
              "       [-1.3731271e-02],\n",
              "       [ 7.9446161e-01],\n",
              "       [-4.1952997e-02],\n",
              "       [ 1.0985270e+00],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 1.4679826e+00],\n",
              "       [ 1.7767603e+00],\n",
              "       [ 1.9479749e+00],\n",
              "       [ 2.9454494e+00],\n",
              "       [ 2.1299675e+00],\n",
              "       [ 2.9899845e+00],\n",
              "       [ 2.0952659e+00],\n",
              "       [ 2.7677507e+00],\n",
              "       [ 3.5393506e-02],\n",
              "       [ 2.9764888e+00],\n",
              "       [ 2.9823742e+00],\n",
              "       [ 2.2252431e+00],\n",
              "       [ 1.2409225e+00],\n",
              "       [ 4.7542053e-01],\n",
              "       [ 8.0493295e-01],\n",
              "       [ 2.6534402e-01],\n",
              "       [-2.4744123e-02],\n",
              "       [ 3.0308551e-01],\n",
              "       [ 2.8935485e+00],\n",
              "       [ 2.9021914e+00],\n",
              "       [ 2.8886397e+00],\n",
              "       [ 2.8884335e+00],\n",
              "       [ 2.8498182e+00],\n",
              "       [ 2.8786674e+00],\n",
              "       [ 2.8812599e+00],\n",
              "       [ 2.8650968e+00],\n",
              "       [ 2.8643088e+00],\n",
              "       [ 2.7877603e+00],\n",
              "       [ 2.8656912e+00],\n",
              "       [ 2.8474450e+00],\n",
              "       [ 2.8262527e+00],\n",
              "       [ 2.8337023e+00],\n",
              "       [ 2.8613312e+00],\n",
              "       [ 2.8393238e+00],\n",
              "       [ 2.8487337e+00],\n",
              "       [ 2.8564439e+00],\n",
              "       [ 2.8133459e+00],\n",
              "       [ 2.7638555e+00],\n",
              "       [ 2.9909582e+00],\n",
              "       [ 1.4322490e+00],\n",
              "       [ 1.3206783e-01],\n",
              "       [ 1.2190263e+00],\n",
              "       [ 2.7818513e+00],\n",
              "       [ 4.7604156e-01],\n",
              "       [-1.8017769e-02],\n",
              "       [ 2.5243268e+00],\n",
              "       [ 1.7665949e+00],\n",
              "       [ 1.6483318e+00],\n",
              "       [ 1.4795271e+00],\n",
              "       [ 3.0106406e+00],\n",
              "       [ 3.0119197e+00],\n",
              "       [ 3.0098791e+00],\n",
              "       [ 3.0097742e+00],\n",
              "       [ 3.0100660e+00],\n",
              "       [ 3.0077555e+00],\n",
              "       [ 3.0044041e+00],\n",
              "       [-2.1162900e-01],\n",
              "       [-1.9695172e-01],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 2.4945827e+00],\n",
              "       [ 1.4027022e+00],\n",
              "       [ 2.4617376e+00],\n",
              "       [ 2.2463553e+00],\n",
              "       [-4.8993528e-03],\n",
              "       [ 2.8965511e+00],\n",
              "       [ 6.2504011e-01],\n",
              "       [ 2.1746156e+00],\n",
              "       [ 1.5675848e+00],\n",
              "       [ 1.0130141e+00],\n",
              "       [ 2.2958581e+00],\n",
              "       [ 2.3396444e+00],\n",
              "       [ 2.5149975e+00],\n",
              "       [ 8.1658363e-05],\n",
              "       [ 1.0151930e+00],\n",
              "       [ 2.9951482e+00],\n",
              "       [ 2.6329854e+00],\n",
              "       [ 2.0028429e+00],\n",
              "       [ 2.9913368e+00],\n",
              "       [ 2.1256313e+00],\n",
              "       [ 2.9945889e+00],\n",
              "       [ 2.9927843e+00],\n",
              "       [ 2.3051162e+00],\n",
              "       [ 2.8550887e+00],\n",
              "       [ 2.9864693e+00],\n",
              "       [ 2.9719267e+00],\n",
              "       [-9.5418692e-03],\n",
              "       [-2.1577328e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0F_iMkMYorA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "203ff549-830b-4649-a0fb-0eadcd3be069"
      },
      "source": [
        "label_test.target.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(399,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z9Nl14zYopB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "d65114e8-0cdc-4cf1-aeec-dfb422285c5a"
      },
      "source": [
        "plt.title(\"comparisone between actual and predicted values\")\n",
        "#plt=plt.figure(200)\n",
        "plt.plot(predictions)\n",
        "plt.plot(label_test.target)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1736ad3908>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM3ZZiWVcj0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}