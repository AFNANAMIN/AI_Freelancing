{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning with Quantum Computing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMqEOBaL6Tnx08x2UxAgQF1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AFNANAMIN/AI_Freelancing/blob/master/Reinforcement_Learning_with_Quantum_Computing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yKAjqreuzxu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "66e8ce21-e983-416f-9c95-7822ae0e8fef"
      },
      "source": [
        "!pip install pennylane "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.6/dist-packages (0.8.1)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.6/dist-packages (from pennylane) (1.4.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pennylane) (0.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from pennylane) (2.4)\n",
            "Requirement already satisfied: semantic-version==2.6 in /usr/local/lib/python3.6/dist-packages (from pennylane) (2.6.0)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.6/dist-packages (from pennylane) (1.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pennylane) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pennylane) (1.18.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->pennylane) (4.4.2)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd->pennylane) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3ayKRgPuSZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from pennylane.optimize import NesterovMomentumOptimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "import gym\n",
        "import time\n",
        "import random\n",
        "from collections import namedtuple\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5tg29ThABzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.envs import toy_text\n",
        "class ShortestPathFrozenLake(toy_text.frozen_lake.FrozenLakeEnv):\n",
        "\tdef __init__(self, **kwargs):\n",
        "\t\tsuper(ShortestPathFrozenLake, self).__init__(**kwargs)\n",
        "\n",
        "\t\tfor state in range(self.nS): # for all states\n",
        "\t\t\tfor action in range(self.nA): # for all actions\n",
        "\t\t\t\tmy_transitions = []\n",
        "\t\t\t\tfor (prob, next_state, _, is_terminal) in self.P[state][action]:\n",
        "\t\t\t\t\trow = next_state // self.ncol\n",
        "\t\t\t\t\tcol = next_state - row * self.ncol\n",
        "\t\t\t\t\ttile_type = self.desc[row, col]\n",
        "\t\t\t\t\tif tile_type == b'H':\n",
        "\t\t\t\t\t\treward = -0.2\n",
        "\t\t\t\t\telif tile_type == b'G':\n",
        "\t\t\t\t\t\treward = 1.\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\treward = -0.01\n",
        "\n",
        "\t\t\t\t\tmy_transitions.append((prob, next_state, reward, is_terminal))\n",
        "\t\t\t\tself.P[state][action] = my_transitions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG51TdI-ukRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym.envs.registration import register\n",
        "register(\n",
        "    id='Deterministic-ShortestPath-4x4-FrozenLake-v0', # name given to this new environment\n",
        "    entry_point='ShortestPathFrozenLake:ShortestPathFrozenLake', # env entry point\n",
        "    kwargs={'map_name': '4x4', 'is_slippery': False} # argument passed to the env\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCQgS-QVu3UE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "\t\t\t\t\t\t('state', 'action', 'reward', 'next_state', 'done'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwQ4asilvZ85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "\tdef __init__(self, capacity):\n",
        "\t\tself.capacity = capacity\n",
        "\t\tself.memory = []\n",
        "\t\tself.position = 0\n",
        "\n",
        "\tdef push(self, *args):\n",
        "\t\t\"\"\"Saves a transition.\"\"\"\n",
        "\t\tif len(self.memory) < self.capacity:\n",
        "\t\t\tself.memory.append(None)\n",
        "\t\tself.memory[self.position] = Transition(*args)\n",
        "\t\tself.position = (self.position + 1) % self.capacity\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\treturn random.sample(self.memory, batch_size)\n",
        "\n",
        "\tdef output_all(self):\n",
        "\t\treturn self.memory\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rWJdHxAvgx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotTrainingResultCombined(_iter_index, _iter_reward, _iter_total_steps, _fileTitle):\n",
        "\tfig, ax = plt.subplots()\n",
        "\t# plt.yscale('log')\n",
        "\tax.plot(_iter_index, _iter_reward, '-b', label='Reward')\n",
        "\tax.plot(_iter_index, _iter_total_steps, '-r', label='Total Steps')\n",
        "\tleg = ax.legend();\n",
        "\n",
        "\tax.set(xlabel='Iteration Index', \n",
        "\t\t   title=_fileTitle)\n",
        "\tfig.savefig(_fileTitle + \"_\"+ datetime.now().strftime(\"NO%Y%m%d%H%M%S\") + \".png\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU5CFzmEvjKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotTrainingResultReward(_iter_index, _iter_reward, _iter_total_steps, _fileTitle):\n",
        "\tfig, ax = plt.subplots()\n",
        "\t# plt.yscale('log')\n",
        "\tax.plot(_iter_index, _iter_reward, '-b', label='Reward')\n",
        "\t# ax.plot(_iter_index, _iter_total_steps, '-r', label='Total Steps')\n",
        "\tleg = ax.legend();\n",
        "\n",
        "\tax.set(xlabel='Iteration Index', \n",
        "\t\t   title=_fileTitle)\n",
        "\tfig.savefig(_fileTitle + \"_REWARD\" + \"_\"+ datetime.now().strftime(\"NO%Y%m%d%H%M%S\") + \".png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGN6Nmm1v2lO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decimalToBinaryFixLength(_length, _decimal):\n",
        "\tbinNum = bin(int(_decimal))[2:]\n",
        "\toutputNum = [int(item) for item in binNum]\n",
        "\tif len(outputNum) < _length:\n",
        "\t\toutputNum = np.concatenate((np.zeros((_length-len(outputNum),)),np.array(outputNum)))\n",
        "\telse:\n",
        "\t\toutputNum = np.array(outputNum)\n",
        "\treturn outputNum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ5gtZ_Yv5-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtype = torch.DoubleTensor\n",
        "\n",
        "## Define a FOUR qubit system\n",
        "dev = qml.device('default.qubit', wires=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uovu5Mhqv-1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def statepreparation(a):\n",
        "\n",
        "\t\n",
        "\t# Rot to computational basis encoding\n",
        "\t# a = [a_0, a_1, a_2, a_3, a_4, a_5, a_6, a_7, a_8]\n",
        "\n",
        "\tfor ind in range(len(a)):\n",
        "\t\tqml.RX(np.pi * a[ind], wires=ind)\n",
        "\t\tqml.RZ(np.pi * a[ind], wires=ind)\n",
        "\n",
        "\n",
        "def layer(W):\n",
        "\t\"\"\" Single layer of the variational classifier.\n",
        "\tArgs:\n",
        "\t\tW (array[float]): 2-d array of variables for one layer\n",
        "\t\"\"\"\n",
        "\n",
        "\tqml.CNOT(wires=[0, 1])\n",
        "\tqml.CNOT(wires=[1, 2])\n",
        "\tqml.CNOT(wires=[2, 3])\n",
        "\n",
        "\n",
        "\tqml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
        "\tqml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n",
        "\tqml.Rot(W[2, 0], W[2, 1], W[2, 2], wires=2)\n",
        "\tqml.Rot(W[3, 0], W[3, 1], W[3, 2], wires=3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_SMYhbxw-68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "65dcf71c-c78e-4746-fa22-0217eeeb9993"
      },
      "source": [
        "qml.about()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: PennyLane\n",
            "Version: 0.8.1\n",
            "Summary: PennyLane is a Python quantum machine learning library by Xanadu Inc.\n",
            "Home-page: https://github.com/XanaduAI/pennylane\n",
            "Author: None\n",
            "Author-email: None\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: scipy, networkx, appdirs, toml, autograd, semantic-version, numpy\n",
            "Required-by: \n",
            "Platform info:           Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic\n",
            "Python version:          3.6.9\n",
            "Numpy version:           1.18.2\n",
            "Scipy version:           1.4.1\n",
            "Installed devices:\n",
            "- default.gaussian (PennyLane-0.8.1)\n",
            "- default.qubit (PennyLane-0.8.1)\n",
            "- default.tensor (PennyLane-0.8.1)\n",
            "- default.tensor.tf (PennyLane-0.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aHLjDO-xjfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@qml.qnode(dev, interface='torch')\n",
        "def circuit(weights, angles=None):\n",
        "\n",
        "\n",
        "\tstatepreparation(angles)\n",
        "\t\n",
        "\tfor W in weights:\n",
        "\t\tlayer(W)\n",
        "\n",
        "\treturn [qml.expval(qml.PauliZ(ind)) for ind in range(4)]\n",
        "\n",
        "\n",
        "def variational_classifier(var_Q_circuit, var_Q_bias , angles=None):\n",
        "\t\"\"\"The variational classifier.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\tweights = var_Q_circuit\n",
        "\t\n",
        "\t\n",
        "\traw_output = circuit(weights, angles=angles) + var_Q_bias\n",
        "\n",
        "\treturn raw_output\n",
        "\n",
        "\n",
        "def square_loss(labels, predictions):\n",
        "\t\n",
        "\tloss = 0\n",
        "\tfor l, p in zip(labels, predictions):\n",
        "\t    loss = loss + (l - p) ** 2\n",
        "\tloss = loss / len(labels)\n",
        "\t\n",
        "\n",
        "\treturn loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl9663nmyCux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def abs_loss(labels, predictions):\n",
        "\t\"\"\" Square loss function\n",
        "\tArgs:\n",
        "\t\tlabels (array[float]): 1-d array of labels\n",
        "\t\tpredictions (array[float]): 1-d array of predictions\n",
        "\tReturns:\n",
        "\t\tfloat: square loss\n",
        "\t\"\"\"\n",
        "\t\n",
        "\toutput = torch.abs(predictions - labels)\n",
        "\toutput = torch.sum(output) / len(labels)\n",
        "\t# output = loss(torch.tensor(predictions), torch.tensor(labels))\n",
        "\t# print(\"LOSS OUTPUT\")\n",
        "\t# print(output)\n",
        "\n",
        "\treturn output\n",
        "\n",
        "def huber_loss(labels, predictions):\n",
        "\n",
        "\n",
        "\t# loss = nn.MSELoss()\n",
        "\tloss = nn.SmoothL1Loss()\n",
        "\t# output = loss(torch.tensor(predictions), torch.tensor(labels))\n",
        "\t# print(\"LOSS OUTPUT\")\n",
        "\t# print(output)\n",
        "\n",
        "\treturn loss(labels, predictions)\n",
        "\n",
        "\n",
        "def cost(var_Q_circuit, var_Q_bias, features, labels):\n",
        "\t\"\"\"Cost (error) function to be minimized.\"\"\"\n",
        "\n",
        "\t# predictions = [variational_classifier(weights, angles=f) for f in features]\n",
        "\t# Torch data type??\n",
        "\t\n",
        "\tpredictions = [variational_classifier(var_Q_circuit = var_Q_circuit, var_Q_bias = var_Q_bias, angles=decimalToBinaryFixLength(4,item.state))[item.action] for item in features]\n",
        "\n",
        "\n",
        "\treturn square_loss(labels, predictions)\n",
        "\n",
        "\n",
        "#############################\n",
        "\n",
        "def epsilon_greedy(var_Q_circuit, var_Q_bias, epsilon, n_actions, s, train=False):\n",
        "\t\"\"\"\n",
        "\t@param Q Q values state x action -> value\n",
        "\t@param epsilon for exploration\n",
        "\t@param s number of states\n",
        "\t@param train if true then no random actions selected\n",
        "\t\"\"\"\n",
        "\n",
        "\t\n",
        "\n",
        "\n",
        "\tif train or np.random.rand() < ((epsilon/n_actions)+(1-epsilon)):\n",
        "\t\t# action = np.argmax(Q[s, :])\n",
        "\t\t# variational classifier output is torch tensor\n",
        "\t\t# action = np.argmax(variational_classifier(var_Q_circuit = var_Q_circuit, var_Q_bias = var_Q_bias, angles = decimalToBinaryFixLength(9,s)))\n",
        "\t\taction = torch.argmax(variational_classifier(var_Q_circuit = var_Q_circuit, var_Q_bias = var_Q_bias, angles = decimalToBinaryFixLength(4,s)))\n",
        "\telse:\n",
        "\t\t# need to be torch tensor\n",
        "\t\taction = torch.tensor(np.random.randint(0, n_actions))\n",
        "\treturn action\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvtKX2nfyoQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deep_Q_Learning(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
        "\n",
        "\n",
        "\t\n",
        "\tenv = gym.make('Deterministic-ShortestPath-4x4-FrozenLake-v0')\n",
        "\n",
        "\tn_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "\tprint(\"NUMBER OF STATES:\" + str(n_states))\n",
        "\tprint(\"NUMBER OF ACTIONS:\" + str(n_actions))\n",
        "\n",
        "\t\n",
        "\tnum_qubits = 4\n",
        "\tnum_layers = 2\n",
        "\t# var_init = (0.01 * np.random.randn(num_layers, num_qubits, 3), 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
        "\n",
        "\tvar_init_circuit = Variable(torch.tensor(0.01 * np.random.randn(num_layers, num_qubits, 3), device='cpu').type(dtype), requires_grad=True)\n",
        "\tvar_init_bias = Variable(torch.tensor([0.0, 0.0, 0.0, 0.0], device='cpu').type(dtype), requires_grad=True)\n",
        "\n",
        "\n",
        "\tvar_Q_circuit = var_init_circuit\n",
        "\tvar_Q_bias = var_init_bias\n",
        "\t# print(\"INIT PARAMS\")\n",
        "\t# print(var_Q_circuit)\n",
        "\n",
        "\tvar_target_Q_circuit = var_Q_circuit.clone().detach()\n",
        "\tvar_target_Q_bias = var_Q_bias.clone().detach()\n",
        "\n",
        "\topt = torch.optim.RMSprop([var_Q_circuit, var_Q_bias], lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
        "\n",
        "\t## NEed to move out of the function\n",
        "\tTARGET_UPDATE = 20\n",
        "\tbatch_size = 5\n",
        "\tOPTIMIZE_STEPS = 5\n",
        "\t##\n",
        "\n",
        "\n",
        "\ttarget_update_counter = 0\n",
        "\n",
        "\titer_index = []\n",
        "\titer_reward = []\n",
        "\titer_total_steps = []\n",
        "\n",
        "\tcost_list = []\n",
        "\n",
        "\n",
        "\ttimestep_reward = []\n",
        "\n",
        "\n",
        "\t\n",
        "\tmemory = ReplayMemory(80)\n",
        "\n",
        "\t\n",
        "\n",
        "\n",
        "\tfor episode in range(episodes):\n",
        "\t\tprint(f\"Episode: {episode}\")\n",
        "\t\t# Output a s in decimal format\n",
        "\t\ts = env.reset()\n",
        "\t\t# Doing epsilog greedy action selection\n",
        "\t\t# With var_Q\n",
        "\t\ta = epsilon_greedy(var_Q_circuit = var_Q_circuit, var_Q_bias = var_Q_bias, epsilon = epsilon, n_actions = n_actions, s = s).item()\n",
        "\t\tt = 0\n",
        "\t\ttotal_reward = 0\n",
        "\t\tdone = False\n",
        "\n",
        "\n",
        "\t\twhile t < max_steps:\n",
        "\t\t\tif render:\n",
        "\t\t\t\tprint(\"###RENDER###\")\n",
        "\t\t\t\tenv.render()\n",
        "\t\t\t\tprint(\"###RENDER###\")\n",
        "\t\t\tt += 1\n",
        "\n",
        "\t\t\ttarget_update_counter += 1\n",
        "\n",
        "\t\t\t# Execute the action \n",
        "\t\t\ts_, reward, done, info = env.step(a)\n",
        "\t\t\n",
        "\t\t\ttotal_reward += reward\n",
        "\t\t\t# a_ = np.argmax(Q[s_, :])\n",
        "\t\t\ta_ = epsilon_greedy(var_Q_circuit = var_Q_circuit, var_Q_bias = var_Q_bias, epsilon = epsilon, n_actions = n_actions, s = s_).item()\n",
        "\t\t\t\n",
        "\t\t\t# print(\"ACTION:\")\n",
        "\t\t\t# print(a_)\n",
        "\n",
        "\t\t\tmemory.push(s, a, reward, s_, done)\n",
        "\n",
        "\t\t\tif len(memory) > batch_size:\n",
        "\n",
        "\n",
        "\n",
        "\t\t\t\tbatch_sampled = memory.sample(batch_size = batch_size)\n",
        "\n",
        "\t\t\t\n",
        "\n",
        "\t\t\t\tQ_target = [item.reward + (1 - int(item.done)) * gamma * torch.max(variational_classifier(var_Q_circuit = var_target_Q_circuit, var_Q_bias = var_target_Q_bias, angles=decimalToBinaryFixLength(4,item.next_state))) for item in batch_sampled]\n",
        "\t\t\t\n",
        "\n",
        "\t\t\t\tdef closure():\n",
        "\t\t\t\t\topt.zero_grad()\n",
        "\t\t\t\t\tloss = cost(var_Q_circuit = var_Q_circuit, var_Q_bias = var_Q_bias, features = batch_sampled, labels = Q_target)\n",
        "\t\t\t\t\t# print(loss)\n",
        "\t\t\t\t\tloss.backward()\n",
        "\t\t\t\t\treturn loss\n",
        "\t\t\t\topt.step(closure)\n",
        "\n",
        "\t\t\t\t# print(\"UPDATING PARAMS COMPLETED\")\n",
        "\t\t\t\tcurrent_replay_memory = memory.output_all()\n",
        "\t\t\t\tcurrent_target_for_replay_memory = [item.reward + (1 - int(item.done)) * gamma * torch.max(variational_classifier(var_Q_circuit = var_target_Q_circuit, var_Q_bias = var_target_Q_bias, angles=decimalToBinaryFixLength(4,item.next_state))) for item in current_replay_memory]\n",
        "\t\t\t\t# current_target_for_replay_memory = [item.reward + (1 - int(item.done)) * gamma * np.max(variational_classifier(var_target_Q, angles=decimalToBinaryFixLength(9,item.next_state))) for item in current_replay_memory]\n",
        "\n",
        "\t\t\t\n",
        "\n",
        "\t\t\tif target_update_counter > TARGET_UPDATE:\n",
        "\t\t\t\tprint(\"UPDATEING TARGET CIRCUIT...\")\n",
        "\n",
        "\t\t\t\tvar_target_Q_circuit = var_Q_circuit.clone().detach()\n",
        "\t\t\t\tvar_target_Q_bias = var_Q_bias.clone().detach()\n",
        "\t\t\t\t\n",
        "\t\t\t\ttarget_update_counter = 0\n",
        "\n",
        "\t\t\ts, a = s_, a_\n",
        "\n",
        "\t\t\tif done:\n",
        "\t\t\t\tif render:\n",
        "\t\t\t\t\tprint(\"###FINAL RENDER###\")\n",
        "\t\t\t\t\tenv.render()\n",
        "\t\t\t\t\tprint(\"###FINAL RENDER###\")\n",
        "\t\t\t\t\tprint(f\"This episode took {t} timesteps and reward: {total_reward}\")\n",
        "\t\t\t\tepsilon = epsilon / ((episode/100) + 1)\n",
        "\t\t\t\t# print(\"Q Circuit Params:\")\n",
        "\t\t\t\t# print(var_Q_circuit)\n",
        "\t\t\t\tprint(f\"This episode took {t} timesteps and reward: {total_reward}\")\n",
        "\t\t\t\ttimestep_reward.append(total_reward)\n",
        "\t\t\t\titer_index.append(episode)\n",
        "\t\t\t\titer_reward.append(total_reward)\n",
        "\t\t\t\titer_total_steps.append(t)\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\treturn timestep_reward, iter_index, iter_reward, iter_total_steps, var_Q_circuit, var_Q_bias\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaQ6KscqyROq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e51dbf6-111a-4390-dcc1-17e156e4a398"
      },
      "source": [
        "if __name__ ==\"__main__\":\n",
        "\talpha = 0.4\n",
        "\tgamma = 0.999\n",
        "\tepsilon = 1.\n",
        "\tepisodes = 50\n",
        "\tmax_steps = 2500\n",
        "\tn_tests = 2\n",
        "\ttimestep_reward, iter_index, iter_reward, iter_total_steps , var_Q_circuit, var_Q_bias = deep_Q_Learning(alpha, gamma, epsilon, episodes, max_steps, n_tests, test = False)\n",
        "\t\n",
        "\tprint(timestep_reward)\n",
        "\t\n",
        "\n",
        "\t## Drawing Training Result ##\n",
        "\tfile_title = 'VQDQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp' + datetime.now().strftime(\"NO%Y%m%d%H%M%S\")\n",
        "\t\n",
        "\tplotTrainingResultReward(_iter_index = iter_index, _iter_reward = iter_reward, _iter_total_steps = iter_total_steps, _fileTitle = 'Quantum_DQN_Frozen_Lake_NonSlip_Dynamic_Epsilon_RMSProp')\n",
        "\n",
        "\t## Saving the model\n",
        "\twith open(file_title + \"_var_Q_circuit\" + \".txt\", \"wb\") as fp:\n",
        "\t\t\tpickle.dump(var_Q_circuit, fp)\n",
        "\n",
        "\twith open(file_title + \"_var_Q_bias\" + \".txt\", \"wb\") as fp:\n",
        "\t\t\tpickle.dump(var_Q_bias, fp)\n",
        "\n",
        "\twith open(file_title + \"_iter_reward\" + \".txt\", \"wb\") as fp:\n",
        "\t\t\tpickle.dump(iter_reward, fp)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NUMBER OF STATES:16\n",
            "NUMBER OF ACTIONS:4\n",
            "Episode: 0\n",
            "This episode took 3 timesteps and reward: -0.22\n",
            "Episode: 1\n",
            "This episode took 10 timesteps and reward: -0.29000000000000004\n",
            "Episode: 2\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 9 timesteps and reward: -0.28\n",
            "Episode: 3\n",
            "This episode took 3 timesteps and reward: -0.22\n",
            "Episode: 4\n",
            "This episode took 4 timesteps and reward: -0.23\n",
            "Episode: 5\n",
            "This episode took 12 timesteps and reward: -0.31\n",
            "Episode: 6\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 4 timesteps and reward: -0.23\n",
            "Episode: 7\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 21 timesteps and reward: -0.4\n",
            "Episode: 8\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 24 timesteps and reward: -0.43000000000000005\n",
            "Episode: 9\n",
            "This episode took 3 timesteps and reward: -0.22\n",
            "Episode: 10\n",
            "This episode took 2 timesteps and reward: -0.21000000000000002\n",
            "Episode: 11\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 91 timesteps and reward: -1.1000000000000005\n",
            "Episode: 12\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 50 timesteps and reward: -0.6900000000000003\n",
            "Episode: 13\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 103 timesteps and reward: -1.2200000000000006\n",
            "Episode: 14\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 24 timesteps and reward: -0.43000000000000005\n",
            "Episode: 15\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 50 timesteps and reward: -0.6900000000000003\n",
            "Episode: 16\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 49 timesteps and reward: -0.6800000000000003\n",
            "Episode: 17\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 152 timesteps and reward: -1.710000000000001\n",
            "Episode: 18\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 71 timesteps and reward: -0.9000000000000004\n",
            "Episode: 19\n",
            "This episode took 4 timesteps and reward: -0.23\n",
            "Episode: 20\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 307 timesteps and reward: -3.259999999999979\n",
            "Episode: 21\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 14 timesteps and reward: -0.32999999999999996\n",
            "Episode: 22\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 393 timesteps and reward: -4.11999999999996\n",
            "Episode: 23\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 32 timesteps and reward: -0.5100000000000001\n",
            "Episode: 24\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 53 timesteps and reward: -0.7200000000000002\n",
            "Episode: 25\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "This episode took 131 timesteps and reward: -1.5000000000000009\n",
            "Episode: 26\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n",
            "UPDATEING TARGET CIRCUIT...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohB0CvGY2AUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}