{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AFNANAMIN/AI_Freelancing/blob/master/task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpQZPnKafTay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg0SMPc9ferl",
        "colab_type": "code",
        "outputId": "d84b1fbf-ed59-4019-ddf8-18b8f9a22928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8znfBi2Mfg-c",
        "colab_type": "code",
        "outputId": "25fe0767-68f4-4a61-82d7-51509402a9eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/task2/"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/task2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUzLl2_ngHhF",
        "colab_type": "code",
        "outputId": "c6e43a34-24ce-4acf-d6c9-df4af3eb404f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w1.txt\t\t\t   Weights-019--1.23467.hdf5  Weights-424--1.09380.hdf5\n",
            "w2.txt\t\t\t   Weights-020--1.22652.hdf5  Weights-469--1.06395.hdf5\n",
            "Weights-001--1.81072.hdf5  Weights-028--1.20386.hdf5  Weights-482--1.05137.hdf5\n",
            "Weights-002--1.70528.hdf5  Weights-031--1.14942.hdf5  Weights-483--1.04641.hdf5\n",
            "Weights-003--1.63445.hdf5  Weights-282--1.14684.hdf5  X_test.csv\n",
            "Weights-006--1.42797.hdf5  Weights-331--1.13016.hdf5  X_train.csv\n",
            "Weights-009--1.33325.hdf5  Weights-346--1.11884.hdf5  Y_test.csv\n",
            "Weights-012--1.24048.hdf5  Weights-368--1.10558.hdf5  Y_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbGSIm9ugf40",
        "colab_type": "code",
        "outputId": "555ccbb5-5fed-4a51-d16a-138c6b45db69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "train=pd.read_csv('X_train.csv',header=None)\n",
        "test=pd.read_csv('X_test.csv',header=None)\n",
        "label_train=pd.read_csv('Y_train.csv',names=['target'])\n",
        "label_test=pd.read_csv('Y_test.csv',names=['target'])\n",
        "merged_train = train.join(label_train)\n",
        "merged_train"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>859</th>\n",
              "      <th>860</th>\n",
              "      <th>861</th>\n",
              "      <th>862</th>\n",
              "      <th>863</th>\n",
              "      <th>864</th>\n",
              "      <th>865</th>\n",
              "      <th>866</th>\n",
              "      <th>867</th>\n",
              "      <th>868</th>\n",
              "      <th>869</th>\n",
              "      <th>870</th>\n",
              "      <th>871</th>\n",
              "      <th>872</th>\n",
              "      <th>873</th>\n",
              "      <th>874</th>\n",
              "      <th>875</th>\n",
              "      <th>876</th>\n",
              "      <th>877</th>\n",
              "      <th>878</th>\n",
              "      <th>879</th>\n",
              "      <th>880</th>\n",
              "      <th>881</th>\n",
              "      <th>882</th>\n",
              "      <th>883</th>\n",
              "      <th>884</th>\n",
              "      <th>885</th>\n",
              "      <th>886</th>\n",
              "      <th>887</th>\n",
              "      <th>888</th>\n",
              "      <th>889</th>\n",
              "      <th>890</th>\n",
              "      <th>891</th>\n",
              "      <th>892</th>\n",
              "      <th>893</th>\n",
              "      <th>894</th>\n",
              "      <th>895</th>\n",
              "      <th>896</th>\n",
              "      <th>897</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>77</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>92</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>121</td>\n",
              "      <td>0</td>\n",
              "      <td>79</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>647</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>647</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>223</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>127</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>37</td>\n",
              "      <td>149</td>\n",
              "      <td>0</td>\n",
              "      <td>226</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1143</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>1143</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>225</td>\n",
              "      <td>6</td>\n",
              "      <td>22</td>\n",
              "      <td>16</td>\n",
              "      <td>127</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>59</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>228</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>65.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1230</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>1230</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>336</td>\n",
              "      <td>10</td>\n",
              "      <td>25</td>\n",
              "      <td>40</td>\n",
              "      <td>243</td>\n",
              "      <td>29</td>\n",
              "      <td>45</td>\n",
              "      <td>93</td>\n",
              "      <td>314</td>\n",
              "      <td>6</td>\n",
              "      <td>341</td>\n",
              "      <td>14</td>\n",
              "      <td>9</td>\n",
              "      <td>25</td>\n",
              "      <td>40</td>\n",
              "      <td>2</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>41.250000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>2159</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>2159</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>13</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>457</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>53</td>\n",
              "      <td>277</td>\n",
              "      <td>29</td>\n",
              "      <td>45</td>\n",
              "      <td>105</td>\n",
              "      <td>331</td>\n",
              "      <td>6</td>\n",
              "      <td>463</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>30</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>39.200000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>2586</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>2586</td>\n",
              "      <td>1553.000000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>40</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>204</td>\n",
              "      <td>4</td>\n",
              "      <td>46</td>\n",
              "      <td>17</td>\n",
              "      <td>51</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>75</td>\n",
              "      <td>192</td>\n",
              "      <td>0</td>\n",
              "      <td>208</td>\n",
              "      <td>44</td>\n",
              "      <td>19</td>\n",
              "      <td>47</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>44</td>\n",
              "      <td>66.181818</td>\n",
              "      <td>0.244949</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1385</td>\n",
              "      <td>18</td>\n",
              "      <td>54.485322</td>\n",
              "      <td>5</td>\n",
              "      <td>1385</td>\n",
              "      <td>1156.777778</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1196</th>\n",
              "      <td>40</td>\n",
              "      <td>6</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>209</td>\n",
              "      <td>4</td>\n",
              "      <td>52</td>\n",
              "      <td>18</td>\n",
              "      <td>64</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>212</td>\n",
              "      <td>0</td>\n",
              "      <td>213</td>\n",
              "      <td>44</td>\n",
              "      <td>19</td>\n",
              "      <td>53</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>6</td>\n",
              "      <td>46</td>\n",
              "      <td>73.500000</td>\n",
              "      <td>0.252315</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>1473</td>\n",
              "      <td>18</td>\n",
              "      <td>54.485322</td>\n",
              "      <td>5</td>\n",
              "      <td>1473</td>\n",
              "      <td>1156.777778</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1197</th>\n",
              "      <td>42</td>\n",
              "      <td>6</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>214</td>\n",
              "      <td>4</td>\n",
              "      <td>54</td>\n",
              "      <td>18</td>\n",
              "      <td>64</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>88</td>\n",
              "      <td>221</td>\n",
              "      <td>0</td>\n",
              "      <td>218</td>\n",
              "      <td>47</td>\n",
              "      <td>19</td>\n",
              "      <td>55</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>48</td>\n",
              "      <td>71.307692</td>\n",
              "      <td>0.232906</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>1531</td>\n",
              "      <td>18</td>\n",
              "      <td>54.485322</td>\n",
              "      <td>5</td>\n",
              "      <td>1531</td>\n",
              "      <td>1156.777778</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1198</th>\n",
              "      <td>43</td>\n",
              "      <td>9</td>\n",
              "      <td>14</td>\n",
              "      <td>5</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>277</td>\n",
              "      <td>4</td>\n",
              "      <td>54</td>\n",
              "      <td>19</td>\n",
              "      <td>145</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>88</td>\n",
              "      <td>334</td>\n",
              "      <td>0</td>\n",
              "      <td>282</td>\n",
              "      <td>76</td>\n",
              "      <td>19</td>\n",
              "      <td>55</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>71</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>7</td>\n",
              "      <td>48</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>0.287698</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>1962</td>\n",
              "      <td>18</td>\n",
              "      <td>54.485322</td>\n",
              "      <td>5</td>\n",
              "      <td>1962</td>\n",
              "      <td>1156.777778</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1199</th>\n",
              "      <td>45</td>\n",
              "      <td>9</td>\n",
              "      <td>15</td>\n",
              "      <td>5</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>279</td>\n",
              "      <td>4</td>\n",
              "      <td>54</td>\n",
              "      <td>21</td>\n",
              "      <td>148</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>88</td>\n",
              "      <td>339</td>\n",
              "      <td>0</td>\n",
              "      <td>285</td>\n",
              "      <td>77</td>\n",
              "      <td>20</td>\n",
              "      <td>55</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>48</td>\n",
              "      <td>65.466667</td>\n",
              "      <td>0.335185</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.066667</td>\n",
              "      <td>1999</td>\n",
              "      <td>18</td>\n",
              "      <td>54.485322</td>\n",
              "      <td>5</td>\n",
              "      <td>1999</td>\n",
              "      <td>1156.777778</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1200 rows × 899 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0   1   2   3         4  ...        894  895   896          897  target\n",
              "0     11   3   0   4 -1.000000  ...  36.990000    2   647  1553.000000       3\n",
              "1     14   4   1   6 -1.000000  ...  36.990000    2  1143  1553.000000       0\n",
              "2     14   4   2   6 -1.000000  ...  36.990000    2  1230  1553.000000       3\n",
              "3     24   9   4  10 -1.000000  ...  36.990000    2  2159  1553.000000       2\n",
              "4     28  10   5  13 -1.000000  ...  36.990000    2  2586  1553.000000       3\n",
              "...   ..  ..  ..  ..       ...  ...        ...  ...   ...          ...     ...\n",
              "1195  40   6  11   5  0.333333  ...  54.485322    5  1385  1156.777778       1\n",
              "1196  40   6  12   5  0.333333  ...  54.485322    5  1473  1156.777778       0\n",
              "1197  42   6  13   5  0.333333  ...  54.485322    5  1531  1156.777778       3\n",
              "1198  43   9  14   5  0.333333  ...  54.485322    5  1962  1156.777778       3\n",
              "1199  45   9  15   5  1.000000  ...  54.485322    5  1999  1156.777778       0\n",
              "\n",
              "[1200 rows x 899 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN59qo5qRJzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine=train.append(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5aH68bBRJ2v",
        "colab_type": "code",
        "outputId": "d7b28755-266d-405f-9cea-8917f1cd5472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "combine"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>858</th>\n",
              "      <th>859</th>\n",
              "      <th>860</th>\n",
              "      <th>861</th>\n",
              "      <th>862</th>\n",
              "      <th>863</th>\n",
              "      <th>864</th>\n",
              "      <th>865</th>\n",
              "      <th>866</th>\n",
              "      <th>867</th>\n",
              "      <th>868</th>\n",
              "      <th>869</th>\n",
              "      <th>870</th>\n",
              "      <th>871</th>\n",
              "      <th>872</th>\n",
              "      <th>873</th>\n",
              "      <th>874</th>\n",
              "      <th>875</th>\n",
              "      <th>876</th>\n",
              "      <th>877</th>\n",
              "      <th>878</th>\n",
              "      <th>879</th>\n",
              "      <th>880</th>\n",
              "      <th>881</th>\n",
              "      <th>882</th>\n",
              "      <th>883</th>\n",
              "      <th>884</th>\n",
              "      <th>885</th>\n",
              "      <th>886</th>\n",
              "      <th>887</th>\n",
              "      <th>888</th>\n",
              "      <th>889</th>\n",
              "      <th>890</th>\n",
              "      <th>891</th>\n",
              "      <th>892</th>\n",
              "      <th>893</th>\n",
              "      <th>894</th>\n",
              "      <th>895</th>\n",
              "      <th>896</th>\n",
              "      <th>897</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>77</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>92</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>121</td>\n",
              "      <td>0</td>\n",
              "      <td>79</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>647</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>647</td>\n",
              "      <td>1553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>223</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>127</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>37</td>\n",
              "      <td>149</td>\n",
              "      <td>0</td>\n",
              "      <td>226</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>39.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1143</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>1143</td>\n",
              "      <td>1553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>225</td>\n",
              "      <td>6</td>\n",
              "      <td>22</td>\n",
              "      <td>16</td>\n",
              "      <td>127</td>\n",
              "      <td>14</td>\n",
              "      <td>31</td>\n",
              "      <td>59</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>228</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>65.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1230</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>1230</td>\n",
              "      <td>1553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>336</td>\n",
              "      <td>10</td>\n",
              "      <td>25</td>\n",
              "      <td>40</td>\n",
              "      <td>243</td>\n",
              "      <td>29</td>\n",
              "      <td>45</td>\n",
              "      <td>93</td>\n",
              "      <td>314</td>\n",
              "      <td>6</td>\n",
              "      <td>341</td>\n",
              "      <td>14</td>\n",
              "      <td>9</td>\n",
              "      <td>25</td>\n",
              "      <td>40</td>\n",
              "      <td>2</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>41.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.5</td>\n",
              "      <td>2159</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>2159</td>\n",
              "      <td>1553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>13</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>9</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>457</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>53</td>\n",
              "      <td>277</td>\n",
              "      <td>29</td>\n",
              "      <td>45</td>\n",
              "      <td>105</td>\n",
              "      <td>331</td>\n",
              "      <td>6</td>\n",
              "      <td>463</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>30</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>2</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>39.20</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.6</td>\n",
              "      <td>2586</td>\n",
              "      <td>5</td>\n",
              "      <td>36.990000</td>\n",
              "      <td>2</td>\n",
              "      <td>2586</td>\n",
              "      <td>1553.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>103</td>\n",
              "      <td>2</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>103</td>\n",
              "      <td>125.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>34.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>147</td>\n",
              "      <td>2</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>147</td>\n",
              "      <td>125.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>15</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>201</td>\n",
              "      <td>7</td>\n",
              "      <td>64.138095</td>\n",
              "      <td>5</td>\n",
              "      <td>201</td>\n",
              "      <td>1779.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>21</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>131</td>\n",
              "      <td>4</td>\n",
              "      <td>22</td>\n",
              "      <td>6</td>\n",
              "      <td>69</td>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>23</td>\n",
              "      <td>156</td>\n",
              "      <td>0</td>\n",
              "      <td>134</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>22</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>121.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>833</td>\n",
              "      <td>7</td>\n",
              "      <td>64.138095</td>\n",
              "      <td>5</td>\n",
              "      <td>833</td>\n",
              "      <td>1779.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>21</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>151</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>8</td>\n",
              "      <td>73</td>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>26</td>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "      <td>155</td>\n",
              "      <td>31</td>\n",
              "      <td>20</td>\n",
              "      <td>26</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>95.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>923</td>\n",
              "      <td>7</td>\n",
              "      <td>64.138095</td>\n",
              "      <td>5</td>\n",
              "      <td>923</td>\n",
              "      <td>1779.142857</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1599 rows × 898 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3    4    5    ...   892  893        894  895   896          897\n",
              "0     11    3    0    4 -1.0 -1.0  ...   647    5  36.990000    2   647  1553.000000\n",
              "1     14    4    1    6 -1.0  1.0  ...  1143    5  36.990000    2  1143  1553.000000\n",
              "2     14    4    2    6 -1.0  1.0  ...  1230    5  36.990000    2  1230  1553.000000\n",
              "3     24    9    4   10 -1.0  0.0  ...  2159    5  36.990000    2  2159  1553.000000\n",
              "4     28   10    5   13 -1.0  0.5  ...  2586    5  36.990000    2  2586  1553.000000\n",
              "..   ...  ...  ...  ...  ...  ...  ...   ...  ...        ...  ...   ...          ...\n",
              "394    7    1    0    0 -1.0 -1.0  ...   103    2  17.000000    2   103   125.000000\n",
              "395    8    1    1    0 -1.0  1.0  ...   147    2  17.000000    2   147   125.000000\n",
              "396    7    1    0    2 -1.0 -1.0  ...   201    7  64.138095    5   201  1779.142857\n",
              "397   21    8    1    4 -1.0 -1.0  ...   833    7  64.138095    5   833  1779.142857\n",
              "398   21    8    2    5  1.0 -1.0  ...   923    7  64.138095    5   923  1779.142857\n",
              "\n",
              "[1599 rows x 898 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-ZMLzhppE21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "87a2ac29-6028-49d5-e6f3-058059ed2da9"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_scale = min_max_scaler.fit_transform(combine)\n",
        "#X_scale = min_max_scaler.fit_transform(train_x_test)\n",
        "X_scale.shape"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1599, 898)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO_BNidaVKis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_combined():\n",
        "    global X_scale\n",
        "    train = X_scale[:1200]\n",
        "    test = X_scale[1200:]\n",
        "\n",
        "    return train , test "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGP4v_NZVg9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = split_combined()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yxaERg5qLSz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "4e33a070-c95e-4b53-ad34-872d7a20005b"
      },
      "source": [
        "train"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01694915, 0.03896104, 0.        , ..., 0.25      , 0.02611293,\n",
              "        0.09353988],\n",
              "       [0.02157165, 0.05194805, 0.00512821, ..., 0.25      , 0.04613149,\n",
              "        0.09353988],\n",
              "       [0.02157165, 0.05194805, 0.01025641, ..., 0.25      , 0.04964281,\n",
              "        0.09353988],\n",
              "       ...,\n",
              "       [0.06471495, 0.07792208, 0.06666667, ..., 1.        , 0.06179118,\n",
              "        0.06967473],\n",
              "       [0.06625578, 0.11688312, 0.07179487, ..., 1.        , 0.07918634,\n",
              "        0.06967473],\n",
              "       [0.06933744, 0.11688312, 0.07692308, ..., 1.        , 0.08067966,\n",
              "        0.06967473]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NavfRvwhVg7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvUBESCXVg4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ca7b1dVKez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTL7RdCYWD1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayH00fNAWD5M",
        "colab_type": "code",
        "outputId": "4e9c53ed-e031-4530-8164-e115bdf55f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
        "NN_model.summary()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_16 (Dense)             (None, 128)               115072    \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 279,937\n",
            "Trainable params: 279,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YTeMeDAWDyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PThPvfm6Wfnf",
        "colab_type": "code",
        "outputId": "6365050e-2a6c-4712-cf07-72170a015376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "NN_model.fit(train, label_train.target, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 960 samples, validate on 240 samples\n",
            "Epoch 1/500\n",
            "960/960 [==============================] - 1s 688us/step - loss: 1.3450 - acc: 0.2458 - val_loss: 1.2719 - val_acc: 0.1042\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.27190, saving model to Weights-001--1.27190.hdf5\n",
            "Epoch 2/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 1.0045 - acc: 0.3031 - val_loss: 1.0806 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.27190 to 1.08058, saving model to Weights-002--1.08058.hdf5\n",
            "Epoch 3/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.9132 - acc: 0.3406 - val_loss: 1.1153 - val_acc: 0.3042\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.08058\n",
            "Epoch 4/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.8539 - acc: 0.4083 - val_loss: 0.9350 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.08058 to 0.93502, saving model to Weights-004--0.93502.hdf5\n",
            "Epoch 5/500\n",
            "960/960 [==============================] - 0s 208us/step - loss: 0.7757 - acc: 0.4698 - val_loss: 0.9690 - val_acc: 0.2833\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.93502\n",
            "Epoch 6/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.7517 - acc: 0.4917 - val_loss: 1.0362 - val_acc: 0.2875\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.93502\n",
            "Epoch 7/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.6968 - acc: 0.5406 - val_loss: 1.1511 - val_acc: 0.2917\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.93502\n",
            "Epoch 8/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.6942 - acc: 0.5406 - val_loss: 0.9828 - val_acc: 0.4250\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.93502\n",
            "Epoch 9/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.6772 - acc: 0.5396 - val_loss: 0.9978 - val_acc: 0.4292\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.93502\n",
            "Epoch 10/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.6601 - acc: 0.5729 - val_loss: 1.1969 - val_acc: 0.2792\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.93502\n",
            "Epoch 11/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.6089 - acc: 0.6219 - val_loss: 1.1281 - val_acc: 0.3250\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.93502\n",
            "Epoch 12/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.6108 - acc: 0.6219 - val_loss: 1.0241 - val_acc: 0.2667\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.93502\n",
            "Epoch 13/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.5923 - acc: 0.6135 - val_loss: 1.0404 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.93502\n",
            "Epoch 14/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.5716 - acc: 0.6479 - val_loss: 1.0742 - val_acc: 0.3000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.93502\n",
            "Epoch 15/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.5676 - acc: 0.6323 - val_loss: 0.9890 - val_acc: 0.4208\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.93502\n",
            "Epoch 16/500\n",
            "960/960 [==============================] - 0s 222us/step - loss: 0.5643 - acc: 0.6531 - val_loss: 0.9701 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.93502\n",
            "Epoch 17/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.5250 - acc: 0.6771 - val_loss: 1.0083 - val_acc: 0.3167\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.93502\n",
            "Epoch 18/500\n",
            "960/960 [==============================] - 0s 239us/step - loss: 0.5288 - acc: 0.6677 - val_loss: 1.1325 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.93502\n",
            "Epoch 19/500\n",
            "960/960 [==============================] - 0s 237us/step - loss: 0.5588 - acc: 0.6396 - val_loss: 1.0546 - val_acc: 0.4083\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.93502\n",
            "Epoch 20/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.5241 - acc: 0.6729 - val_loss: 1.1343 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.93502\n",
            "Epoch 21/500\n",
            "960/960 [==============================] - 0s 228us/step - loss: 0.5248 - acc: 0.6802 - val_loss: 1.0489 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.93502\n",
            "Epoch 22/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.5140 - acc: 0.6875 - val_loss: 1.1047 - val_acc: 0.3167\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.93502\n",
            "Epoch 23/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.5303 - acc: 0.6594 - val_loss: 1.0077 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.93502\n",
            "Epoch 24/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.4823 - acc: 0.6990 - val_loss: 1.0030 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.93502\n",
            "Epoch 25/500\n",
            "960/960 [==============================] - 0s 210us/step - loss: 0.4616 - acc: 0.7240 - val_loss: 0.9472 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.93502\n",
            "Epoch 26/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.4632 - acc: 0.7188 - val_loss: 1.1217 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.93502\n",
            "Epoch 27/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.4854 - acc: 0.6958 - val_loss: 0.9433 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.93502\n",
            "Epoch 28/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.4616 - acc: 0.7250 - val_loss: 0.9889 - val_acc: 0.4292\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.93502\n",
            "Epoch 29/500\n",
            "960/960 [==============================] - 0s 230us/step - loss: 0.4442 - acc: 0.7417 - val_loss: 1.1258 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.93502\n",
            "Epoch 30/500\n",
            "960/960 [==============================] - 0s 206us/step - loss: 0.4619 - acc: 0.7125 - val_loss: 0.9813 - val_acc: 0.4292\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.93502\n",
            "Epoch 31/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.4408 - acc: 0.7302 - val_loss: 1.0696 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.93502\n",
            "Epoch 32/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.4258 - acc: 0.7490 - val_loss: 1.0288 - val_acc: 0.4125\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.93502\n",
            "Epoch 33/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.4277 - acc: 0.7375 - val_loss: 1.1064 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.93502\n",
            "Epoch 34/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.4407 - acc: 0.7333 - val_loss: 0.9791 - val_acc: 0.3292\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.93502\n",
            "Epoch 35/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.4155 - acc: 0.7510 - val_loss: 0.9119 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.93502 to 0.91191, saving model to Weights-035--0.91191.hdf5\n",
            "Epoch 36/500\n",
            "960/960 [==============================] - 0s 233us/step - loss: 0.4359 - acc: 0.7365 - val_loss: 0.9992 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.91191\n",
            "Epoch 37/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.4058 - acc: 0.7521 - val_loss: 1.0596 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.91191\n",
            "Epoch 38/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.3990 - acc: 0.7437 - val_loss: 1.0604 - val_acc: 0.4250\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.91191\n",
            "Epoch 39/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.4065 - acc: 0.7531 - val_loss: 0.9411 - val_acc: 0.4083\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.91191\n",
            "Epoch 40/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.4113 - acc: 0.7500 - val_loss: 1.0641 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.91191\n",
            "Epoch 41/500\n",
            "960/960 [==============================] - 0s 231us/step - loss: 0.3902 - acc: 0.7552 - val_loss: 1.0987 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.91191\n",
            "Epoch 42/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.3715 - acc: 0.7656 - val_loss: 1.0722 - val_acc: 0.4333\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.91191\n",
            "Epoch 43/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.3819 - acc: 0.7594 - val_loss: 1.0521 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.91191\n",
            "Epoch 44/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.3720 - acc: 0.7719 - val_loss: 1.0625 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.91191\n",
            "Epoch 45/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.3714 - acc: 0.7771 - val_loss: 1.0861 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.91191\n",
            "Epoch 46/500\n",
            "960/960 [==============================] - 0s 245us/step - loss: 0.3690 - acc: 0.7823 - val_loss: 1.0414 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.91191\n",
            "Epoch 47/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.3674 - acc: 0.7771 - val_loss: 1.1382 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.91191\n",
            "Epoch 48/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.3661 - acc: 0.7865 - val_loss: 1.0917 - val_acc: 0.4125\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.91191\n",
            "Epoch 49/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.3612 - acc: 0.7844 - val_loss: 1.1250 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.91191\n",
            "Epoch 50/500\n",
            "960/960 [==============================] - 0s 230us/step - loss: 0.3563 - acc: 0.7781 - val_loss: 0.9936 - val_acc: 0.4125\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.91191\n",
            "Epoch 51/500\n",
            "960/960 [==============================] - 0s 237us/step - loss: 0.3550 - acc: 0.7885 - val_loss: 1.1440 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.91191\n",
            "Epoch 52/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.3926 - acc: 0.7656 - val_loss: 0.9761 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.91191\n",
            "Epoch 53/500\n",
            "960/960 [==============================] - 0s 203us/step - loss: 0.3907 - acc: 0.7833 - val_loss: 1.0143 - val_acc: 0.4208\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.91191\n",
            "Epoch 54/500\n",
            "960/960 [==============================] - 0s 236us/step - loss: 0.3752 - acc: 0.7760 - val_loss: 1.0193 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.91191\n",
            "Epoch 55/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.3442 - acc: 0.7990 - val_loss: 1.0912 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.91191\n",
            "Epoch 56/500\n",
            "960/960 [==============================] - 0s 234us/step - loss: 0.3584 - acc: 0.7802 - val_loss: 0.9929 - val_acc: 0.4083\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.91191\n",
            "Epoch 57/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.3442 - acc: 0.7844 - val_loss: 1.0021 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.91191\n",
            "Epoch 58/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.3406 - acc: 0.7885 - val_loss: 1.1255 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.91191\n",
            "Epoch 59/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.3270 - acc: 0.8000 - val_loss: 1.0667 - val_acc: 0.3375\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.91191\n",
            "Epoch 60/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.3361 - acc: 0.7990 - val_loss: 1.1389 - val_acc: 0.3375\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.91191\n",
            "Epoch 61/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.3315 - acc: 0.8125 - val_loss: 1.0823 - val_acc: 0.4292\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.91191\n",
            "Epoch 62/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.3558 - acc: 0.7969 - val_loss: 1.0208 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.91191\n",
            "Epoch 63/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.3338 - acc: 0.8083 - val_loss: 1.0232 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.91191\n",
            "Epoch 64/500\n",
            "960/960 [==============================] - 0s 222us/step - loss: 0.3281 - acc: 0.8042 - val_loss: 1.1069 - val_acc: 0.4083\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.91191\n",
            "Epoch 65/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.3137 - acc: 0.8073 - val_loss: 1.0004 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.91191\n",
            "Epoch 66/500\n",
            "960/960 [==============================] - 0s 228us/step - loss: 0.3207 - acc: 0.8094 - val_loss: 0.9948 - val_acc: 0.4333\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.91191\n",
            "Epoch 67/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.3264 - acc: 0.8062 - val_loss: 1.0521 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.91191\n",
            "Epoch 68/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.3074 - acc: 0.8146 - val_loss: 1.0120 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.91191\n",
            "Epoch 69/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.3033 - acc: 0.8104 - val_loss: 1.0980 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.91191\n",
            "Epoch 70/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.3171 - acc: 0.8146 - val_loss: 1.1494 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.91191\n",
            "Epoch 71/500\n",
            "960/960 [==============================] - 0s 222us/step - loss: 0.2984 - acc: 0.8219 - val_loss: 1.1002 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.91191\n",
            "Epoch 72/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.2922 - acc: 0.8260 - val_loss: 1.1618 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.91191\n",
            "Epoch 73/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.2924 - acc: 0.8323 - val_loss: 1.1012 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.91191\n",
            "Epoch 74/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.2884 - acc: 0.8323 - val_loss: 1.0741 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.91191\n",
            "Epoch 75/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.2913 - acc: 0.8406 - val_loss: 1.0580 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.91191\n",
            "Epoch 76/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.2921 - acc: 0.8333 - val_loss: 1.1690 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.91191\n",
            "Epoch 77/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.3004 - acc: 0.8240 - val_loss: 1.0594 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.91191\n",
            "Epoch 78/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.2969 - acc: 0.8365 - val_loss: 1.1239 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.91191\n",
            "Epoch 79/500\n",
            "960/960 [==============================] - 0s 207us/step - loss: 0.3007 - acc: 0.8104 - val_loss: 1.0265 - val_acc: 0.4083\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.91191\n",
            "Epoch 80/500\n",
            "960/960 [==============================] - 0s 211us/step - loss: 0.2986 - acc: 0.8167 - val_loss: 1.0752 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.91191\n",
            "Epoch 81/500\n",
            "960/960 [==============================] - 0s 208us/step - loss: 0.2946 - acc: 0.8323 - val_loss: 1.0389 - val_acc: 0.4167\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.91191\n",
            "Epoch 82/500\n",
            "960/960 [==============================] - 0s 209us/step - loss: 0.3032 - acc: 0.8115 - val_loss: 1.0067 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.91191\n",
            "Epoch 83/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.3003 - acc: 0.8198 - val_loss: 1.0118 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.91191\n",
            "Epoch 84/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.2937 - acc: 0.8281 - val_loss: 1.0378 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.91191\n",
            "Epoch 85/500\n",
            "960/960 [==============================] - 0s 208us/step - loss: 0.2889 - acc: 0.8344 - val_loss: 1.0515 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.91191\n",
            "Epoch 86/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.2854 - acc: 0.8302 - val_loss: 1.0593 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.91191\n",
            "Epoch 87/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.2941 - acc: 0.8302 - val_loss: 0.9879 - val_acc: 0.4167\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.91191\n",
            "Epoch 88/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.2988 - acc: 0.8281 - val_loss: 1.1784 - val_acc: 0.3375\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.91191\n",
            "Epoch 89/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.2783 - acc: 0.8354 - val_loss: 1.0634 - val_acc: 0.4250\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.91191\n",
            "Epoch 90/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.2796 - acc: 0.8365 - val_loss: 1.0856 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.91191\n",
            "Epoch 91/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.2810 - acc: 0.8417 - val_loss: 1.0730 - val_acc: 0.4167\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.91191\n",
            "Epoch 92/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.2748 - acc: 0.8490 - val_loss: 1.0652 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.91191\n",
            "Epoch 93/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.2712 - acc: 0.8396 - val_loss: 1.0837 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.91191\n",
            "Epoch 94/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.2432 - acc: 0.8646 - val_loss: 1.0722 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.91191\n",
            "Epoch 95/500\n",
            "960/960 [==============================] - 0s 233us/step - loss: 0.2484 - acc: 0.8500 - val_loss: 1.1626 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.91191\n",
            "Epoch 96/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.2553 - acc: 0.8417 - val_loss: 1.1569 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.91191\n",
            "Epoch 97/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.2444 - acc: 0.8625 - val_loss: 1.0680 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.91191\n",
            "Epoch 98/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.2647 - acc: 0.8458 - val_loss: 1.0364 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.91191\n",
            "Epoch 99/500\n",
            "960/960 [==============================] - 0s 230us/step - loss: 0.2617 - acc: 0.8573 - val_loss: 1.0174 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.91191\n",
            "Epoch 100/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.2792 - acc: 0.8438 - val_loss: 1.0422 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.91191\n",
            "Epoch 101/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.2503 - acc: 0.8448 - val_loss: 1.1721 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.91191\n",
            "Epoch 102/500\n",
            "960/960 [==============================] - 0s 228us/step - loss: 0.2649 - acc: 0.8417 - val_loss: 1.1259 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.91191\n",
            "Epoch 103/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.2502 - acc: 0.8635 - val_loss: 1.0228 - val_acc: 0.4208\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.91191\n",
            "Epoch 104/500\n",
            "960/960 [==============================] - 0s 231us/step - loss: 0.2480 - acc: 0.8562 - val_loss: 1.0762 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.91191\n",
            "Epoch 105/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.2461 - acc: 0.8542 - val_loss: 1.1219 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.91191\n",
            "Epoch 106/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.2474 - acc: 0.8625 - val_loss: 1.1360 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.91191\n",
            "Epoch 107/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.2505 - acc: 0.8542 - val_loss: 1.0594 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.91191\n",
            "Epoch 108/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.2417 - acc: 0.8698 - val_loss: 1.0757 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.91191\n",
            "Epoch 109/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.2534 - acc: 0.8573 - val_loss: 1.1808 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.91191\n",
            "Epoch 110/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.2519 - acc: 0.8573 - val_loss: 1.0569 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.91191\n",
            "Epoch 111/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.2349 - acc: 0.8687 - val_loss: 1.0303 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.91191\n",
            "Epoch 112/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.2351 - acc: 0.8656 - val_loss: 1.0266 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.91191\n",
            "Epoch 113/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.2353 - acc: 0.8740 - val_loss: 1.0438 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.91191\n",
            "Epoch 114/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.2359 - acc: 0.8719 - val_loss: 1.0675 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.91191\n",
            "Epoch 115/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.2372 - acc: 0.8646 - val_loss: 1.0630 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.91191\n",
            "Epoch 116/500\n",
            "960/960 [==============================] - 0s 249us/step - loss: 0.2221 - acc: 0.8760 - val_loss: 1.0064 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.91191\n",
            "Epoch 117/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.2303 - acc: 0.8687 - val_loss: 1.0107 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.91191\n",
            "Epoch 118/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.2363 - acc: 0.8573 - val_loss: 1.1330 - val_acc: 0.3417\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.91191\n",
            "Epoch 119/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.2472 - acc: 0.8615 - val_loss: 1.1423 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.91191\n",
            "Epoch 120/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.2365 - acc: 0.8531 - val_loss: 0.9997 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.91191\n",
            "Epoch 121/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.2364 - acc: 0.8687 - val_loss: 1.0330 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.91191\n",
            "Epoch 122/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.2390 - acc: 0.8594 - val_loss: 1.1207 - val_acc: 0.3292\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.91191\n",
            "Epoch 123/500\n",
            "960/960 [==============================] - 0s 210us/step - loss: 0.2421 - acc: 0.8542 - val_loss: 1.0535 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.91191\n",
            "Epoch 124/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.2359 - acc: 0.8583 - val_loss: 1.0652 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.91191\n",
            "Epoch 125/500\n",
            "960/960 [==============================] - 0s 232us/step - loss: 0.2362 - acc: 0.8521 - val_loss: 1.0010 - val_acc: 0.4208\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.91191\n",
            "Epoch 126/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.2286 - acc: 0.8687 - val_loss: 1.1215 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.91191\n",
            "Epoch 127/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.2468 - acc: 0.8667 - val_loss: 1.1432 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.91191\n",
            "Epoch 128/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.2259 - acc: 0.8823 - val_loss: 1.0569 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.91191\n",
            "Epoch 129/500\n",
            "960/960 [==============================] - 0s 237us/step - loss: 0.2261 - acc: 0.8708 - val_loss: 1.0736 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.91191\n",
            "Epoch 130/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.2173 - acc: 0.8740 - val_loss: 1.1121 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.91191\n",
            "Epoch 131/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.2306 - acc: 0.8656 - val_loss: 1.1546 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.91191\n",
            "Epoch 132/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.2241 - acc: 0.8760 - val_loss: 1.1977 - val_acc: 0.3375\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.91191\n",
            "Epoch 133/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.2357 - acc: 0.8698 - val_loss: 1.1262 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.91191\n",
            "Epoch 134/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.2176 - acc: 0.8740 - val_loss: 1.0739 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.91191\n",
            "Epoch 135/500\n",
            "960/960 [==============================] - 0s 211us/step - loss: 0.2179 - acc: 0.8750 - val_loss: 1.1707 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.91191\n",
            "Epoch 136/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.2220 - acc: 0.8750 - val_loss: 1.1227 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.91191\n",
            "Epoch 137/500\n",
            "960/960 [==============================] - 0s 205us/step - loss: 0.2213 - acc: 0.8813 - val_loss: 1.0390 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.91191\n",
            "Epoch 138/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.2295 - acc: 0.8677 - val_loss: 1.0127 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.91191\n",
            "Epoch 139/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.2405 - acc: 0.8510 - val_loss: 1.0443 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.91191\n",
            "Epoch 140/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.2270 - acc: 0.8750 - val_loss: 1.0988 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.91191\n",
            "Epoch 141/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.2152 - acc: 0.8802 - val_loss: 1.0895 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.91191\n",
            "Epoch 142/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.2139 - acc: 0.8760 - val_loss: 1.1396 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.91191\n",
            "Epoch 143/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.2012 - acc: 0.8917 - val_loss: 1.0265 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.91191\n",
            "Epoch 144/500\n",
            "960/960 [==============================] - 0s 228us/step - loss: 0.2296 - acc: 0.8823 - val_loss: 1.0498 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.91191\n",
            "Epoch 145/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.2280 - acc: 0.8719 - val_loss: 1.0052 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.91191\n",
            "Epoch 146/500\n",
            "960/960 [==============================] - 0s 222us/step - loss: 0.2230 - acc: 0.8760 - val_loss: 1.0215 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.91191\n",
            "Epoch 147/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.2126 - acc: 0.8844 - val_loss: 1.1371 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.91191\n",
            "Epoch 148/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.2232 - acc: 0.8698 - val_loss: 1.1332 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.91191\n",
            "Epoch 149/500\n",
            "960/960 [==============================] - 0s 211us/step - loss: 0.2315 - acc: 0.8729 - val_loss: 1.0320 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.91191\n",
            "Epoch 150/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.2278 - acc: 0.8729 - val_loss: 1.0262 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.91191\n",
            "Epoch 151/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.2219 - acc: 0.8781 - val_loss: 1.1228 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.91191\n",
            "Epoch 152/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.2208 - acc: 0.8844 - val_loss: 1.0387 - val_acc: 0.3417\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.91191\n",
            "Epoch 153/500\n",
            "960/960 [==============================] - 0s 211us/step - loss: 0.2121 - acc: 0.8865 - val_loss: 1.0778 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.91191\n",
            "Epoch 154/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.2077 - acc: 0.8927 - val_loss: 1.0480 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.91191\n",
            "Epoch 155/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.2147 - acc: 0.8802 - val_loss: 1.0809 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.91191\n",
            "Epoch 156/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1996 - acc: 0.8823 - val_loss: 1.1219 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.91191\n",
            "Epoch 157/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.2067 - acc: 0.8823 - val_loss: 1.1904 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.91191\n",
            "Epoch 158/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1969 - acc: 0.8969 - val_loss: 1.0290 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.91191\n",
            "Epoch 159/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.2158 - acc: 0.8719 - val_loss: 1.0349 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.91191\n",
            "Epoch 160/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.2014 - acc: 0.8833 - val_loss: 1.0508 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.91191\n",
            "Epoch 161/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.2187 - acc: 0.8854 - val_loss: 1.1306 - val_acc: 0.4125\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.91191\n",
            "Epoch 162/500\n",
            "960/960 [==============================] - 0s 239us/step - loss: 0.2364 - acc: 0.8698 - val_loss: 1.0206 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.91191\n",
            "Epoch 163/500\n",
            "960/960 [==============================] - 0s 234us/step - loss: 0.2054 - acc: 0.8802 - val_loss: 1.0150 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.91191\n",
            "Epoch 164/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.2043 - acc: 0.8771 - val_loss: 1.0515 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.91191\n",
            "Epoch 165/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.2062 - acc: 0.8896 - val_loss: 1.1282 - val_acc: 0.3375\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.91191\n",
            "Epoch 166/500\n",
            "960/960 [==============================] - 0s 248us/step - loss: 0.2042 - acc: 0.8906 - val_loss: 1.0861 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.91191\n",
            "Epoch 167/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.1908 - acc: 0.8906 - val_loss: 1.0614 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.91191\n",
            "Epoch 168/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1836 - acc: 0.8823 - val_loss: 1.0478 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.91191\n",
            "Epoch 169/500\n",
            "960/960 [==============================] - 0s 207us/step - loss: 0.1959 - acc: 0.8906 - val_loss: 1.0709 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.91191\n",
            "Epoch 170/500\n",
            "960/960 [==============================] - 0s 232us/step - loss: 0.1928 - acc: 0.8844 - val_loss: 1.0471 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.91191\n",
            "Epoch 171/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1858 - acc: 0.8948 - val_loss: 1.1397 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.91191\n",
            "Epoch 172/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1956 - acc: 0.8917 - val_loss: 1.1376 - val_acc: 0.3375\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.91191\n",
            "Epoch 173/500\n",
            "960/960 [==============================] - 0s 208us/step - loss: 0.2062 - acc: 0.8969 - val_loss: 1.0914 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.91191\n",
            "Epoch 174/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.2075 - acc: 0.8844 - val_loss: 1.2741 - val_acc: 0.3375\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.91191\n",
            "Epoch 175/500\n",
            "960/960 [==============================] - 0s 230us/step - loss: 0.2130 - acc: 0.8802 - val_loss: 1.0498 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.91191\n",
            "Epoch 176/500\n",
            "960/960 [==============================] - 0s 222us/step - loss: 0.1936 - acc: 0.8865 - val_loss: 1.0760 - val_acc: 0.3417\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.91191\n",
            "Epoch 177/500\n",
            "960/960 [==============================] - 0s 234us/step - loss: 0.1944 - acc: 0.8969 - val_loss: 1.0049 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.91191\n",
            "Epoch 178/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.2139 - acc: 0.8750 - val_loss: 1.0159 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.91191\n",
            "Epoch 179/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.2014 - acc: 0.8938 - val_loss: 1.0000 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.91191\n",
            "Epoch 180/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.2210 - acc: 0.8833 - val_loss: 1.0254 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.91191\n",
            "Epoch 181/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.2045 - acc: 0.8927 - val_loss: 1.0264 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.91191\n",
            "Epoch 182/500\n",
            "960/960 [==============================] - 0s 229us/step - loss: 0.1934 - acc: 0.8896 - val_loss: 1.0536 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.91191\n",
            "Epoch 183/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.2001 - acc: 0.8844 - val_loss: 1.0680 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.91191\n",
            "Epoch 184/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1826 - acc: 0.9042 - val_loss: 1.0759 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.91191\n",
            "Epoch 185/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1904 - acc: 0.8865 - val_loss: 1.0538 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.91191\n",
            "Epoch 186/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1814 - acc: 0.8927 - val_loss: 0.9982 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.91191\n",
            "Epoch 187/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1786 - acc: 0.8958 - val_loss: 1.0324 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.91191\n",
            "Epoch 188/500\n",
            "960/960 [==============================] - 0s 208us/step - loss: 0.1864 - acc: 0.9000 - val_loss: 1.0245 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.91191\n",
            "Epoch 189/500\n",
            "960/960 [==============================] - 0s 230us/step - loss: 0.1933 - acc: 0.8938 - val_loss: 1.0476 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.91191\n",
            "Epoch 190/500\n",
            "960/960 [==============================] - 0s 235us/step - loss: 0.1847 - acc: 0.8958 - val_loss: 1.0409 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.91191\n",
            "Epoch 191/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.1806 - acc: 0.9000 - val_loss: 1.0606 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.91191\n",
            "Epoch 192/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.2043 - acc: 0.8813 - val_loss: 1.0843 - val_acc: 0.3375\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.91191\n",
            "Epoch 193/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.2123 - acc: 0.8792 - val_loss: 1.0357 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.91191\n",
            "Epoch 194/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.2091 - acc: 0.8646 - val_loss: 1.0301 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.91191\n",
            "Epoch 195/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1895 - acc: 0.8823 - val_loss: 1.0934 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.91191\n",
            "Epoch 196/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1914 - acc: 0.8938 - val_loss: 0.9911 - val_acc: 0.4083\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.91191\n",
            "Epoch 197/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.2059 - acc: 0.8802 - val_loss: 1.0537 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.91191\n",
            "Epoch 198/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.2093 - acc: 0.8813 - val_loss: 1.0695 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.91191\n",
            "Epoch 199/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.2125 - acc: 0.8771 - val_loss: 1.1493 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.91191\n",
            "Epoch 200/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.2077 - acc: 0.8865 - val_loss: 1.0214 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.91191\n",
            "Epoch 201/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.2259 - acc: 0.8594 - val_loss: 1.0251 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.91191\n",
            "Epoch 202/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.2067 - acc: 0.8667 - val_loss: 1.0478 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.91191\n",
            "Epoch 203/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1898 - acc: 0.8833 - val_loss: 1.0687 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.91191\n",
            "Epoch 204/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1978 - acc: 0.8760 - val_loss: 1.0852 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.91191\n",
            "Epoch 205/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1910 - acc: 0.8813 - val_loss: 1.0760 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.91191\n",
            "Epoch 206/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1819 - acc: 0.8875 - val_loss: 1.0926 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.91191\n",
            "Epoch 207/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.2161 - acc: 0.8823 - val_loss: 1.0099 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.91191\n",
            "Epoch 208/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1970 - acc: 0.8844 - val_loss: 1.0275 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.91191\n",
            "Epoch 209/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1825 - acc: 0.8802 - val_loss: 1.0518 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.91191\n",
            "Epoch 210/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1818 - acc: 0.9000 - val_loss: 1.0745 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.91191\n",
            "Epoch 211/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1862 - acc: 0.8823 - val_loss: 1.0968 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.91191\n",
            "Epoch 212/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1910 - acc: 0.8948 - val_loss: 1.1179 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.91191\n",
            "Epoch 213/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1759 - acc: 0.9062 - val_loss: 1.1247 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.91191\n",
            "Epoch 214/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.1983 - acc: 0.8938 - val_loss: 1.2252 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.91191\n",
            "Epoch 215/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.2006 - acc: 0.8833 - val_loss: 1.0215 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.91191\n",
            "Epoch 216/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1867 - acc: 0.8990 - val_loss: 1.0623 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.91191\n",
            "Epoch 217/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1894 - acc: 0.8958 - val_loss: 1.0217 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.91191\n",
            "Epoch 218/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1841 - acc: 0.9073 - val_loss: 1.0556 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.91191\n",
            "Epoch 219/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1861 - acc: 0.8979 - val_loss: 1.0875 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.91191\n",
            "Epoch 220/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1926 - acc: 0.8854 - val_loss: 1.0880 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.91191\n",
            "Epoch 221/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.2371 - acc: 0.8708 - val_loss: 1.0949 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.91191\n",
            "Epoch 222/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.2139 - acc: 0.8813 - val_loss: 1.1470 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.91191\n",
            "Epoch 223/500\n",
            "960/960 [==============================] - 0s 235us/step - loss: 0.2020 - acc: 0.8896 - val_loss: 1.1909 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.91191\n",
            "Epoch 224/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.2060 - acc: 0.8823 - val_loss: 1.1198 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.91191\n",
            "Epoch 225/500\n",
            "960/960 [==============================] - 0s 229us/step - loss: 0.2279 - acc: 0.8750 - val_loss: 1.1399 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.91191\n",
            "Epoch 226/500\n",
            "960/960 [==============================] - 0s 239us/step - loss: 0.2041 - acc: 0.8844 - val_loss: 1.0740 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.91191\n",
            "Epoch 227/500\n",
            "960/960 [==============================] - 0s 228us/step - loss: 0.1907 - acc: 0.8906 - val_loss: 1.0589 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.91191\n",
            "Epoch 228/500\n",
            "960/960 [==============================] - 0s 229us/step - loss: 0.1835 - acc: 0.8875 - val_loss: 1.0209 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.91191\n",
            "Epoch 229/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1633 - acc: 0.9083 - val_loss: 1.1452 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.91191\n",
            "Epoch 230/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1780 - acc: 0.8958 - val_loss: 1.0523 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.91191\n",
            "Epoch 231/500\n",
            "960/960 [==============================] - 0s 228us/step - loss: 0.1855 - acc: 0.8938 - val_loss: 1.0874 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.91191\n",
            "Epoch 232/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1721 - acc: 0.8948 - val_loss: 1.0186 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.91191\n",
            "Epoch 233/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1601 - acc: 0.9042 - val_loss: 1.0329 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.91191\n",
            "Epoch 234/500\n",
            "960/960 [==============================] - 0s 222us/step - loss: 0.1660 - acc: 0.9010 - val_loss: 1.0603 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.91191\n",
            "Epoch 235/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1870 - acc: 0.8875 - val_loss: 1.0838 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.91191\n",
            "Epoch 236/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1909 - acc: 0.8938 - val_loss: 1.0868 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.91191\n",
            "Epoch 237/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1717 - acc: 0.9125 - val_loss: 1.1100 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.91191\n",
            "Epoch 238/500\n",
            "960/960 [==============================] - 0s 229us/step - loss: 0.1947 - acc: 0.8906 - val_loss: 0.9864 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.91191\n",
            "Epoch 239/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1857 - acc: 0.8865 - val_loss: 1.0030 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.91191\n",
            "Epoch 240/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1813 - acc: 0.9052 - val_loss: 1.0917 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.91191\n",
            "Epoch 241/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1974 - acc: 0.8896 - val_loss: 1.1401 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.91191\n",
            "Epoch 242/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.1845 - acc: 0.9000 - val_loss: 1.0482 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.91191\n",
            "Epoch 243/500\n",
            "960/960 [==============================] - 0s 232us/step - loss: 0.1713 - acc: 0.9052 - val_loss: 1.0534 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.91191\n",
            "Epoch 244/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.1790 - acc: 0.9000 - val_loss: 1.0647 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.91191\n",
            "Epoch 245/500\n",
            "960/960 [==============================] - 0s 230us/step - loss: 0.1860 - acc: 0.8969 - val_loss: 0.9890 - val_acc: 0.4083\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.91191\n",
            "Epoch 246/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1939 - acc: 0.8865 - val_loss: 1.1973 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.91191\n",
            "Epoch 247/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.2426 - acc: 0.8531 - val_loss: 1.0852 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.91191\n",
            "Epoch 248/500\n",
            "960/960 [==============================] - 0s 203us/step - loss: 0.1989 - acc: 0.8865 - val_loss: 1.0107 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.91191\n",
            "Epoch 249/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1844 - acc: 0.8896 - val_loss: 1.0209 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.91191\n",
            "Epoch 250/500\n",
            "960/960 [==============================] - 0s 209us/step - loss: 0.1769 - acc: 0.8958 - val_loss: 1.0044 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.91191\n",
            "Epoch 251/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.1802 - acc: 0.9021 - val_loss: 1.0279 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.91191\n",
            "Epoch 252/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.1906 - acc: 0.8906 - val_loss: 1.0858 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.91191\n",
            "Epoch 253/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.1933 - acc: 0.8906 - val_loss: 1.0913 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.91191\n",
            "Epoch 254/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1809 - acc: 0.8990 - val_loss: 1.1200 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.91191\n",
            "Epoch 255/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1732 - acc: 0.9031 - val_loss: 1.1647 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.91191\n",
            "Epoch 256/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1829 - acc: 0.8917 - val_loss: 1.0288 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.91191\n",
            "Epoch 257/500\n",
            "960/960 [==============================] - 0s 222us/step - loss: 0.1740 - acc: 0.9000 - val_loss: 1.0063 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.91191\n",
            "Epoch 258/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1575 - acc: 0.9083 - val_loss: 1.0573 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.91191\n",
            "Epoch 259/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.1578 - acc: 0.8979 - val_loss: 1.1321 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.91191\n",
            "Epoch 260/500\n",
            "960/960 [==============================] - 0s 233us/step - loss: 0.1731 - acc: 0.9042 - val_loss: 1.1464 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.91191\n",
            "Epoch 261/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1854 - acc: 0.8896 - val_loss: 1.0538 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.91191\n",
            "Epoch 262/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.1747 - acc: 0.8969 - val_loss: 1.1149 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.91191\n",
            "Epoch 263/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1593 - acc: 0.9094 - val_loss: 1.1062 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.91191\n",
            "Epoch 264/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1960 - acc: 0.8906 - val_loss: 1.0510 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.91191\n",
            "Epoch 265/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.2000 - acc: 0.9010 - val_loss: 1.0664 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.91191\n",
            "Epoch 266/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1712 - acc: 0.9000 - val_loss: 1.0710 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.91191\n",
            "Epoch 267/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.1730 - acc: 0.9021 - val_loss: 1.0296 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.91191\n",
            "Epoch 268/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1656 - acc: 0.9010 - val_loss: 1.0455 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.91191\n",
            "Epoch 269/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.1588 - acc: 0.9042 - val_loss: 1.0800 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.91191\n",
            "Epoch 270/500\n",
            "960/960 [==============================] - 0s 228us/step - loss: 0.1652 - acc: 0.8948 - val_loss: 1.0368 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.91191\n",
            "Epoch 271/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1616 - acc: 0.9104 - val_loss: 1.0660 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.91191\n",
            "Epoch 272/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.1669 - acc: 0.9083 - val_loss: 1.0946 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.91191\n",
            "Epoch 273/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1653 - acc: 0.9104 - val_loss: 1.0526 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.91191\n",
            "Epoch 274/500\n",
            "960/960 [==============================] - 0s 222us/step - loss: 0.1559 - acc: 0.9187 - val_loss: 1.0741 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.91191\n",
            "Epoch 275/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.1656 - acc: 0.9115 - val_loss: 1.0932 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.91191\n",
            "Epoch 276/500\n",
            "960/960 [==============================] - 0s 206us/step - loss: 0.1608 - acc: 0.9042 - val_loss: 1.0812 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.91191\n",
            "Epoch 277/500\n",
            "960/960 [==============================] - 0s 228us/step - loss: 0.1804 - acc: 0.8948 - val_loss: 1.0653 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.91191\n",
            "Epoch 278/500\n",
            "960/960 [==============================] - 0s 232us/step - loss: 0.1678 - acc: 0.9021 - val_loss: 1.0422 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.91191\n",
            "Epoch 279/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1697 - acc: 0.9104 - val_loss: 1.0802 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.91191\n",
            "Epoch 280/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1707 - acc: 0.8927 - val_loss: 1.0345 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.91191\n",
            "Epoch 281/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1765 - acc: 0.9083 - val_loss: 1.0494 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.91191\n",
            "Epoch 282/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1626 - acc: 0.9062 - val_loss: 1.0399 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.91191\n",
            "Epoch 283/500\n",
            "960/960 [==============================] - 0s 211us/step - loss: 0.1645 - acc: 0.9062 - val_loss: 1.0230 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.91191\n",
            "Epoch 284/500\n",
            "960/960 [==============================] - 0s 238us/step - loss: 0.1674 - acc: 0.8990 - val_loss: 1.0656 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.91191\n",
            "Epoch 285/500\n",
            "960/960 [==============================] - 0s 209us/step - loss: 0.1629 - acc: 0.9115 - val_loss: 1.0289 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.91191\n",
            "Epoch 286/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1559 - acc: 0.9094 - val_loss: 1.0808 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.91191\n",
            "Epoch 287/500\n",
            "960/960 [==============================] - 0s 237us/step - loss: 0.1688 - acc: 0.9010 - val_loss: 1.0749 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.91191\n",
            "Epoch 288/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1745 - acc: 0.8979 - val_loss: 1.0505 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.91191\n",
            "Epoch 289/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1785 - acc: 0.8927 - val_loss: 0.9975 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.91191\n",
            "Epoch 290/500\n",
            "960/960 [==============================] - 0s 208us/step - loss: 0.1650 - acc: 0.9031 - val_loss: 1.0604 - val_acc: 0.4083\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.91191\n",
            "Epoch 291/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1706 - acc: 0.8979 - val_loss: 1.0738 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.91191\n",
            "Epoch 292/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1567 - acc: 0.9125 - val_loss: 1.0448 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.91191\n",
            "Epoch 293/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1707 - acc: 0.9042 - val_loss: 1.0695 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.91191\n",
            "Epoch 294/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1676 - acc: 0.9031 - val_loss: 1.0962 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.91191\n",
            "Epoch 295/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1622 - acc: 0.9125 - val_loss: 1.0850 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.91191\n",
            "Epoch 296/500\n",
            "960/960 [==============================] - 0s 237us/step - loss: 0.1553 - acc: 0.9073 - val_loss: 1.0701 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.91191\n",
            "Epoch 297/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1631 - acc: 0.9062 - val_loss: 1.0451 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.91191\n",
            "Epoch 298/500\n",
            "960/960 [==============================] - 0s 209us/step - loss: 0.1533 - acc: 0.9146 - val_loss: 1.0543 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.91191\n",
            "Epoch 299/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.1664 - acc: 0.9000 - val_loss: 0.9872 - val_acc: 0.4125\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.91191\n",
            "Epoch 300/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1729 - acc: 0.8969 - val_loss: 1.0817 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.91191\n",
            "Epoch 301/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1668 - acc: 0.9021 - val_loss: 1.0691 - val_acc: 0.3417\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.91191\n",
            "Epoch 302/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1818 - acc: 0.8948 - val_loss: 1.1095 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.91191\n",
            "Epoch 303/500\n",
            "960/960 [==============================] - 0s 229us/step - loss: 0.1614 - acc: 0.9083 - val_loss: 1.0600 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.91191\n",
            "Epoch 304/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.1922 - acc: 0.8958 - val_loss: 1.0409 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.91191\n",
            "Epoch 305/500\n",
            "960/960 [==============================] - 0s 211us/step - loss: 0.1983 - acc: 0.8865 - val_loss: 1.1260 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.91191\n",
            "Epoch 306/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.1835 - acc: 0.8885 - val_loss: 1.0996 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.91191\n",
            "Epoch 307/500\n",
            "960/960 [==============================] - 0s 229us/step - loss: 0.1692 - acc: 0.9042 - val_loss: 1.0644 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.91191\n",
            "Epoch 308/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1567 - acc: 0.9073 - val_loss: 1.0460 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.91191\n",
            "Epoch 309/500\n",
            "960/960 [==============================] - 0s 210us/step - loss: 0.1505 - acc: 0.9156 - val_loss: 1.0262 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.91191\n",
            "Epoch 310/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1584 - acc: 0.9000 - val_loss: 1.0904 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.91191\n",
            "Epoch 311/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1626 - acc: 0.9042 - val_loss: 1.1005 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.91191\n",
            "Epoch 312/500\n",
            "960/960 [==============================] - 0s 207us/step - loss: 0.1715 - acc: 0.8969 - val_loss: 1.1027 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.91191\n",
            "Epoch 313/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.1902 - acc: 0.8906 - val_loss: 1.0548 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.91191\n",
            "Epoch 314/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1662 - acc: 0.9031 - val_loss: 1.0680 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.91191\n",
            "Epoch 315/500\n",
            "960/960 [==============================] - 0s 229us/step - loss: 0.1717 - acc: 0.8969 - val_loss: 1.0197 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.91191\n",
            "Epoch 316/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.1700 - acc: 0.9000 - val_loss: 1.0734 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.91191\n",
            "Epoch 317/500\n",
            "960/960 [==============================] - 0s 245us/step - loss: 0.1845 - acc: 0.8938 - val_loss: 1.1397 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.91191\n",
            "Epoch 318/500\n",
            "960/960 [==============================] - 0s 210us/step - loss: 0.1766 - acc: 0.8979 - val_loss: 1.0222 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.91191\n",
            "Epoch 319/500\n",
            "960/960 [==============================] - 0s 211us/step - loss: 0.1794 - acc: 0.8958 - val_loss: 1.0924 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.91191\n",
            "Epoch 320/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1693 - acc: 0.9094 - val_loss: 1.0934 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.91191\n",
            "Epoch 321/500\n",
            "960/960 [==============================] - 0s 200us/step - loss: 0.1603 - acc: 0.9073 - val_loss: 1.0405 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.91191\n",
            "Epoch 322/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1721 - acc: 0.9021 - val_loss: 1.0813 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.91191\n",
            "Epoch 323/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1865 - acc: 0.8948 - val_loss: 0.9878 - val_acc: 0.4292\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.91191\n",
            "Epoch 324/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.1812 - acc: 0.8948 - val_loss: 1.1053 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.91191\n",
            "Epoch 325/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1690 - acc: 0.9021 - val_loss: 1.0691 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.91191\n",
            "Epoch 326/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1666 - acc: 0.8990 - val_loss: 1.1909 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.91191\n",
            "Epoch 327/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1710 - acc: 0.9031 - val_loss: 1.1066 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.91191\n",
            "Epoch 328/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1768 - acc: 0.8906 - val_loss: 1.1026 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.91191\n",
            "Epoch 329/500\n",
            "960/960 [==============================] - 0s 240us/step - loss: 0.1617 - acc: 0.9104 - val_loss: 1.0864 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.91191\n",
            "Epoch 330/500\n",
            "960/960 [==============================] - 0s 209us/step - loss: 0.1690 - acc: 0.8990 - val_loss: 1.0548 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.91191\n",
            "Epoch 331/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.1588 - acc: 0.9083 - val_loss: 1.1448 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.91191\n",
            "Epoch 332/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.1773 - acc: 0.8969 - val_loss: 1.1155 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.91191\n",
            "Epoch 333/500\n",
            "960/960 [==============================] - 0s 238us/step - loss: 0.1830 - acc: 0.8917 - val_loss: 1.1498 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.91191\n",
            "Epoch 334/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1593 - acc: 0.9073 - val_loss: 1.1723 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.91191\n",
            "Epoch 335/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1567 - acc: 0.9052 - val_loss: 1.1406 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.91191\n",
            "Epoch 336/500\n",
            "960/960 [==============================] - 0s 222us/step - loss: 0.1478 - acc: 0.9146 - val_loss: 1.1540 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.91191\n",
            "Epoch 337/500\n",
            "960/960 [==============================] - 0s 207us/step - loss: 0.1435 - acc: 0.9167 - val_loss: 1.0727 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.91191\n",
            "Epoch 338/500\n",
            "960/960 [==============================] - 0s 229us/step - loss: 0.1527 - acc: 0.9135 - val_loss: 1.0546 - val_acc: 0.4208\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.91191\n",
            "Epoch 339/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1693 - acc: 0.8979 - val_loss: 1.1403 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.91191\n",
            "Epoch 340/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1452 - acc: 0.9177 - val_loss: 1.0755 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.91191\n",
            "Epoch 341/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.1577 - acc: 0.9062 - val_loss: 1.0919 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.91191\n",
            "Epoch 342/500\n",
            "960/960 [==============================] - 0s 233us/step - loss: 0.1539 - acc: 0.9146 - val_loss: 1.1147 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.91191\n",
            "Epoch 343/500\n",
            "960/960 [==============================] - 0s 234us/step - loss: 0.1534 - acc: 0.9094 - val_loss: 1.0847 - val_acc: 0.4125\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.91191\n",
            "Epoch 344/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1481 - acc: 0.9042 - val_loss: 1.1033 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.91191\n",
            "Epoch 345/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1531 - acc: 0.9073 - val_loss: 1.0638 - val_acc: 0.4083\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.91191\n",
            "Epoch 346/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1644 - acc: 0.9042 - val_loss: 1.1465 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.91191\n",
            "Epoch 347/500\n",
            "960/960 [==============================] - 0s 230us/step - loss: 0.1503 - acc: 0.9177 - val_loss: 1.1443 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.91191\n",
            "Epoch 348/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.1511 - acc: 0.9083 - val_loss: 1.0924 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.91191\n",
            "Epoch 349/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1572 - acc: 0.9094 - val_loss: 1.0774 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.91191\n",
            "Epoch 350/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1646 - acc: 0.9115 - val_loss: 1.1884 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.91191\n",
            "Epoch 351/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1747 - acc: 0.9021 - val_loss: 1.1125 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.91191\n",
            "Epoch 352/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.1635 - acc: 0.8990 - val_loss: 1.1592 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.91191\n",
            "Epoch 353/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1526 - acc: 0.9052 - val_loss: 1.1807 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.91191\n",
            "Epoch 354/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1534 - acc: 0.9073 - val_loss: 1.1917 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.91191\n",
            "Epoch 355/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1572 - acc: 0.9083 - val_loss: 1.0747 - val_acc: 0.4167\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.91191\n",
            "Epoch 356/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1418 - acc: 0.9187 - val_loss: 1.1779 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.91191\n",
            "Epoch 357/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1450 - acc: 0.9115 - val_loss: 1.0308 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.91191\n",
            "Epoch 358/500\n",
            "960/960 [==============================] - 0s 231us/step - loss: 0.1556 - acc: 0.9094 - val_loss: 1.1520 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.91191\n",
            "Epoch 359/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1531 - acc: 0.9094 - val_loss: 1.1229 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.91191\n",
            "Epoch 360/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1519 - acc: 0.9042 - val_loss: 1.1414 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.91191\n",
            "Epoch 361/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1578 - acc: 0.9000 - val_loss: 1.1703 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.91191\n",
            "Epoch 362/500\n",
            "960/960 [==============================] - 0s 228us/step - loss: 0.1427 - acc: 0.9104 - val_loss: 1.1146 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.91191\n",
            "Epoch 363/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1521 - acc: 0.9010 - val_loss: 1.1111 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.91191\n",
            "Epoch 364/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1396 - acc: 0.9177 - val_loss: 1.0872 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.91191\n",
            "Epoch 365/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1551 - acc: 0.9146 - val_loss: 1.0761 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.91191\n",
            "Epoch 366/500\n",
            "960/960 [==============================] - 0s 209us/step - loss: 0.1454 - acc: 0.9240 - val_loss: 1.1082 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.91191\n",
            "Epoch 367/500\n",
            "960/960 [==============================] - 0s 232us/step - loss: 0.1482 - acc: 0.9125 - val_loss: 1.1310 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.91191\n",
            "Epoch 368/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1495 - acc: 0.9146 - val_loss: 1.0785 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.91191\n",
            "Epoch 369/500\n",
            "960/960 [==============================] - 0s 240us/step - loss: 0.1593 - acc: 0.9042 - val_loss: 1.1032 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.91191\n",
            "Epoch 370/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1563 - acc: 0.9094 - val_loss: 1.0708 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.91191\n",
            "Epoch 371/500\n",
            "960/960 [==============================] - 0s 230us/step - loss: 0.1691 - acc: 0.8958 - val_loss: 1.1456 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.91191\n",
            "Epoch 372/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.1725 - acc: 0.8969 - val_loss: 1.1081 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.91191\n",
            "Epoch 373/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1635 - acc: 0.9073 - val_loss: 1.2104 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.91191\n",
            "Epoch 374/500\n",
            "960/960 [==============================] - 0s 199us/step - loss: 0.1683 - acc: 0.9021 - val_loss: 1.1395 - val_acc: 0.3292\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.91191\n",
            "Epoch 375/500\n",
            "960/960 [==============================] - 0s 231us/step - loss: 0.1559 - acc: 0.9073 - val_loss: 1.0052 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.91191\n",
            "Epoch 376/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1542 - acc: 0.9094 - val_loss: 1.0455 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.91191\n",
            "Epoch 377/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1571 - acc: 0.9042 - val_loss: 0.9986 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.91191\n",
            "Epoch 378/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1501 - acc: 0.9135 - val_loss: 1.0886 - val_acc: 0.3417\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.91191\n",
            "Epoch 379/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1640 - acc: 0.8979 - val_loss: 1.0354 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.91191\n",
            "Epoch 380/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1471 - acc: 0.9187 - val_loss: 1.1175 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.91191\n",
            "Epoch 381/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1463 - acc: 0.9125 - val_loss: 1.0703 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.91191\n",
            "Epoch 382/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.1318 - acc: 0.9260 - val_loss: 1.0564 - val_acc: 0.4167\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.91191\n",
            "Epoch 383/500\n",
            "960/960 [==============================] - 0s 244us/step - loss: 0.1621 - acc: 0.9073 - val_loss: 1.0829 - val_acc: 0.3375\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.91191\n",
            "Epoch 384/500\n",
            "960/960 [==============================] - 0s 244us/step - loss: 0.1517 - acc: 0.9104 - val_loss: 1.0771 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.91191\n",
            "Epoch 385/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1583 - acc: 0.9094 - val_loss: 1.1032 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.91191\n",
            "Epoch 386/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1556 - acc: 0.9062 - val_loss: 1.0639 - val_acc: 0.4125\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.91191\n",
            "Epoch 387/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.1562 - acc: 0.9104 - val_loss: 1.0610 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.91191\n",
            "Epoch 388/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1637 - acc: 0.8979 - val_loss: 1.1574 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.91191\n",
            "Epoch 389/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1668 - acc: 0.8969 - val_loss: 1.1147 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.91191\n",
            "Epoch 390/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1541 - acc: 0.9094 - val_loss: 1.1112 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.91191\n",
            "Epoch 391/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1566 - acc: 0.9083 - val_loss: 1.0766 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.91191\n",
            "Epoch 392/500\n",
            "960/960 [==============================] - 0s 243us/step - loss: 0.1437 - acc: 0.9198 - val_loss: 1.0909 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.91191\n",
            "Epoch 393/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1510 - acc: 0.9115 - val_loss: 1.0800 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.91191\n",
            "Epoch 394/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1620 - acc: 0.9073 - val_loss: 1.1228 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.91191\n",
            "Epoch 395/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1611 - acc: 0.9062 - val_loss: 1.0614 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.91191\n",
            "Epoch 396/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1663 - acc: 0.9021 - val_loss: 1.0989 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.91191\n",
            "Epoch 397/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.1590 - acc: 0.9031 - val_loss: 1.1198 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.91191\n",
            "Epoch 398/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1668 - acc: 0.8979 - val_loss: 1.0652 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.91191\n",
            "Epoch 399/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1652 - acc: 0.9062 - val_loss: 1.0624 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.91191\n",
            "Epoch 400/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1711 - acc: 0.9010 - val_loss: 1.1214 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.91191\n",
            "Epoch 401/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1721 - acc: 0.8958 - val_loss: 1.0668 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.91191\n",
            "Epoch 402/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1722 - acc: 0.8938 - val_loss: 1.0815 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.91191\n",
            "Epoch 403/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.1724 - acc: 0.8875 - val_loss: 1.1279 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.91191\n",
            "Epoch 404/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1663 - acc: 0.8844 - val_loss: 1.1126 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.91191\n",
            "Epoch 405/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1595 - acc: 0.9021 - val_loss: 1.0978 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.91191\n",
            "Epoch 406/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1646 - acc: 0.9010 - val_loss: 1.0441 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.91191\n",
            "Epoch 407/500\n",
            "960/960 [==============================] - 0s 231us/step - loss: 0.1641 - acc: 0.8990 - val_loss: 1.1310 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.91191\n",
            "Epoch 408/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1834 - acc: 0.8885 - val_loss: 1.1191 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.91191\n",
            "Epoch 409/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1712 - acc: 0.8990 - val_loss: 1.0610 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.91191\n",
            "Epoch 410/500\n",
            "960/960 [==============================] - 0s 209us/step - loss: 0.1844 - acc: 0.9042 - val_loss: 1.0519 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.91191\n",
            "Epoch 411/500\n",
            "960/960 [==============================] - 0s 244us/step - loss: 0.1724 - acc: 0.8948 - val_loss: 1.0714 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.91191\n",
            "Epoch 412/500\n",
            "960/960 [==============================] - 0s 232us/step - loss: 0.1585 - acc: 0.9073 - val_loss: 1.1018 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.91191\n",
            "Epoch 413/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1609 - acc: 0.9073 - val_loss: 1.0727 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.91191\n",
            "Epoch 414/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1614 - acc: 0.9104 - val_loss: 1.0895 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.91191\n",
            "Epoch 415/500\n",
            "960/960 [==============================] - 0s 232us/step - loss: 0.1655 - acc: 0.9000 - val_loss: 1.1168 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.91191\n",
            "Epoch 416/500\n",
            "960/960 [==============================] - 0s 228us/step - loss: 0.1611 - acc: 0.8958 - val_loss: 1.0962 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.91191\n",
            "Epoch 417/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1600 - acc: 0.9021 - val_loss: 1.0637 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.91191\n",
            "Epoch 418/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1750 - acc: 0.9062 - val_loss: 1.0891 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.91191\n",
            "Epoch 419/500\n",
            "960/960 [==============================] - 0s 211us/step - loss: 0.1908 - acc: 0.8896 - val_loss: 1.0741 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.91191\n",
            "Epoch 420/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1809 - acc: 0.8948 - val_loss: 1.0385 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.91191\n",
            "Epoch 421/500\n",
            "960/960 [==============================] - 0s 208us/step - loss: 0.1488 - acc: 0.9042 - val_loss: 1.0289 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.91191\n",
            "Epoch 422/500\n",
            "960/960 [==============================] - 0s 236us/step - loss: 0.1492 - acc: 0.9042 - val_loss: 1.0419 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.91191\n",
            "Epoch 423/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1558 - acc: 0.9042 - val_loss: 1.1429 - val_acc: 0.3375\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.91191\n",
            "Epoch 424/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1463 - acc: 0.9135 - val_loss: 1.0582 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.91191\n",
            "Epoch 425/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1418 - acc: 0.9156 - val_loss: 1.0513 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.91191\n",
            "Epoch 426/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.1388 - acc: 0.9156 - val_loss: 1.0324 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.91191\n",
            "Epoch 427/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1434 - acc: 0.9135 - val_loss: 1.0181 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.91191\n",
            "Epoch 428/500\n",
            "960/960 [==============================] - 0s 229us/step - loss: 0.1488 - acc: 0.9167 - val_loss: 1.1119 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.91191\n",
            "Epoch 429/500\n",
            "960/960 [==============================] - 0s 222us/step - loss: 0.1642 - acc: 0.8990 - val_loss: 1.0519 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.91191\n",
            "Epoch 430/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1602 - acc: 0.8969 - val_loss: 1.1533 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.91191\n",
            "Epoch 431/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.1524 - acc: 0.9125 - val_loss: 1.0711 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.91191\n",
            "Epoch 432/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.1604 - acc: 0.8969 - val_loss: 1.0526 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.91191\n",
            "Epoch 433/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.1597 - acc: 0.9104 - val_loss: 1.0522 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.91191\n",
            "Epoch 434/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.1631 - acc: 0.9073 - val_loss: 1.0533 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.91191\n",
            "Epoch 435/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1720 - acc: 0.9000 - val_loss: 1.0779 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.91191\n",
            "Epoch 436/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1719 - acc: 0.8990 - val_loss: 1.1219 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.91191\n",
            "Epoch 437/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.1648 - acc: 0.9083 - val_loss: 1.1711 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.91191\n",
            "Epoch 438/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1592 - acc: 0.9083 - val_loss: 1.0963 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.91191\n",
            "Epoch 439/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1571 - acc: 0.9083 - val_loss: 1.0329 - val_acc: 0.4083\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.91191\n",
            "Epoch 440/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.1380 - acc: 0.9167 - val_loss: 1.0460 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.91191\n",
            "Epoch 441/500\n",
            "960/960 [==============================] - 0s 229us/step - loss: 0.1373 - acc: 0.9187 - val_loss: 1.1259 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.91191\n",
            "Epoch 442/500\n",
            "960/960 [==============================] - 0s 224us/step - loss: 0.1272 - acc: 0.9271 - val_loss: 1.0878 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.91191\n",
            "Epoch 443/500\n",
            "960/960 [==============================] - 0s 228us/step - loss: 0.1240 - acc: 0.9281 - val_loss: 1.0671 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.91191\n",
            "Epoch 444/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1457 - acc: 0.9104 - val_loss: 0.9878 - val_acc: 0.4250\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.91191\n",
            "Epoch 445/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1526 - acc: 0.9135 - val_loss: 1.1036 - val_acc: 0.3792\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.91191\n",
            "Epoch 446/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.1672 - acc: 0.8979 - val_loss: 1.0515 - val_acc: 0.4125\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.91191\n",
            "Epoch 447/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1502 - acc: 0.9135 - val_loss: 1.0644 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.91191\n",
            "Epoch 448/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.1510 - acc: 0.9135 - val_loss: 1.0983 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.91191\n",
            "Epoch 449/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1442 - acc: 0.9135 - val_loss: 1.0284 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.91191\n",
            "Epoch 450/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1392 - acc: 0.9219 - val_loss: 1.0202 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.91191\n",
            "Epoch 451/500\n",
            "960/960 [==============================] - 0s 212us/step - loss: 0.1338 - acc: 0.9187 - val_loss: 1.0627 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.91191\n",
            "Epoch 452/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.1326 - acc: 0.9177 - val_loss: 1.0639 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.91191\n",
            "Epoch 453/500\n",
            "960/960 [==============================] - 0s 234us/step - loss: 0.1409 - acc: 0.9250 - val_loss: 1.1170 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.91191\n",
            "Epoch 454/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.1311 - acc: 0.9250 - val_loss: 1.0836 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.91191\n",
            "Epoch 455/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1355 - acc: 0.9229 - val_loss: 1.0816 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.91191\n",
            "Epoch 456/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1560 - acc: 0.9135 - val_loss: 1.0839 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.91191\n",
            "Epoch 457/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1473 - acc: 0.9177 - val_loss: 1.0590 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.91191\n",
            "Epoch 458/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1432 - acc: 0.9219 - val_loss: 1.0741 - val_acc: 0.3625\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.91191\n",
            "Epoch 459/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1419 - acc: 0.9208 - val_loss: 1.0659 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.91191\n",
            "Epoch 460/500\n",
            "960/960 [==============================] - 0s 229us/step - loss: 0.1411 - acc: 0.9271 - val_loss: 1.0721 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.91191\n",
            "Epoch 461/500\n",
            "960/960 [==============================] - 0s 211us/step - loss: 0.1384 - acc: 0.9292 - val_loss: 1.1227 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.91191\n",
            "Epoch 462/500\n",
            "960/960 [==============================] - 0s 225us/step - loss: 0.1396 - acc: 0.9177 - val_loss: 1.1372 - val_acc: 0.3250\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.91191\n",
            "Epoch 463/500\n",
            "960/960 [==============================] - 0s 222us/step - loss: 0.1335 - acc: 0.9271 - val_loss: 1.1066 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.91191\n",
            "Epoch 464/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1615 - acc: 0.9115 - val_loss: 1.1224 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.91191\n",
            "Epoch 465/500\n",
            "960/960 [==============================] - 0s 207us/step - loss: 0.1464 - acc: 0.9167 - val_loss: 1.0740 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.91191\n",
            "Epoch 466/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.1462 - acc: 0.9146 - val_loss: 1.0898 - val_acc: 0.3208\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.91191\n",
            "Epoch 467/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1359 - acc: 0.9167 - val_loss: 1.0746 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.91191\n",
            "Epoch 468/500\n",
            "960/960 [==============================] - 0s 235us/step - loss: 0.1353 - acc: 0.9198 - val_loss: 1.0536 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.91191\n",
            "Epoch 469/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1402 - acc: 0.9240 - val_loss: 1.1002 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.91191\n",
            "Epoch 470/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1308 - acc: 0.9229 - val_loss: 1.0329 - val_acc: 0.3917\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.91191\n",
            "Epoch 471/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1312 - acc: 0.9271 - val_loss: 1.0643 - val_acc: 0.3417\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.91191\n",
            "Epoch 472/500\n",
            "960/960 [==============================] - 0s 229us/step - loss: 0.1280 - acc: 0.9229 - val_loss: 1.0772 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.91191\n",
            "Epoch 473/500\n",
            "960/960 [==============================] - 0s 236us/step - loss: 0.1373 - acc: 0.9219 - val_loss: 1.0941 - val_acc: 0.3958\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.91191\n",
            "Epoch 474/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1369 - acc: 0.9177 - val_loss: 1.0578 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.91191\n",
            "Epoch 475/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.1469 - acc: 0.9115 - val_loss: 1.0602 - val_acc: 0.3875\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.91191\n",
            "Epoch 476/500\n",
            "960/960 [==============================] - 0s 223us/step - loss: 0.1329 - acc: 0.9229 - val_loss: 1.0573 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.91191\n",
            "Epoch 477/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1471 - acc: 0.9156 - val_loss: 1.0296 - val_acc: 0.4042\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.91191\n",
            "Epoch 478/500\n",
            "960/960 [==============================] - 0s 220us/step - loss: 0.1491 - acc: 0.9104 - val_loss: 1.0946 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.91191\n",
            "Epoch 479/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1667 - acc: 0.8958 - val_loss: 1.0749 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.91191\n",
            "Epoch 480/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1434 - acc: 0.9177 - val_loss: 1.0971 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.91191\n",
            "Epoch 481/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.1425 - acc: 0.9219 - val_loss: 1.1782 - val_acc: 0.3333\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.91191\n",
            "Epoch 482/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1429 - acc: 0.9167 - val_loss: 1.1110 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.91191\n",
            "Epoch 483/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1375 - acc: 0.9198 - val_loss: 1.1070 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.91191\n",
            "Epoch 484/500\n",
            "960/960 [==============================] - 0s 221us/step - loss: 0.1388 - acc: 0.9219 - val_loss: 1.0966 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.91191\n",
            "Epoch 485/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.1311 - acc: 0.9333 - val_loss: 1.1214 - val_acc: 0.3458\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.91191\n",
            "Epoch 486/500\n",
            "960/960 [==============================] - 0s 216us/step - loss: 0.1375 - acc: 0.9167 - val_loss: 1.1305 - val_acc: 0.3208\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.91191\n",
            "Epoch 487/500\n",
            "960/960 [==============================] - 0s 227us/step - loss: 0.1331 - acc: 0.9156 - val_loss: 1.0925 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.91191\n",
            "Epoch 488/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1312 - acc: 0.9281 - val_loss: 1.1116 - val_acc: 0.3250\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.91191\n",
            "Epoch 489/500\n",
            "960/960 [==============================] - 0s 219us/step - loss: 0.1298 - acc: 0.9240 - val_loss: 1.1019 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.91191\n",
            "Epoch 490/500\n",
            "960/960 [==============================] - 0s 218us/step - loss: 0.1367 - acc: 0.9219 - val_loss: 1.0766 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.91191\n",
            "Epoch 491/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1326 - acc: 0.9240 - val_loss: 1.0932 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.91191\n",
            "Epoch 492/500\n",
            "960/960 [==============================] - 0s 226us/step - loss: 0.1302 - acc: 0.9240 - val_loss: 1.0644 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.91191\n",
            "Epoch 493/500\n",
            "960/960 [==============================] - 0s 214us/step - loss: 0.1369 - acc: 0.9146 - val_loss: 1.0896 - val_acc: 0.3583\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.91191\n",
            "Epoch 494/500\n",
            "960/960 [==============================] - 0s 207us/step - loss: 0.1382 - acc: 0.9135 - val_loss: 1.0530 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.91191\n",
            "Epoch 495/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1369 - acc: 0.9177 - val_loss: 1.0708 - val_acc: 0.3417\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.91191\n",
            "Epoch 496/500\n",
            "960/960 [==============================] - 0s 222us/step - loss: 0.1386 - acc: 0.9240 - val_loss: 1.0389 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.91191\n",
            "Epoch 497/500\n",
            "960/960 [==============================] - 0s 213us/step - loss: 0.1529 - acc: 0.9135 - val_loss: 1.0906 - val_acc: 0.3542\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.91191\n",
            "Epoch 498/500\n",
            "960/960 [==============================] - 0s 215us/step - loss: 0.1417 - acc: 0.9198 - val_loss: 1.1362 - val_acc: 0.3500\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.91191\n",
            "Epoch 499/500\n",
            "960/960 [==============================] - 0s 217us/step - loss: 0.1381 - acc: 0.9208 - val_loss: 1.0599 - val_acc: 0.3708\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.91191\n",
            "Epoch 500/500\n",
            "960/960 [==============================] - 0s 207us/step - loss: 0.1332 - acc: 0.9219 - val_loss: 1.0413 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.91191\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fecafb95fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3H3UmbbWfuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load wights file of the best model :\n",
        "wights_file = 'Weights-483--1.04641.hdf5' # choose the best checkpoint \n",
        "NN_model.load_weights(wights_file) # load it\n",
        "NN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6VyfdDBWfrf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0ccba699-02ca-4d6f-b40e-e3a61f46b287"
      },
      "source": [
        "\n",
        "predictions = NN_model.predict(test)\n",
        "predictions\n",
        "#scores = NN_model.evaluate(predictions,label_test.target.values, verbose=1)\n",
        "#res=NN_model.evaluate(predictions,label_test.target)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.4775821],\n",
              "       [1.4474071],\n",
              "       [1.8636788],\n",
              "       [1.7674415],\n",
              "       [1.5171188],\n",
              "       [1.4717461],\n",
              "       [1.3327038],\n",
              "       [1.4276465],\n",
              "       [1.2691916],\n",
              "       [1.7284085],\n",
              "       [1.7189759],\n",
              "       [1.5568954],\n",
              "       [1.4766835],\n",
              "       [1.7321483],\n",
              "       [1.7358632],\n",
              "       [1.7459153],\n",
              "       [1.8139917],\n",
              "       [1.7737814],\n",
              "       [1.544687 ],\n",
              "       [1.732504 ],\n",
              "       [1.7372626],\n",
              "       [1.7159225],\n",
              "       [1.7785374],\n",
              "       [1.2944459],\n",
              "       [1.278225 ],\n",
              "       [1.7614675],\n",
              "       [1.6735955],\n",
              "       [1.6296952],\n",
              "       [1.7630457],\n",
              "       [1.2199517],\n",
              "       [1.863884 ],\n",
              "       [1.8261522],\n",
              "       [1.6151024],\n",
              "       [1.5814903],\n",
              "       [1.7771891],\n",
              "       [1.9159453],\n",
              "       [1.8419005],\n",
              "       [1.7009153],\n",
              "       [1.8864454],\n",
              "       [1.8804176],\n",
              "       [1.8591343],\n",
              "       [1.6754752],\n",
              "       [1.714995 ],\n",
              "       [1.7063984],\n",
              "       [1.6672928],\n",
              "       [1.5272135],\n",
              "       [1.7415657],\n",
              "       [1.5879565],\n",
              "       [1.6080159],\n",
              "       [1.6017834],\n",
              "       [1.6679989],\n",
              "       [1.8288144],\n",
              "       [1.7417771],\n",
              "       [1.5589972],\n",
              "       [1.3210146],\n",
              "       [1.5395044],\n",
              "       [1.5740435],\n",
              "       [1.7334443],\n",
              "       [1.8930956],\n",
              "       [1.7310419],\n",
              "       [1.5919212],\n",
              "       [1.6859568],\n",
              "       [1.3985127],\n",
              "       [1.3107542],\n",
              "       [1.5119222],\n",
              "       [1.8439854],\n",
              "       [1.452299 ],\n",
              "       [1.7978998],\n",
              "       [1.704295 ],\n",
              "       [1.6546206],\n",
              "       [1.4273733],\n",
              "       [1.7817353],\n",
              "       [1.5706962],\n",
              "       [1.6764339],\n",
              "       [1.6228617],\n",
              "       [1.4980859],\n",
              "       [1.4378589],\n",
              "       [1.1734307],\n",
              "       [1.259091 ],\n",
              "       [1.3517269],\n",
              "       [1.1578504],\n",
              "       [1.3074615],\n",
              "       [1.0677004],\n",
              "       [1.2110165],\n",
              "       [1.8363491],\n",
              "       [1.8478712],\n",
              "       [1.7899426],\n",
              "       [1.7337219],\n",
              "       [1.6520486],\n",
              "       [1.6152242],\n",
              "       [1.494616 ],\n",
              "       [1.7129904],\n",
              "       [1.4930481],\n",
              "       [1.7348932],\n",
              "       [1.8663517],\n",
              "       [1.7822899],\n",
              "       [1.7499225],\n",
              "       [1.293946 ],\n",
              "       [1.5243081],\n",
              "       [1.3606275],\n",
              "       [1.7844377],\n",
              "       [1.595323 ],\n",
              "       [1.0294353],\n",
              "       [1.735051 ],\n",
              "       [1.8981841],\n",
              "       [1.9399318],\n",
              "       [1.9852413],\n",
              "       [1.8044776],\n",
              "       [1.591766 ],\n",
              "       [1.7200307],\n",
              "       [1.647468 ],\n",
              "       [1.705104 ],\n",
              "       [1.8149432],\n",
              "       [1.8648227],\n",
              "       [1.5830133],\n",
              "       [1.5742468],\n",
              "       [1.6289705],\n",
              "       [1.8281757],\n",
              "       [1.4649013],\n",
              "       [1.830046 ],\n",
              "       [1.6639522],\n",
              "       [1.6677089],\n",
              "       [1.6547683],\n",
              "       [1.6119626],\n",
              "       [1.7149248],\n",
              "       [1.8224787],\n",
              "       [1.6312003],\n",
              "       [1.4320688],\n",
              "       [1.3307023],\n",
              "       [1.625054 ],\n",
              "       [1.6291817],\n",
              "       [1.8709714],\n",
              "       [1.5179046],\n",
              "       [1.5517913],\n",
              "       [1.4395747],\n",
              "       [1.825688 ],\n",
              "       [1.9087179],\n",
              "       [1.9092482],\n",
              "       [1.6636094],\n",
              "       [1.6584855],\n",
              "       [1.4266928],\n",
              "       [1.4988389],\n",
              "       [1.3395479],\n",
              "       [1.3919909],\n",
              "       [1.4638268],\n",
              "       [1.449864 ],\n",
              "       [1.4427801],\n",
              "       [1.4205226],\n",
              "       [1.770533 ],\n",
              "       [1.5487319],\n",
              "       [1.6713216],\n",
              "       [1.5688356],\n",
              "       [1.5567175],\n",
              "       [1.544418 ],\n",
              "       [1.5377179],\n",
              "       [1.584056 ],\n",
              "       [1.7490689],\n",
              "       [1.7794683],\n",
              "       [1.4740108],\n",
              "       [1.8265728],\n",
              "       [1.8128241],\n",
              "       [1.0385234],\n",
              "       [1.8492066],\n",
              "       [1.8115126],\n",
              "       [1.8591176],\n",
              "       [1.4900944],\n",
              "       [1.2507417],\n",
              "       [1.4006046],\n",
              "       [1.4292688],\n",
              "       [1.4971945],\n",
              "       [1.4903177],\n",
              "       [1.2518812],\n",
              "       [1.682511 ],\n",
              "       [1.8228554],\n",
              "       [1.7872722],\n",
              "       [1.852204 ],\n",
              "       [1.6307266],\n",
              "       [1.7235309],\n",
              "       [1.68651  ],\n",
              "       [1.7230245],\n",
              "       [1.7536211],\n",
              "       [1.72232  ],\n",
              "       [1.7235588],\n",
              "       [1.6231668],\n",
              "       [1.5597452],\n",
              "       [1.6355236],\n",
              "       [1.6797811],\n",
              "       [1.4936496],\n",
              "       [1.5006449],\n",
              "       [1.4751846],\n",
              "       [1.3662106],\n",
              "       [1.391878 ],\n",
              "       [1.3896422],\n",
              "       [1.4278604],\n",
              "       [1.3512908],\n",
              "       [1.4134365],\n",
              "       [1.3628937],\n",
              "       [1.4017949],\n",
              "       [1.4108924],\n",
              "       [1.4921631],\n",
              "       [1.4298168],\n",
              "       [1.5023764],\n",
              "       [1.5012811],\n",
              "       [1.4935387],\n",
              "       [1.5010049],\n",
              "       [1.4872401],\n",
              "       [1.4905636],\n",
              "       [1.4841326],\n",
              "       [1.4218652],\n",
              "       [1.3950266],\n",
              "       [1.4650668],\n",
              "       [1.437437 ],\n",
              "       [1.4498619],\n",
              "       [1.4133447],\n",
              "       [1.3263772],\n",
              "       [1.4076688],\n",
              "       [1.2315224],\n",
              "       [1.3084853],\n",
              "       [1.3347577],\n",
              "       [1.2572547],\n",
              "       [1.3811235],\n",
              "       [1.3968503],\n",
              "       [1.4414687],\n",
              "       [1.291644 ],\n",
              "       [1.4186413],\n",
              "       [1.4363557],\n",
              "       [1.3168635],\n",
              "       [1.3350457],\n",
              "       [1.33935  ],\n",
              "       [1.4298137],\n",
              "       [1.4746726],\n",
              "       [1.4312415],\n",
              "       [1.3242122],\n",
              "       [1.3890685],\n",
              "       [1.4068335],\n",
              "       [1.3877988],\n",
              "       [1.3779101],\n",
              "       [1.3131175],\n",
              "       [1.4339765],\n",
              "       [1.3435074],\n",
              "       [1.3413558],\n",
              "       [1.3253517],\n",
              "       [1.3848392],\n",
              "       [1.8687608],\n",
              "       [1.5214925],\n",
              "       [1.5717102],\n",
              "       [1.3674363],\n",
              "       [1.5329674],\n",
              "       [1.4569544],\n",
              "       [1.3424186],\n",
              "       [1.5305248],\n",
              "       [1.5324297],\n",
              "       [1.6273443],\n",
              "       [1.7173783],\n",
              "       [1.5900408],\n",
              "       [1.5351686],\n",
              "       [1.3720902],\n",
              "       [1.3873262],\n",
              "       [1.7636294],\n",
              "       [1.7837023],\n",
              "       [1.5983888],\n",
              "       [1.5759245],\n",
              "       [1.3975288],\n",
              "       [1.4906679],\n",
              "       [1.5183893],\n",
              "       [1.8104473],\n",
              "       [1.2782214],\n",
              "       [1.2021251],\n",
              "       [1.177171 ],\n",
              "       [1.7361082],\n",
              "       [1.817362 ],\n",
              "       [1.7385086],\n",
              "       [1.4429623],\n",
              "       [1.6559948],\n",
              "       [1.7904891],\n",
              "       [1.3872477],\n",
              "       [1.3430527],\n",
              "       [1.6522688],\n",
              "       [1.3786346],\n",
              "       [1.6826817],\n",
              "       [1.4674579],\n",
              "       [1.4529692],\n",
              "       [1.4483174],\n",
              "       [1.4474941],\n",
              "       [1.3826499],\n",
              "       [1.8303614],\n",
              "       [1.6583778],\n",
              "       [1.9078506],\n",
              "       [1.845117 ],\n",
              "       [1.7676679],\n",
              "       [1.7135369],\n",
              "       [1.655805 ],\n",
              "       [1.7485974],\n",
              "       [1.4684741],\n",
              "       [1.8266753],\n",
              "       [1.6319351],\n",
              "       [1.7634872],\n",
              "       [1.5663623],\n",
              "       [1.5182632],\n",
              "       [1.2441626],\n",
              "       [1.1845111],\n",
              "       [1.034113 ],\n",
              "       [1.0300122],\n",
              "       [1.0373988],\n",
              "       [1.0417492],\n",
              "       [1.8201443],\n",
              "       [1.7278472],\n",
              "       [1.4915644],\n",
              "       [1.6069902],\n",
              "       [1.2682434],\n",
              "       [1.386263 ],\n",
              "       [1.7458228],\n",
              "       [1.2868698],\n",
              "       [1.2816566],\n",
              "       [1.0803388],\n",
              "       [1.2668957],\n",
              "       [1.733904 ],\n",
              "       [1.6545105],\n",
              "       [1.5185982],\n",
              "       [1.5269982],\n",
              "       [1.6819973],\n",
              "       [1.8419536],\n",
              "       [1.6744057],\n",
              "       [1.8406292],\n",
              "       [1.8607134],\n",
              "       [1.842717 ],\n",
              "       [1.9021508],\n",
              "       [1.877686 ],\n",
              "       [1.7693335],\n",
              "       [1.7260436],\n",
              "       [1.7571493],\n",
              "       [1.7079494],\n",
              "       [1.722006 ],\n",
              "       [1.7912244],\n",
              "       [1.7151711],\n",
              "       [1.7255588],\n",
              "       [1.7585021],\n",
              "       [1.8390899],\n",
              "       [1.8254704],\n",
              "       [1.8584206],\n",
              "       [1.8044697],\n",
              "       [1.8116804],\n",
              "       [1.8211086],\n",
              "       [1.8302989],\n",
              "       [1.8224554],\n",
              "       [1.8485494],\n",
              "       [1.8682804],\n",
              "       [1.8710312],\n",
              "       [1.8779672],\n",
              "       [1.9096744],\n",
              "       [1.7591008],\n",
              "       [1.4968526],\n",
              "       [1.4115878],\n",
              "       [1.4361569],\n",
              "       [1.3665258],\n",
              "       [1.7549704],\n",
              "       [1.666804 ],\n",
              "       [1.7939174],\n",
              "       [1.7010757],\n",
              "       [1.5789182],\n",
              "       [1.5388476],\n",
              "       [1.614695 ],\n",
              "       [1.7620668],\n",
              "       [1.767099 ],\n",
              "       [1.696906 ],\n",
              "       [1.5747242],\n",
              "       [1.4412743],\n",
              "       [1.2853601],\n",
              "       [1.768542 ],\n",
              "       [1.798664 ],\n",
              "       [1.7207552],\n",
              "       [1.8468372],\n",
              "       [1.9129142],\n",
              "       [1.8285339],\n",
              "       [1.472617 ],\n",
              "       [1.0737959],\n",
              "       [1.1278646],\n",
              "       [1.0426499],\n",
              "       [1.0498964],\n",
              "       [1.2492291],\n",
              "       [1.2138295],\n",
              "       [1.842975 ],\n",
              "       [1.7812661],\n",
              "       [1.7280943],\n",
              "       [1.4187804],\n",
              "       [1.6967914],\n",
              "       [1.6367533],\n",
              "       [1.5711073],\n",
              "       [1.7478603],\n",
              "       [1.8548034],\n",
              "       [1.8150316],\n",
              "       [1.8096331],\n",
              "       [1.5666791],\n",
              "       [1.4115945],\n",
              "       [1.8096497],\n",
              "       [1.8067042],\n",
              "       [1.8562669],\n",
              "       [1.4444402],\n",
              "       [1.245971 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z9Nl14zYopB",
        "colab_type": "code",
        "outputId": "c1446166-7f02-4e67-c4c5-80ad6f2ae102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "plt.title(\"comparisone between actual and predicted values\")\n",
        "#plt=plt.figure(200)\n",
        "plt.plot(predictions)\n",
        "plt.plot(label_test.target)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fecaf9fad30>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM3ZZiWVcj0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}